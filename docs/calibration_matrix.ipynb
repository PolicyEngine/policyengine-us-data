{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Calibration Matrix\n",
    "\n",
    "The calibration pipeline has three stages: (1) compute uprated target values, (2) assemble the sparse constraint matrix (this notebook), and (3) optimize weights (`unified_calibration.py`). This notebook is the diagnostic checkpoint between stages 1 and 2 — understand your matrix before you optimize.\n",
    "\n",
    "We build the full calibration matrix using `UnifiedMatrixBuilder` with clone-based geography from `assign_random_geography`, then inspect its structure: what rows and columns represent, how target groups partition the loss function, and where sparsity patterns emerge.\n",
    "\n",
    "**Column layout:** `col = clone_idx * n_records + record_idx`\n",
    "\n",
    "**Requirements:** `policy_data.db`, `block_cd_distributions.csv.gz`, and the stratified CPS h5 file in `STORAGE_FOLDER`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baogorek/envs/sep/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from policyengine_us import Microsimulation\n",
    "from policyengine_us_data.storage import STORAGE_FOLDER\n",
    "from policyengine_us_data.calibration.unified_matrix_builder import (\n",
    "    UnifiedMatrixBuilder,\n",
    ")\n",
    "from policyengine_us_data.calibration.clone_and_assign import (\n",
    "    assign_random_geography,\n",
    ")\n",
    "from policyengine_us_data.datasets.cps.local_area_calibration.calibration_utils import (\n",
    "    create_target_groups,\n",
    "    drop_target_groups,\n",
    "    get_geo_level,\n",
    "    STATE_CODES,\n",
    ")\n",
    "\n",
    "db_path = STORAGE_FOLDER / \"calibration\" / \"policy_data.db\"\n",
    "db_uri = f\"sqlite:///{db_path}\"\n",
    "dataset_path = STORAGE_FOLDER / \"stratified_extended_cps_2024.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records: 11,999, Clones: 3, Total columns: 35,997\n",
      "Matrix shape: (1411, 35997)\n",
      "Non-zero entries: 29,425\n"
     ]
    }
   ],
   "source": [
    "sim = Microsimulation(dataset=str(dataset_path))\n",
    "n_records = sim.calculate(\"household_id\", map_to=\"household\").values.shape[0]\n",
    "\n",
    "N_CLONES = 3  # keep small for diagnostics\n",
    "geography = assign_random_geography(n_records, n_clones=N_CLONES, seed=42)\n",
    "\n",
    "builder = UnifiedMatrixBuilder(\n",
    "    db_uri=db_uri,\n",
    "    time_period=2024,\n",
    "    dataset_path=str(dataset_path),\n",
    ")\n",
    "\n",
    "targets_df, X_sparse, target_names = builder.build_matrix(\n",
    "    geography,\n",
    "    sim,\n",
    "    target_filter={\"domain_variables\": [\"aca_ptc\", \"snap\"]},\n",
    "    hierarchical_domains=[\"aca_ptc\", \"snap\"],\n",
    ")\n",
    "\n",
    "n_total = n_records * N_CLONES\n",
    "print(f\"Records: {n_records:,}, Clones: {N_CLONES}, Total columns: {n_total:,}\")\n",
    "print(f\"Matrix shape: {X_sparse.shape}\")\n",
    "print(f\"Non-zero entries: {X_sparse.nnz:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Matrix overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets: 1411\n",
      "Columns: 35,997 (3 clones x 11,999 records)\n",
      "Non-zeros: 29,425\n",
      "Density: 0.000579\n",
      "  National: 1 targets\n",
      "  State: 102 targets\n",
      "  District: 1308 targets\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets: {X_sparse.shape[0]}\")\n",
    "print(f\"Columns: {X_sparse.shape[1]:,} ({N_CLONES} clones x {n_records:,} records)\")\n",
    "print(f\"Non-zeros: {X_sparse.nnz:,}\")\n",
    "print(f\"Density: {X_sparse.nnz / (X_sparse.shape[0] * X_sparse.shape[1]):.6f}\")\n",
    "\n",
    "geo_levels = targets_df[\"geographic_id\"].apply(get_geo_level)\n",
    "level_names = {0: \"National\", 1: \"State\", 2: \"District\"}\n",
    "for level in [0, 1, 2]:\n",
    "    n = (geo_levels == level).sum()\n",
    "    if n > 0:\n",
    "        print(f\"  {level_names[level]}: {n} targets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Anatomy of a row\n",
    "\n",
    "Each row is one calibration target — a known aggregate (dollar total, household count, person count) that the optimizer tries to match. The row vector's non-zero entries identify which cloned records can contribute to that target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 705: cd_3402/household_count/[snap>0]\n",
      "  variable: household_count\n",
      "  geographic_id: 3402\n",
      "  geo_level: district\n",
      "  target value: 48,652\n",
      "  uprating_factor: 1.0\n"
     ]
    }
   ],
   "source": [
    "mid_row = X_sparse.shape[0] // 2\n",
    "row = targets_df.iloc[mid_row]\n",
    "print(f\"Row {mid_row}: {target_names[mid_row]}\")\n",
    "print(f\"  variable: {row['variable']}\")\n",
    "print(f\"  geographic_id: {row['geographic_id']}\")\n",
    "print(f\"  geo_level: {row['geo_level']}\")\n",
    "print(f\"  target value: {row['value']:,.0f}\")\n",
    "print(f\"  uprating_factor: {row.get('uprating_factor', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 705 has 10 non-zero columns\n",
      "  Spans 3 clone(s)\n",
      "  Spans 10 unique record(s)\n",
      "\n",
      "First non-zero column (1212):\n",
      "  clone_idx: 0\n",
      "  record_idx: 1212\n",
      "  state_fips: 34\n",
      "  cd_geoid: 3402\n",
      "  value: 1.00\n"
     ]
    }
   ],
   "source": [
    "row_vec = X_sparse[mid_row, :]\n",
    "nz_cols = row_vec.nonzero()[1]\n",
    "print(f\"Row {mid_row} has {len(nz_cols):,} non-zero columns\")\n",
    "\n",
    "if len(nz_cols) > 0:\n",
    "    clone_indices = nz_cols // n_records\n",
    "    record_indices = nz_cols % n_records\n",
    "    print(f\"  Spans {len(np.unique(clone_indices))} clone(s)\")\n",
    "    print(f\"  Spans {len(np.unique(record_indices))} unique record(s)\")\n",
    "\n",
    "    first_col = nz_cols[0]\n",
    "    print(f\"\\nFirst non-zero column ({first_col}):\")\n",
    "    print(f\"  clone_idx: {first_col // n_records}\")\n",
    "    print(f\"  record_idx: {first_col % n_records}\")\n",
    "    print(f\"  state_fips: {geography.state_fips[first_col]}\")\n",
    "    print(f\"  cd_geoid: {geography.cd_geoid[first_col]}\")\n",
    "    print(f\"  value: {X_sparse[mid_row, first_col]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Anatomy of a column\n",
    "\n",
    "Each column represents one (record, clone) pair. Columns are organized in clone blocks: the first `n_records` columns belong to clone 0, the next to clone 1, and so on. The block formula is:\n",
    "\n",
    "$$\\text{column\\_idx} = \\text{clone\\_idx} \\times n_{\\text{records}} + \\text{record\\_idx}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 12041:\n",
      "  clone_idx: 1\n",
      "  record_idx: 42\n",
      "  state_fips: 45\n",
      "  cd_geoid: 4507\n",
      "  block_geoid: 450410002022009\n",
      "\n",
      "This column has non-zero values in 0 target rows\n"
     ]
    }
   ],
   "source": [
    "col_idx = 1 * n_records + 42  # clone 1, record 42\n",
    "clone_idx = col_idx // n_records\n",
    "record_idx = col_idx % n_records\n",
    "print(f\"Column {col_idx}:\")\n",
    "print(f\"  clone_idx: {clone_idx}\")\n",
    "print(f\"  record_idx: {record_idx}\")\n",
    "print(f\"  state_fips: {geography.state_fips[col_idx]}\")\n",
    "print(f\"  cd_geoid: {geography.cd_geoid[col_idx]}\")\n",
    "print(f\"  block_geoid: {geography.block_geoid[col_idx]}\")\n",
    "\n",
    "col_vec = X_sparse[:, col_idx]\n",
    "nz_rows = col_vec.nonzero()[0]\n",
    "print(f\"\\nThis column has non-zero values in {len(nz_rows)} target rows\")\n",
    "if len(nz_rows) > 0:\n",
    "    print(\"First 5 target rows:\")\n",
    "    for r in nz_rows[:5]:\n",
    "        row = targets_df.iloc[r]\n",
    "        print(\n",
    "            f\"  row {r}: {row['variable']} \"\n",
    "            f\"(geo={row['geographic_id']}, \"\n",
    "            f\"val={X_sparse[r, col_idx]:.2f})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block formula verified: clone_idx=1 * n_records=11999 + record_idx=42 = 12041\n"
     ]
    }
   ],
   "source": [
    "expected_col = 1 * n_records + 42\n",
    "assert col_idx == expected_col, f\"{col_idx} != {expected_col}\"\n",
    "print(\n",
    "    f\"Block formula verified: \"\n",
    "    f\"clone_idx=1 * n_records={n_records} + record_idx=42 = {expected_col}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Target groups and loss weighting\n",
    "\n",
    "Target groups partition the rows by (domain, variable, geographic level). Each group contributes equally to the loss function, so 436 district-level rows don't drown out 1 national row. The group IDs depend on the current database contents and may change if targets are added or removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Creating Target Groups ===\n",
      "\n",
      "National targets:\n",
      "  Group 0: ACA PTC Person Count = 19,743,689\n",
      "\n",
      "State targets:\n",
      "  Group 1: SNAP Household Count (51 targets)\n",
      "  Group 2: Snap (51 targets)\n",
      "\n",
      "District targets:\n",
      "  Group 3: Aca Ptc (436 targets)\n",
      "  Group 4: ACA PTC Tax Unit Count (436 targets)\n",
      "  Group 5: SNAP Household Count (436 targets)\n",
      "\n",
      "Total groups created: 6\n",
      "========================================\n",
      " group_id                                                         description  n_targets    min_value  median_value    max_value\n",
      "        0 Group 0: National ACA PTC Person Count (1 target, value=19,743,689)          1 1.974369e+07  1.974369e+07 1.974369e+07\n",
      "        1                    Group 1: State SNAP Household Count (51 targets)         51 1.369100e+04  2.772372e+05 3.128640e+06\n",
      "        2                                    Group 2: State Snap (51 targets)         51 5.670186e+07  1.293585e+09 1.237718e+10\n",
      "        3                             Group 3: District Aca Ptc (436 targets)        436 5.420354e+06  2.937431e+07 3.880971e+08\n",
      "        4              Group 4: District ACA PTC Tax Unit Count (436 targets)        436 3.529773e+03  1.686570e+04 9.260854e+04\n",
      "        5                Group 5: District SNAP Household Count (436 targets)        436 1.156792e+04  4.687966e+04 1.735910e+05\n"
     ]
    }
   ],
   "source": [
    "target_groups, group_info = create_target_groups(targets_df)\n",
    "\n",
    "records = []\n",
    "for gid, info in enumerate(group_info):\n",
    "    mask = target_groups == gid\n",
    "    vals = targets_df.loc[mask, \"value\"]\n",
    "    records.append({\n",
    "        \"group_id\": gid,\n",
    "        \"description\": info,\n",
    "        \"n_targets\": mask.sum(),\n",
    "        \"min_value\": vals.min(),\n",
    "        \"median_value\": vals.median(),\n",
    "        \"max_value\": vals.max(),\n",
    "    })\n",
    "\n",
    "group_df = pd.DataFrame(records)\n",
    "print(group_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Group 0: National ACA PTC Person Count (1 target, value=19,743,689) ---\n",
      "    variable geographic_id      value\n",
      "person_count            US 19743689.0\n",
      "\n",
      "--- Group 2: State Snap (51 targets) ---\n",
      "variable geographic_id        value\n",
      "    snap             1 1733693703.0\n",
      "    snap            10  254854243.0\n",
      "    snap            11  319119173.0\n",
      "    snap            12 6604797454.0\n",
      "    snap            13 3281329856.0\n",
      "    snap            15  731331421.0\n",
      "    snap            16  281230283.0\n",
      "    snap            17 4469341818.0\n",
      "\n",
      "--- Group 4: District ACA PTC Tax Unit Count (436 targets) ---\n",
      "      variable geographic_id        value\n",
      "tax_unit_count          1000 25064.255490\n",
      "tax_unit_count           101  9794.081624\n",
      "tax_unit_count           102 11597.544977\n",
      "tax_unit_count           103  9160.097959\n",
      "tax_unit_count           104  9786.728220\n",
      "tax_unit_count           105 18266.234326\n",
      "tax_unit_count           106 25397.026846\n",
      "tax_unit_count           107 11798.642968\n"
     ]
    }
   ],
   "source": [
    "for gid in [0, 2, 4]:\n",
    "    if gid >= len(group_info):\n",
    "        continue\n",
    "    mask = target_groups == gid\n",
    "    rows = targets_df[mask][[\"variable\", \"geographic_id\", \"value\"]].head(8)\n",
    "    print(f\"\\n--- {group_info[gid]} ---\")\n",
    "    print(rows.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tracing a household across clones\n",
    "\n",
    "One CPS record appears once per clone (N_CLONES column positions). Each clone places it in a different census block/CD/state, so it contributes to different geographic targets depending on the clone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example SNAP-receiving household: record index 2\n",
      "SNAP value: $679\n",
      "\n",
      "Column positions across 3 clones:\n",
      "  col 2: TX (state=48, CD=4814) — 4 non-zero rows\n",
      "  col 12001: IN (state=18, CD=1804) — 3 non-zero rows\n",
      "  col 24000: PA (state=42, CD=4212) — 3 non-zero rows\n"
     ]
    }
   ],
   "source": [
    "snap_values = sim.calculate(\"snap\", map_to=\"household\").values\n",
    "hh_ids = sim.calculate(\"household_id\", map_to=\"household\").values\n",
    "positive_snap = hh_ids[snap_values > 0]\n",
    "example_hh_idx = int(np.where(snap_values > 0)[0][0])\n",
    "print(f\"Example SNAP-receiving household: record index {example_hh_idx}\")\n",
    "print(f\"SNAP value: ${snap_values[example_hh_idx]:,.0f}\")\n",
    "\n",
    "clone_cols = [c * n_records + example_hh_idx for c in range(N_CLONES)]\n",
    "print(f\"\\nColumn positions across {N_CLONES} clones:\")\n",
    "for col in clone_cols:\n",
    "    state = geography.state_fips[col]\n",
    "    cd = geography.cd_geoid[col]\n",
    "    block = geography.block_geoid[col]\n",
    "    col_vec = X_sparse[:, col]\n",
    "    nnz = col_vec.nnz\n",
    "    abbr = STATE_CODES.get(state, \"??\")\n",
    "    print(f\"  col {col}: {abbr} (state={state}, CD={cd}) — {nnz} non-zero rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clone 0 (col 2, CD 4814):\n",
      "  person_count (geo=US): 3.00\n",
      "  household_count (geo=48): 1.00\n",
      "  snap (geo=48): 678.60\n",
      "  household_count (geo=4814): 1.00\n",
      "\n",
      "Clone 1 (col 12001, CD 1804):\n",
      "  household_count (geo=18): 1.00\n",
      "  snap (geo=18): 678.60\n",
      "  household_count (geo=1804): 1.00\n",
      "\n",
      "Clone 2 (col 24000, CD 4212):\n",
      "  household_count (geo=42): 1.00\n",
      "  snap (geo=42): 678.60\n",
      "  household_count (geo=4212): 1.00\n"
     ]
    }
   ],
   "source": [
    "for col in clone_cols:\n",
    "    col_vec = X_sparse[:, col]\n",
    "    nz_rows = col_vec.nonzero()[0]\n",
    "    if len(nz_rows) == 0:\n",
    "        continue\n",
    "    clone_i = col // n_records\n",
    "    print(f\"\\nClone {clone_i} (col {col}, CD {geography.cd_geoid[col]}):\")\n",
    "    for r in nz_rows[:5]:\n",
    "        row = targets_df.iloc[r]\n",
    "        print(\n",
    "            f\"  {row['variable']} (geo={row['geographic_id']}): \"\n",
    "            f\"{X_sparse[r, col]:.2f}\"\n",
    "        )\n",
    "    if len(nz_rows) > 5:\n",
    "        print(f\"  ... and {len(nz_rows) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sparsity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cells: 50,791,767\n",
      "Non-zero entries: 29,425\n",
      "Density: 0.000579\n",
      "Sparsity: 99.9421%\n"
     ]
    }
   ],
   "source": [
    "total_cells = X_sparse.shape[0] * X_sparse.shape[1]\n",
    "density = X_sparse.nnz / total_cells\n",
    "print(f\"Total cells: {total_cells:,}\")\n",
    "print(f\"Non-zero entries: {X_sparse.nnz:,}\")\n",
    "print(f\"Density: {density:.6f}\")\n",
    "print(f\"Sparsity: {1 - density:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zeros per row:\n",
      "  min:    0\n",
      "  median: 10\n",
      "  mean:   21\n",
      "  max:    3,408\n",
      "\n",
      "By geographic level:\n",
      "  National  : n=   1, median nnz=  3,408, range=[3,408, 3,408]\n",
      "  State     : n= 102, median nnz=     80, range=[10, 694]\n",
      "  District  : n=1308, median nnz=      9, range=[0, 27]\n"
     ]
    }
   ],
   "source": [
    "nnz_per_row = np.diff(X_sparse.indptr)\n",
    "print(f\"Non-zeros per row:\")\n",
    "print(f\"  min:    {nnz_per_row.min():,}\")\n",
    "print(f\"  median: {int(np.median(nnz_per_row)):,}\")\n",
    "print(f\"  mean:   {nnz_per_row.mean():,.0f}\")\n",
    "print(f\"  max:    {nnz_per_row.max():,}\")\n",
    "\n",
    "geo_levels = targets_df[\"geographic_id\"].apply(get_geo_level)\n",
    "level_names = {0: \"National\", 1: \"State\", 2: \"District\"}\n",
    "print(\"\\nBy geographic level:\")\n",
    "for level in [0, 1, 2]:\n",
    "    mask = (geo_levels == level).values\n",
    "    if mask.any():\n",
    "        vals = nnz_per_row[mask]\n",
    "        print(\n",
    "            f\"  {level_names[level]:10s}: \"\n",
    "            f\"n={mask.sum():>4d}, \"\n",
    "            f\"median nnz={int(np.median(vals)):>7,}, \"\n",
    "            f\"range=[{vals.min():,}, {vals.max():,}]\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zeros per clone block:\n",
      " clone  nnz  unique_states\n",
      "     0 9775             51\n",
      "     1 9810             51\n",
      "     2 9840             51\n"
     ]
    }
   ],
   "source": [
    "clone_nnz = []\n",
    "for ci in range(N_CLONES):\n",
    "    block = X_sparse[:, ci * n_records : (ci + 1) * n_records]\n",
    "    n_states = len(np.unique(geography.state_fips[ci * n_records : (ci + 1) * n_records]))\n",
    "    clone_nnz.append({\n",
    "        \"clone\": ci,\n",
    "        \"nnz\": block.nnz,\n",
    "        \"unique_states\": n_states,\n",
    "    })\n",
    "\n",
    "clone_df = pd.DataFrame(clone_nnz)\n",
    "print(\"Non-zeros per clone block:\")\n",
    "print(clone_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dropping target groups\n",
    "\n",
    "Some target groups are redundant after hierarchical uprating. For example, state-level SNAP Household Count (Group 1) is redundant with district-level SNAP Household Count (Group 5) — the district targets were already reconciled to sum to the state totals.\n",
    "\n",
    "Specify drops as `(variable_label, geo_level)` pairs. The labels come from the group descriptions above; the geo level is \"National\", \"State\", or \"District\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix before: 1411 rows\n",
      "  DROPPING Group 1: State SNAP Household Count (51 targets) (51 rows)\n",
      "\n",
      "  KEEPING  Group 0: National ACA PTC Person Count (1 target, value=19,743,689) (1 rows)\n",
      "  KEEPING  Group 2: State Snap (51 targets) (51 rows)\n",
      "  KEEPING  Group 3: District Aca Ptc (436 targets) (436 rows)\n",
      "  KEEPING  Group 4: District ACA PTC Tax Unit Count (436 targets) (436 rows)\n",
      "  KEEPING  Group 5: District SNAP Household Count (436 targets) (436 rows)\n",
      "\n",
      "Matrix after: 1360 rows\n"
     ]
    }
   ],
   "source": [
    "GROUPS_TO_DROP = [\n",
    "    (\"SNAP Household Count\", \"State\"),\n",
    "]\n",
    "\n",
    "targets_filtered, X_filtered = drop_target_groups(\n",
    "    targets_df, X_sparse, target_groups, group_info, GROUPS_TO_DROP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Creating Target Groups ===\n",
      "\n",
      "National targets:\n",
      "  Group 0: ACA PTC Person Count = 19,743,689\n",
      "\n",
      "State targets:\n",
      "  Group 1: Snap (51 targets)\n",
      "\n",
      "District targets:\n",
      "  Group 2: Aca Ptc (436 targets)\n",
      "  Group 3: ACA PTC Tax Unit Count (436 targets)\n",
      "  Group 4: SNAP Household Count (436 targets)\n",
      "\n",
      "Total groups created: 5\n",
      "========================================\n",
      "\n",
      "Remaining groups (5):\n",
      "  Group 0: National ACA PTC Person Count (1 target, value=19,743,689)\n",
      "  Group 1: State Snap (51 targets)\n",
      "  Group 2: District Aca Ptc (436 targets)\n",
      "  Group 3: District ACA PTC Tax Unit Count (436 targets)\n",
      "  Group 4: District SNAP Household Count (436 targets)\n"
     ]
    }
   ],
   "source": [
    "remaining_groups, remaining_info = create_target_groups(targets_filtered)\n",
    "print(f\"\\nRemaining groups ({len(remaining_info)}):\")\n",
    "for info in remaining_info:\n",
    "    print(f\"  {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Achievable targets\n",
    "\n",
    "A target is achievable if at least one household can contribute to it (row sum > 0). Rows with sum = 0 are impossible constraints that the optimizer cannot satisfy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achievable targets: 1358\n",
      "Impossible targets: 2\n",
      "\n",
      "Impossible targets by (domain, variable):\n",
      "  aca_ptc/aca_ptc: 1\n",
      "  aca_ptc/tax_unit_count: 1\n"
     ]
    }
   ],
   "source": [
    "row_sums = np.array(X_filtered.sum(axis=1)).flatten()\n",
    "achievable_mask = row_sums > 0\n",
    "n_achievable = achievable_mask.sum()\n",
    "n_impossible = (~achievable_mask).sum()\n",
    "\n",
    "print(f\"Achievable targets: {n_achievable}\")\n",
    "print(f\"Impossible targets: {n_impossible}\")\n",
    "\n",
    "if n_impossible > 0:\n",
    "    impossible = targets_filtered[~achievable_mask]\n",
    "    by_var = (\n",
    "        impossible.groupby([\"domain_variable\", \"variable\"])\n",
    "        .agg(count=(\"value\", \"size\"))\n",
    "        .reset_index()\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "    )\n",
    "    print(\"\\nImpossible targets by (domain, variable):\")\n",
    "    for _, r in by_var.iterrows():\n",
    "        print(f\"  {r['domain_variable']}/{r['variable']}: {r['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardest targets (lowest row_sum / target_value ratio):\n",
      "  aca_ptc/aca_ptc (geo=3612): ratio=0.0000, row_sum=5,439, target=376,216,522\n",
      "  aca_ptc/aca_ptc (geo=2508): ratio=0.0000, row_sum=2,024, target=124,980,814\n",
      "  aca_ptc/tax_unit_count (geo=2508): ratio=0.0000, row_sum=1, target=51,937\n",
      "  aca_ptc/tax_unit_count (geo=3612): ratio=0.0000, row_sum=2, target=73,561\n",
      "  aca_ptc/tax_unit_count (geo=1198): ratio=0.0000, row_sum=1, target=30,419\n"
     ]
    }
   ],
   "source": [
    "ratios = row_sums[achievable_mask] / targets_filtered.loc[achievable_mask, \"value\"].values\n",
    "ratio_df = targets_filtered[achievable_mask].copy()\n",
    "ratio_df[\"row_sum\"] = row_sums[achievable_mask]\n",
    "ratio_df[\"ratio\"] = ratios\n",
    "\n",
    "hardest = ratio_df.nsmallest(5, \"ratio\")\n",
    "print(\"Hardest targets (lowest row_sum / target_value ratio):\")\n",
    "for _, r in hardest.iterrows():\n",
    "    print(\n",
    "        f\"  {r.get('domain_variable', '?')}/{r['variable']} \"\n",
    "        f\"(geo={r['geographic_id']}): \"\n",
    "        f\"ratio={r['ratio']:.4f}, \"\n",
    "        f\"row_sum={r['row_sum']:,.0f}, \"\n",
    "        f\"target={r['value']:,.0f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final matrix shape: (1358, 35997)\n",
      "Final non-zero entries: 23,018\n",
      "Final density: 0.000471\n",
      "\n",
      "This is what the optimizer receives.\n"
     ]
    }
   ],
   "source": [
    "X_final = X_filtered[achievable_mask, :]\n",
    "print(f\"Final matrix shape: {X_final.shape}\")\n",
    "print(f\"Final non-zero entries: {X_final.nnz:,}\")\n",
    "print(f\"Final density: {X_final.nnz / (X_final.shape[0] * X_final.shape[1]):.6f}\")\n",
    "print(\"\\nThis is what the optimizer receives.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The calibration matrix pipeline has five steps:\n",
    "\n",
    "1. **Clone + assign** — `assign_random_geography()` creates N clones of each CPS record, each with a random census block (and derived CD/state).\n",
    "2. **Build** — `UnifiedMatrixBuilder.build_matrix()` queries targets, applies hierarchical uprating, simulates each clone with its assigned geography, and assembles the sparse CSR matrix.\n",
    "3. **Groups** — `create_target_groups()` partitions rows for balanced loss weighting. `GROUPS_TO_EXCLUDE` drops redundant constraints.\n",
    "4. **Sparsity** — Most of the matrix is zero. District-level targets confine non-zeros to clones assigned to that district; national targets span all clones.\n",
    "5. **Filter** — Remove impossible targets (row sum = 0) before handing to the optimizer.\n",
    "\n",
    "When adding new domains or variables to the calibration, re-run this notebook to verify the new targets appear correctly and don't introduce impossible constraints."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
