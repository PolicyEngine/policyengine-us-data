{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Calibration Matrix\n",
    "\n",
    "The calibration pipeline has three stages: (1) compute uprated target values ([`hierarchical_uprating.ipynb`](hierarchical_uprating.ipynb)), (2) assemble the sparse constraint matrix (this notebook), and (3) optimize weights ([`fit_calibration_weights.py`](../policyengine_us_data/datasets/cps/local_area_calibration/fit_calibration_weights.py)). This notebook is the diagnostic checkpoint between stages 1 and 2 — understand your matrix before you optimize.\n",
    "\n",
    "We build the full calibration matrix using `SparseMatrixBuilder`, then use `MatrixTracer` to inspect its structure: what rows and columns represent, how target groups partition the loss function, and where sparsity patterns emerge.\n",
    "\n",
    "**Requirements:** `policy_data.db` and the stratified CPS h5 file in `STORAGE_FOLDER`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from policyengine_us import Microsimulation\n",
    "from policyengine_us_data.storage import STORAGE_FOLDER\n",
    "from policyengine_us_data.datasets.cps.local_area_calibration.sparse_matrix_builder import (\n",
    "    SparseMatrixBuilder,\n",
    ")\n",
    "from policyengine_us_data.datasets.cps.local_area_calibration.calibration_utils import (\n",
    "    get_all_cds_from_database,\n",
    "    create_target_groups,\n",
    "    STATE_CODES,\n",
    ")\n",
    "from policyengine_us_data.datasets.cps.local_area_calibration.matrix_tracer import (\n",
    "    MatrixTracer,\n",
    ")\n",
    "\n",
    "db_path = STORAGE_FOLDER / \"calibration\" / \"policy_data.db\"\n",
    "db_uri = f\"sqlite:///{db_path}\"\n",
    "dataset_path = STORAGE_FOLDER / \"stratified_extended_cps_2024.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = Microsimulation(dataset=str(dataset_path))\n",
    "cds_to_calibrate = get_all_cds_from_database(db_uri)\n",
    "\n",
    "builder = SparseMatrixBuilder(\n",
    "    db_uri=db_uri,\n",
    "    time_period=2024,\n",
    "    cds_to_calibrate=cds_to_calibrate,\n",
    "    dataset_path=str(dataset_path),\n",
    ")\n",
    "\n",
    "targets_df, X_sparse, household_id_mapping = builder.build_matrix(\n",
    "    sim,\n",
    "    target_filter={\"domain_variables\": [\"aca_ptc\", \"snap\"]},\n",
    "    hierarchical_domains=[\"aca_ptc\", \"snap\"],\n",
    ")\n",
    "\n",
    "tracer = MatrixTracer(\n",
    "    targets_df, X_sparse, household_id_mapping, cds_to_calibrate, sim\n",
    ")\n",
    "\n",
    "print(f\"Matrix shape: {X_sparse.shape}\")\n",
    "print(f\"Non-zero entries: {X_sparse.nnz:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Matrix overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer.print_matrix_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Anatomy of a row\n",
    "\n",
    "Each row is one calibration target — a known aggregate (dollar total, household count, person count) that the optimizer tries to match. The row vector's non-zero entries identify which (household, CD) pairs can contribute to that target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_row = X_sparse.shape[0] // 2\n",
    "row_info = tracer.get_row_info(mid_row)\n",
    "print(f\"Row {mid_row}:\")\n",
    "for k, v in row_info.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_vec = X_sparse[mid_row, :]\n",
    "nz_cols = row_vec.nonzero()[1]\n",
    "print(f\"Row {mid_row} has {len(nz_cols):,} non-zero columns\")\n",
    "\n",
    "if len(nz_cols) > 0:\n",
    "    first_col = tracer.get_column_info(nz_cols[0])\n",
    "    last_col = tracer.get_column_info(nz_cols[-1])\n",
    "    print(f\"\\nFirst non-zero column ({nz_cols[0]}):\")\n",
    "    for k, v in first_col.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(f\"\\nLast non-zero column ({nz_cols[-1]}):\")\n",
    "    for k, v in last_col.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    unique_cds = set(\n",
    "        tracer.get_column_info(c)[\"cd_geoid\"] for c in nz_cols\n",
    "    )\n",
    "    print(f\"\\nSpans {len(unique_cds)} CD(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Anatomy of a column\n",
    "\n",
    "Each column represents one (household, CD) pair. The columns are organized in blocks: the first `n_households` columns belong to CD 1, the next to CD 2, and so on. The block formula is:\n",
    "\n",
    "$$\\text{column\\_idx} = \\text{cd\\_block} \\times n_{\\text{households}} + \\text{hh\\_index}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_idx = tracer.n_households * 5 + 42\n",
    "col_info = tracer.get_column_info(col_idx)\n",
    "print(f\"Column {col_idx}:\")\n",
    "for k, v in col_info.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "col_vec = X_sparse[:, col_idx]\n",
    "nz_rows = col_vec.nonzero()[0]\n",
    "print(f\"\\nThis column has non-zero values in {len(nz_rows)} target rows\")\n",
    "if len(nz_rows) > 0:\n",
    "    print(\"First 5 target rows:\")\n",
    "    for r in nz_rows[:5]:\n",
    "        ri = tracer.get_row_info(r)\n",
    "        print(\n",
    "            f\"  row {r}: {ri['variable']} \"\n",
    "            f\"(geo={ri['geographic_id']}, \"\n",
    "            f\"val={X_sparse[r, col_idx]:.2f})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_col = 5 * tracer.n_households + 42\n",
    "assert col_idx == expected_col, f\"{col_idx} != {expected_col}\"\n",
    "print(\n",
    "    f\"Block formula verified: \"\n",
    "    f\"cd_block=5 * n_hh={tracer.n_households} + hh_idx=42 = {expected_col}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Target groups and loss weighting\n",
    "\n",
    "Target groups partition the rows by (domain, variable, geographic level). Each group contributes equally to the loss function, so 436 district-level rows don't drown out 1 national row. The group IDs depend on the current database contents and may change if targets are added or removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_groups, group_info = create_target_groups(targets_df)\n",
    "\n",
    "records = []\n",
    "for gid, info in enumerate(group_info):\n",
    "    mask = target_groups == gid\n",
    "    vals = targets_df.loc[mask, \"value\"]\n",
    "    records.append({\n",
    "        \"group_id\": gid,\n",
    "        \"description\": info,\n",
    "        \"n_targets\": mask.sum(),\n",
    "        \"min_value\": vals.min(),\n",
    "        \"median_value\": vals.median(),\n",
    "        \"max_value\": vals.max(),\n",
    "    })\n",
    "\n",
    "group_df = pd.DataFrame(records)\n",
    "print(group_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gid in [0, 2, 4]:\n",
    "    if gid >= len(group_info):\n",
    "        continue\n",
    "    rows = tracer.get_group_rows(gid)\n",
    "    print(f\"\\n--- {group_info[gid]} ---\")\n",
    "    print(rows.head(8).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tracing a household\n",
    "\n",
    "One CPS household appears in every CD block (once per CD = 436 column positions). But most of those columns are zero — the household only contributes where its characteristics match the target constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_values = sim.calculate(\"snap\", map_to=\"household\").values\n",
    "hh_ids = sim.calculate(\"household_id\", map_to=\"household\").values\n",
    "positive_snap = hh_ids[snap_values > 0]\n",
    "example_hh = int(positive_snap[0])\n",
    "print(f\"Example SNAP-receiving household: {example_hh}\")\n",
    "print(f\"SNAP value: ${snap_values[hh_ids == example_hh][0]:,.0f}\")\n",
    "\n",
    "positions = tracer.get_household_column_positions(example_hh)\n",
    "print(f\"Column positions across CDs: {len(positions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_activity = []\n",
    "for cd_geoid, col_pos in positions.items():\n",
    "    col_vec = X_sparse[:, col_pos]\n",
    "    nnz = col_vec.nnz\n",
    "    cd_activity.append({\"cd_geoid\": cd_geoid, \"col_pos\": col_pos, \"nnz\": nnz})\n",
    "\n",
    "cd_df = pd.DataFrame(cd_activity)\n",
    "n_active = (cd_df[\"nnz\"] > 0).sum()\n",
    "n_zero = (cd_df[\"nnz\"] == 0).sum()\n",
    "print(f\"CDs with non-zero entries: {n_active}\")\n",
    "print(f\"CDs with all-zero columns: {n_zero}\")\n",
    "\n",
    "top10 = cd_df.nlargest(10, \"nnz\")\n",
    "print(f\"\\nTop 10 CDs by activity for household {example_hh}:\")\n",
    "for _, r in top10.iterrows():\n",
    "    state_fips = int(r[\"cd_geoid\"]) // 100\n",
    "    abbr = STATE_CODES.get(state_fips, \"??\")\n",
    "    print(f\"  CD {r['cd_geoid']} ({abbr}): {r['nnz']} non-zero rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sparsity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cells = X_sparse.shape[0] * X_sparse.shape[1]\n",
    "density = X_sparse.nnz / total_cells\n",
    "print(f\"Total cells: {total_cells:,}\")\n",
    "print(f\"Non-zero entries: {X_sparse.nnz:,}\")\n",
    "print(f\"Density: {density:.6f}\")\n",
    "print(f\"Sparsity: {1 - density:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnz_per_row = np.diff(X_sparse.indptr)\n",
    "print(f\"Non-zeros per row:\")\n",
    "print(f\"  min:    {nnz_per_row.min():,}\")\n",
    "print(f\"  median: {int(np.median(nnz_per_row)):,}\")\n",
    "print(f\"  mean:   {nnz_per_row.mean():,.0f}\")\n",
    "print(f\"  max:    {nnz_per_row.max():,}\")\n",
    "\n",
    "from policyengine_us_data.datasets.cps.local_area_calibration.calibration_utils import (\n",
    "    _get_geo_level,\n",
    ")\n",
    "\n",
    "geo_levels = targets_df[\"geographic_id\"].apply(_get_geo_level)\n",
    "level_names = {0: \"National\", 1: \"State\", 2: \"District\"}\n",
    "print(\"\\nBy geographic level:\")\n",
    "for level in [0, 1, 2]:\n",
    "    mask = (geo_levels == level).values\n",
    "    if mask.any():\n",
    "        vals = nnz_per_row[mask]\n",
    "        print(\n",
    "            f\"  {level_names[level]:10s}: \"\n",
    "            f\"n={mask.sum():>4d}, \"\n",
    "            f\"median nnz={int(np.median(vals)):>7,}, \"\n",
    "            f\"range=[{vals.min():,}, {vals.max():,}]\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hh = tracer.n_households\n",
    "n_cds = tracer.n_geographies\n",
    "cd_nnz = []\n",
    "for cd_idx in range(n_cds):\n",
    "    block = X_sparse[:, cd_idx * n_hh : (cd_idx + 1) * n_hh]\n",
    "    cd_nnz.append({\n",
    "        \"cd_geoid\": cds_to_calibrate[cd_idx],\n",
    "        \"nnz\": block.nnz,\n",
    "    })\n",
    "\n",
    "cd_nnz_df = pd.DataFrame(cd_nnz)\n",
    "print(f\"Non-zeros per CD block:\")\n",
    "print(f\"  min:    {cd_nnz_df['nnz'].min():,} (CD {cd_nnz_df.loc[cd_nnz_df['nnz'].idxmin(), 'cd_geoid']})\")\n",
    "print(f\"  median: {int(cd_nnz_df['nnz'].median()):,}\")\n",
    "print(f\"  max:    {cd_nnz_df['nnz'].max():,} (CD {cd_nnz_df.loc[cd_nnz_df['nnz'].idxmax(), 'cd_geoid']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Group exclusion\n",
    "\n",
    "`GROUPS_TO_EXCLUDE` removes redundant or harmful constraints before training. For example, state-level SNAP household counts (Group 1) are redundant with reconciled district rows (Group 4) and can confuse the optimizer. Group IDs depend on database contents, so always check `print_matrix_structure()` output first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUPS_TO_EXCLUDE = [1]\n",
    "\n",
    "print(f\"Before exclusion: {X_sparse.shape[0]} rows\")\n",
    "\n",
    "keep_mask = ~np.isin(tracer.target_groups, GROUPS_TO_EXCLUDE)\n",
    "n_dropped = (~keep_mask).sum()\n",
    "print(f\"Excluding groups {GROUPS_TO_EXCLUDE}: dropping {n_dropped} rows\")\n",
    "\n",
    "X_filtered = X_sparse[keep_mask, :]\n",
    "targets_filtered = targets_df[keep_mask].reset_index(drop=True)\n",
    "print(f\"After exclusion: {X_filtered.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_groups, remaining_info = create_target_groups(targets_filtered)\n",
    "print(f\"\\nRemaining groups ({len(remaining_info)}):\")\n",
    "for info in remaining_info:\n",
    "    print(f\"  {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Achievable targets\n",
    "\n",
    "A target is achievable if at least one household can contribute to it (row sum > 0). Rows with sum = 0 are impossible constraints that the optimizer cannot satisfy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = np.array(X_filtered.sum(axis=1)).flatten()\n",
    "achievable_mask = row_sums > 0\n",
    "n_achievable = achievable_mask.sum()\n",
    "n_impossible = (~achievable_mask).sum()\n",
    "\n",
    "print(f\"Achievable targets: {n_achievable}\")\n",
    "print(f\"Impossible targets: {n_impossible}\")\n",
    "\n",
    "if n_impossible > 0:\n",
    "    impossible = targets_filtered[~achievable_mask]\n",
    "    print(\"\\nImpossible targets:\")\n",
    "    for _, r in impossible.iterrows():\n",
    "        print(\n",
    "            f\"  {r.get('domain_variable', '?')}/{r['variable']} \"\n",
    "            f\"(geo={r['geographic_id']})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = row_sums[achievable_mask] / targets_filtered.loc[achievable_mask, \"value\"].values\n",
    "ratio_df = targets_filtered[achievable_mask].copy()\n",
    "ratio_df[\"row_sum\"] = row_sums[achievable_mask]\n",
    "ratio_df[\"ratio\"] = ratios\n",
    "\n",
    "hardest = ratio_df.nsmallest(5, \"ratio\")\n",
    "print(\"Hardest targets (lowest row_sum / target_value ratio):\")\n",
    "for _, r in hardest.iterrows():\n",
    "    print(\n",
    "        f\"  {r.get('domain_variable', '?')}/{r['variable']} \"\n",
    "        f\"(geo={r['geographic_id']}): \"\n",
    "        f\"ratio={r['ratio']:.4f}, \"\n",
    "        f\"row_sum={r['row_sum']:,.0f}, \"\n",
    "        f\"target={r['value']:,.0f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = X_filtered[achievable_mask, :]\n",
    "print(f\"Final matrix shape: {X_final.shape}\")\n",
    "print(f\"Final non-zero entries: {X_final.nnz:,}\")\n",
    "print(f\"Final density: {X_final.nnz / (X_final.shape[0] * X_final.shape[1]):.6f}\")\n",
    "print(\"\\nThis is what the optimizer receives.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The calibration matrix pipeline has five steps:\n",
    "\n",
    "1. **Build** — `SparseMatrixBuilder.build_matrix()` queries targets, applies hierarchical uprating, evaluates constraints, and assembles the sparse CSR matrix.\n",
    "2. **Read** — `MatrixTracer` decodes rows (targets) and columns (household-CD pairs) so you can verify the matrix makes sense.\n",
    "3. **Groups** — `create_target_groups()` partitions rows for balanced loss weighting. `GROUPS_TO_EXCLUDE` drops redundant constraints.\n",
    "4. **Sparsity** — Most of the matrix is zero. District-level targets confine non-zeros to single CD blocks; national targets span all blocks.\n",
    "5. **Filter** — Remove impossible targets (row sum = 0) before handing to the optimizer.\n",
    "\n",
    "When adding new domains or variables to the calibration, re-run this notebook to verify the new targets appear correctly and don't introduce impossible constraints."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}