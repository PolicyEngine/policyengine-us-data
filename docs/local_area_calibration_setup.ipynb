{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Local Area Calibration Setup\n",
    "\n",
    "This notebook demonstrates the clone-based calibration pipeline: how raw CPS records become a calibration matrix and, ultimately, CD-level stacked datasets.\n",
    "\n",
    "The paradigm shift from the old approach: instead of replicating every household into every congressional district, we **clone** each record N times and assign each clone a **random census block** drawn from a population-weighted distribution. Each clone inherits a state, CD, and block — and gets re-simulated under the rules of its assigned state.\n",
    "\n",
    "We follow one household (`record_idx=23`, household_id 654, SNAP \\$70) through the entire pipeline:\n",
    "1. Clone and assign geography\n",
    "2. Simulate under new state rules (`_simulate_clone`)\n",
    "3. Geographic column masking\n",
    "4. Re-randomize takeup per census block\n",
    "5. Build the calibration matrix\n",
    "6. Create stacked datasets from calibrated weights\n",
    "\n",
    "**Companion notebook:** [calibration_matrix.ipynb](calibration_matrix.ipynb) covers the *finished* matrix — row/column anatomy, target groups, sparsity. This notebook covers the *process* that creates it and what happens after (stacked datasets).\n",
    "\n",
    "**Requirements:** `policy_data.db`, `block_cd_distributions.csv.gz`, and the stratified CPS h5 file in `STORAGE_FOLDER`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baogorek/envs/sep/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from policyengine_us import Microsimulation\n",
    "from policyengine_us_data.storage import STORAGE_FOLDER\n",
    "from policyengine_us_data.calibration.clone_and_assign import (\n",
    "    assign_random_geography,\n",
    "    GeographyAssignment,\n",
    "    load_global_block_distribution,\n",
    ")\n",
    "from policyengine_us_data.calibration.unified_matrix_builder import (\n",
    "    UnifiedMatrixBuilder,\n",
    ")\n",
    "from policyengine_us_data.calibration.unified_calibration import (\n",
    "    rerandomize_takeup,\n",
    "    SIMPLE_TAKEUP_VARS,\n",
    ")\n",
    "from policyengine_us_data.utils.randomness import seeded_rng\n",
    "from policyengine_us_data.parameters import load_take_up_rate\n",
    "from policyengine_us_data.datasets.cps.local_area_calibration.calibration_utils import (\n",
    "    get_calculated_variables,\n",
    "    STATE_CODES,\n",
    "    get_all_cds_from_database,\n",
    ")\n",
    "from policyengine_us_data.datasets.cps.local_area_calibration.stacked_dataset_builder import (\n",
    "    create_sparse_cd_stacked_dataset,\n",
    ")\n",
    "\n",
    "db_path = STORAGE_FOLDER / \"calibration\" / \"policy_data.db\"\n",
    "db_uri = f\"sqlite:///{db_path}\"\n",
    "dataset_path = str(STORAGE_FOLDER / \"stratified_extended_cps_2024.h5\")\n",
    "\n",
    "N_CLONES = 3\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base dataset: 11,999 households\n",
      "Example household: record_idx=23, household_id=654, SNAP=$70.08\n"
     ]
    }
   ],
   "source": [
    "sim = Microsimulation(dataset=dataset_path)\n",
    "hh_ids = sim.calculate(\"household_id\", map_to=\"household\").values\n",
    "snap_values = sim.calculate(\"snap\", map_to=\"household\").values\n",
    "n_records = len(hh_ids)\n",
    "\n",
    "record_idx = int(np.where(snap_values > 0)[0][0])\n",
    "example_hh_id = hh_ids[record_idx]\n",
    "print(f\"Base dataset: {n_records:,} households\")\n",
    "print(\n",
    "    f\"Example household: record_idx={record_idx}, \"\n",
    "    f\"household_id={example_hh_id}, \"\n",
    "    f\"SNAP=${snap_values[record_idx]:,.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Section 2: Geography Assignment\n",
    "\n",
    "`assign_random_geography` creates `n_records * n_clones` total records, each assigned a random census block from a population-weighted distribution. State and CD are derived from the block GEOID. The result is a `GeographyAssignment` dataclass with arrays indexed as `clone_idx * n_records + record_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cloned records: 35,997\n",
      "Unique states: 50\n",
      "Unique CDs: 435\n",
      "Unique blocks: 35508\n"
     ]
    }
   ],
   "source": [
    "geography = assign_random_geography(n_records, n_clones=N_CLONES, seed=SEED)\n",
    "n_total = n_records * N_CLONES\n",
    "\n",
    "print(f\"Total cloned records: {n_total:,}\")\n",
    "print(f\"Unique states: {len(np.unique(geography.state_fips))}\")\n",
    "print(f\"Unique CDs: {len(np.unique(geography.cd_geoid))}\")\n",
    "print(f\"Unique blocks: {len(np.unique(geography.block_geoid))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example household (record_idx=23) across 3 clones:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clone</th>\n",
       "      <th>col</th>\n",
       "      <th>state_fips</th>\n",
       "      <th>abbr</th>\n",
       "      <th>cd_geoid</th>\n",
       "      <th>block_geoid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>48</td>\n",
       "      <td>TX</td>\n",
       "      <td>4829</td>\n",
       "      <td>482013214012002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12022</td>\n",
       "      <td>17</td>\n",
       "      <td>IL</td>\n",
       "      <td>1708</td>\n",
       "      <td>170318043051011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>24021</td>\n",
       "      <td>12</td>\n",
       "      <td>FL</td>\n",
       "      <td>1220</td>\n",
       "      <td>120110503112005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clone    col  state_fips abbr cd_geoid      block_geoid\n",
       "0      0     23          48   TX     4829  482013214012002\n",
       "1      1  12022          17   IL     1708  170318043051011\n",
       "2      2  24021          12   FL     1220  120110503112005"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\n",
    "    f\"Example household (record_idx={record_idx}) across {N_CLONES} clones:\\n\"\n",
    ")\n",
    "rows = []\n",
    "for c in range(N_CLONES):\n",
    "    col = c * n_records + record_idx\n",
    "    rows.append(\n",
    "        {\n",
    "            \"clone\": c,\n",
    "            \"col\": col,\n",
    "            \"state_fips\": geography.state_fips[col],\n",
    "            \"abbr\": STATE_CODES.get(geography.state_fips[col], \"??\"),\n",
    "            \"cd_geoid\": geography.cd_geoid[col],\n",
    "            \"block_geoid\": geography.block_geoid[col],\n",
    "        }\n",
    "    )\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "One household, three parallel geographic identities. Each clone will be simulated under different state rules, producing different benefit amounts.\n",
    "\n",
    "**Note:** With only N_CLONES=3 (~36K total samples), small-population areas like DC may not appear in the random draw. The production pipeline uses N_CLONES=10, which covers all 51 state-equivalents and 436 CDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global block distribution: 5,765,442 blocks\n",
      "Top 5 states by total probability:\n",
      "  CA (6): 11.954%\n",
      "  TX (48): 8.736%\n",
      "  FL (12): 6.437%\n",
      "  NY (36): 5.977%\n",
      "  PA (42): 3.908%\n"
     ]
    }
   ],
   "source": [
    "blocks, cds, states, probs = load_global_block_distribution()\n",
    "print(f\"Global block distribution: {len(blocks):,} blocks\")\n",
    "print(f\"Top 5 states by total probability:\")\n",
    "state_prob = pd.Series(probs, index=states).groupby(level=0).sum()\n",
    "top5 = state_prob.nlargest(5)\n",
    "for fips, p in top5.items():\n",
    "    print(f\"  {STATE_CODES.get(fips, '??')} ({fips}): {p:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Section 3: Inside `_simulate_clone` — State-Swap\n",
    "\n",
    "For each clone, `_simulate_clone` does four things:\n",
    "1. Creates a **fresh** `Microsimulation` from the base dataset\n",
    "2. Overwrites `state_fips` with the clone's assigned states\n",
    "3. Optionally calls a `sim_modifier` (e.g., takeup re-randomization)\n",
    "4. **Clears cached formulas** via `get_calculated_variables` — preserving survey inputs and IDs while forcing recalculation of state-dependent variables like SNAP\n",
    "\n",
    "Let's reproduce this manually for clone 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example household (record_idx=23):\n",
      "  Original state: ME (23)\n",
      "  Clone 0 state:  TX (48)\n",
      "  Original SNAP:  $70.08\n",
      "  Clone 0 SNAP:   $0.00\n"
     ]
    }
   ],
   "source": [
    "clone_idx = 0\n",
    "col_start = clone_idx * n_records\n",
    "col_end = col_start + n_records\n",
    "clone_states = geography.state_fips[col_start:col_end]\n",
    "\n",
    "clone_sim = Microsimulation(dataset=dataset_path)\n",
    "clone_sim.set_input(\"state_fips\", 2024, clone_states.astype(np.int32))\n",
    "for var in get_calculated_variables(clone_sim):\n",
    "    clone_sim.delete_arrays(var)\n",
    "\n",
    "new_snap = clone_sim.calculate(\"snap\", map_to=\"household\").values\n",
    "\n",
    "orig_state = sim.calculate(\"state_fips\", map_to=\"household\").values[record_idx]\n",
    "new_state = clone_states[record_idx]\n",
    "\n",
    "print(f\"Example household (record_idx={record_idx}):\")\n",
    "print(\n",
    "    f\"  Original state: {STATE_CODES.get(int(orig_state), '??')} \"\n",
    "    f\"({int(orig_state)})\"\n",
    ")\n",
    "print(\n",
    "    f\"  Clone 0 state:  {STATE_CODES.get(int(new_state), '??')} \"\n",
    "    f\"({int(new_state)})\"\n",
    ")\n",
    "print(f\"  Original SNAP:  ${snap_values[record_idx]:,.2f}\")\n",
    "print(f\"  Clone 0 SNAP:   ${new_snap[record_idx]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNAP for record_idx=23 across all 3 clones:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clone</th>\n",
       "      <th>state</th>\n",
       "      <th>state_fips</th>\n",
       "      <th>SNAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>TX</td>\n",
       "      <td>48</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>IL</td>\n",
       "      <td>17</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>FL</td>\n",
       "      <td>12</td>\n",
       "      <td>$70.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clone state  state_fips    SNAP\n",
       "0      0    TX          48   $0.00\n",
       "1      1    IL          17   $0.00\n",
       "2      2    FL          12  $70.08"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"SNAP for record_idx={record_idx} across all {N_CLONES} clones:\\n\")\n",
    "rows = []\n",
    "for c in range(N_CLONES):\n",
    "    cs = geography.state_fips[c * n_records + record_idx]\n",
    "    s = Microsimulation(dataset=dataset_path)\n",
    "    s.set_input(\n",
    "        \"state_fips\",\n",
    "        2024,\n",
    "        geography.state_fips[c * n_records : (c + 1) * n_records].astype(\n",
    "            np.int32\n",
    "        ),\n",
    "    )\n",
    "    for var in get_calculated_variables(s):\n",
    "        s.delete_arrays(var)\n",
    "    clone_snap = s.calculate(\"snap\", map_to=\"household\").values\n",
    "    rows.append(\n",
    "        {\n",
    "            \"clone\": c,\n",
    "            \"state\": STATE_CODES.get(int(cs), \"??\"),\n",
    "            \"state_fips\": int(cs),\n",
    "            \"SNAP\": f\"${clone_snap[record_idx]:,.2f}\",\n",
    "        }\n",
    "    )\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "`get_calculated_variables` is selective: it identifies variables with formulas (state-dependent computations) while preserving survey-reported inputs and entity IDs. This is what allows the same demographic household to produce different benefit amounts under different state rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Section 4: Geographic Column Masking\n",
    "\n",
    "When assembling the calibration matrix, each target row only \"sees\" columns (clones) whose geography matches the target's geography. This is implemented via `state_to_cols` and `cd_to_cols` dictionaries built from the `GeographyAssignment`.\n",
    "\n",
    "This is step 3 of `build_matrix` — reproduced here for transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique states mapped: 50\n",
      "Unique CDs mapped: 435\n",
      "\n",
      "Columns per state: min=62, median=494, max=4311\n"
     ]
    }
   ],
   "source": [
    "state_col_lists = defaultdict(list)\n",
    "cd_col_lists = defaultdict(list)\n",
    "for col in range(n_total):\n",
    "    state_col_lists[int(geography.state_fips[col])].append(col)\n",
    "    cd_col_lists[str(geography.cd_geoid[col])].append(col)\n",
    "\n",
    "state_to_cols = {s: np.array(c) for s, c in state_col_lists.items()}\n",
    "cd_to_cols = {cd: np.array(c) for cd, c in cd_col_lists.items()}\n",
    "\n",
    "print(f\"Unique states mapped: {len(state_to_cols)}\")\n",
    "print(f\"Unique CDs mapped: {len(cd_to_cols)}\")\n",
    "\n",
    "state_counts = {s: len(c) for s, c in state_to_cols.items()}\n",
    "sc_series = pd.Series(state_counts)\n",
    "print(\n",
    "    f\"\\nColumns per state: min={sc_series.min()}, \"\n",
    "    f\"median={sc_series.median():.0f}, max={sc_series.max()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example household clone visibility:\n",
      "\n",
      "Clone 0 (TX, CD 4829):\n",
      "  Visible to TX state targets: col 23 in state_to_cols[48]? True\n",
      "  Visible to CD 4829 targets: col 23 in cd_to_cols['4829']? True\n",
      "  Visible to NC (37) targets: False\n",
      "\n",
      "Clone 1 (IL, CD 1708):\n",
      "  Visible to IL state targets: col 12022 in state_to_cols[17]? True\n",
      "  Visible to CD 1708 targets: col 12022 in cd_to_cols['1708']? True\n",
      "  Visible to NC (37) targets: False\n",
      "\n",
      "Clone 2 (FL, CD 1220):\n",
      "  Visible to FL state targets: col 24021 in state_to_cols[12]? True\n",
      "  Visible to CD 1220 targets: col 24021 in cd_to_cols['1220']? True\n",
      "  Visible to NC (37) targets: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Example household clone visibility:\\n\")\n",
    "for c in range(N_CLONES):\n",
    "    col = c * n_records + record_idx\n",
    "    state = int(geography.state_fips[col])\n",
    "    cd = str(geography.cd_geoid[col])\n",
    "    abbr = STATE_CODES.get(state, \"??\")\n",
    "    print(f\"Clone {c} ({abbr}, CD {cd}):\")\n",
    "    print(\n",
    "        f\"  Visible to {abbr} state targets: \"\n",
    "        f\"col {col} in state_to_cols[{state}]? \"\n",
    "        f\"{col in state_to_cols.get(state, [])}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Visible to CD {cd} targets: \"\n",
    "        f\"col {col} in cd_to_cols['{cd}']? \"\n",
    "        f\"{col in cd_to_cols.get(cd, [])}\"\n",
    "    )\n",
    "    # Check an unrelated state\n",
    "    print(\n",
    "        f\"  Visible to NC (37) targets: \" f\"{col in state_to_cols.get(37, [])}\"\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "This is the mechanism behind the sparsity pattern in `calibration_matrix.ipynb`: a household clone assigned to TX can contribute to TX state targets and TX CD targets, but produces a zero entry for NC or AK targets. The matrix is sparse because each clone only intersects a small fraction of all geographic targets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Section 5: Takeup Re-randomization\n",
    "\n",
    "The base CPS has fixed takeup decisions (e.g., \"this household takes up SNAP\"). But when we clone a household into different census blocks, each block should have independently drawn takeup — otherwise every clone of a SNAP-participating household would still participate, regardless of geography.\n",
    "\n",
    "`rerandomize_takeup` solves this: for each census block, it uses `seeded_rng(variable_name, salt=block_geoid)` to draw new takeup booleans. The seed is deterministic per (variable, block) pair, so results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 takeup variables:\n",
      "\n",
      "  takes_up_snap_if_eligible                entity=spm_unit   rate=82.00%\n",
      "  takes_up_aca_if_eligible                 entity=tax_unit   rate=67.20%\n",
      "  takes_up_dc_ptc                          entity=tax_unit   rate=32.00%\n",
      "  takes_up_head_start_if_eligible          entity=person     rate=30.00%\n",
      "  takes_up_early_head_start_if_eligible    entity=person     rate=9.00%\n",
      "  takes_up_ssi_if_eligible                 entity=person     rate=50.00%\n",
      "  would_file_taxes_voluntarily             entity=tax_unit   rate=5.00%\n",
      "  takes_up_medicaid_if_eligible            entity=person     rate=dict (51 entries)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(SIMPLE_TAKEUP_VARS)} takeup variables:\\n\")\n",
    "for spec in SIMPLE_TAKEUP_VARS:\n",
    "    rate_key = spec[\"rate_key\"]\n",
    "    if rate_key == \"voluntary_filing\":\n",
    "        rate = 0.05\n",
    "    else:\n",
    "        rate = load_take_up_rate(rate_key, 2024)\n",
    "    rate_str = (\n",
    "        f\"{rate:.2%}\"\n",
    "        if isinstance(rate, float)\n",
    "        else f\"dict ({len(rate)} entries)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  {spec['variable']:40s} \"\n",
    "        f\"entity={spec['entity']:10s} rate={rate_str}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same block + same var (reproducible):\n",
      "  [0.50514599 0.75213437 0.9703409  0.18048868 0.31969517]\n",
      "  [0.50514599 0.75213437 0.9703409  0.18048868 0.31969517]\n",
      "  Match: True\n",
      "\n",
      "Different block, same var:\n",
      "  [0.15503168 0.96707026 0.79019745 0.67544525 0.85245009]\n",
      "  Match: False\n",
      "\n",
      "Same block, different var:\n",
      "  [0.93155876 0.8912794  0.50838888 0.32192278 0.01005173]\n",
      "  Match: False\n"
     ]
    }
   ],
   "source": [
    "block_a = \"482011234567890\"\n",
    "block_b = \"170311234567890\"\n",
    "var = \"takes_up_snap_if_eligible\"\n",
    "\n",
    "rng_a1 = seeded_rng(var, salt=block_a)\n",
    "rng_a2 = seeded_rng(var, salt=block_a)\n",
    "rng_b = seeded_rng(var, salt=block_b)\n",
    "rng_other = seeded_rng(\"takes_up_aca_if_eligible\", salt=block_a)\n",
    "\n",
    "draws_a1 = rng_a1.random(5)\n",
    "draws_a2 = rng_a2.random(5)\n",
    "draws_b = rng_b.random(5)\n",
    "draws_other = rng_other.random(5)\n",
    "\n",
    "print(\"Same block + same var (reproducible):\")\n",
    "print(f\"  {draws_a1}\")\n",
    "print(f\"  {draws_a2}\")\n",
    "print(f\"  Match: {np.allclose(draws_a1, draws_a2)}\")\n",
    "print(f\"\\nDifferent block, same var:\")\n",
    "print(f\"  {draws_b}\")\n",
    "print(f\"  Match: {np.allclose(draws_a1, draws_b)}\")\n",
    "print(f\"\\nSame block, different var:\")\n",
    "print(f\"  {draws_other}\")\n",
    "print(f\"  Match: {np.allclose(draws_a1, draws_other)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Takeup rates before/after re-randomization (clone 0):\n",
      "\n",
      "  takes_up_snap_if_eligible                before=82.333%  after=82.381%\n",
      "  takes_up_aca_if_eligible                 before=66.718%  after=67.486%\n",
      "  takes_up_dc_ptc                          before=31.483%  after=32.044%\n",
      "  takes_up_head_start_if_eligible          before=29.963%  after=29.689%\n",
      "  takes_up_early_head_start_if_eligible    before=8.869%  after=8.721%\n",
      "  takes_up_ssi_if_eligible                 before=100.000%  after=49.776%\n",
      "  would_file_taxes_voluntarily             before=0.000%  after=4.905%\n",
      "  takes_up_medicaid_if_eligible            before=84.496%  after=80.051%\n"
     ]
    }
   ],
   "source": [
    "test_sim = Microsimulation(dataset=dataset_path)\n",
    "clone_0_states = geography.state_fips[:n_records]\n",
    "clone_0_blocks = geography.block_geoid[:n_records]\n",
    "test_sim.set_input(\"state_fips\", 2024, clone_0_states.astype(np.int32))\n",
    "\n",
    "before = {}\n",
    "for spec in SIMPLE_TAKEUP_VARS:\n",
    "    v = spec[\"variable\"]\n",
    "    vals = test_sim.calculate(v, map_to=spec[\"entity\"]).values\n",
    "    before[v] = vals.mean()\n",
    "\n",
    "rerandomize_takeup(test_sim, clone_0_blocks, clone_0_states, 2024)\n",
    "\n",
    "print(\"Takeup rates before/after re-randomization (clone 0):\\n\")\n",
    "for spec in SIMPLE_TAKEUP_VARS:\n",
    "    v = spec[\"variable\"]\n",
    "    vals = test_sim.calculate(v, map_to=spec[\"entity\"]).values\n",
    "    after = vals.mean()\n",
    "    print(f\"  {v:40s} before={before[v]:.3%}  after={after:.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medicaid takeup rates (state-specific), first 10 states:\n",
      "\n",
      "  AK: 88.00%\n",
      "  AL: 92.00%\n",
      "  AR: 79.00%\n",
      "  AZ: 95.00%\n",
      "  CA: 78.00%\n",
      "  CO: 99.00%\n",
      "  CT: 89.00%\n",
      "  DC: 99.00%\n",
      "  DE: 86.00%\n",
      "  FL: 98.00%\n"
     ]
    }
   ],
   "source": [
    "medicaid_rates = load_take_up_rate(\"medicaid\", 2024)\n",
    "print(\"Medicaid takeup rates (state-specific), first 10 states:\\n\")\n",
    "for state, rate in sorted(medicaid_rates.items())[:10]:\n",
    "    print(f\"  {state}: {rate:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "In the full pipeline, `rerandomize_takeup` is passed to `build_matrix` as a `sim_modifier` callback. For each clone, after `state_fips` is set but before formula caches are cleared, the callback draws new takeup booleans per census block. This means the same household in block A might take up SNAP while in block B it doesn't — matching the statistical reality that takeup varies by geography."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Section 6: Matrix Build Verification\n",
    "\n",
    "Let's run the full `build_matrix` pipeline and verify the example household's pattern matches our Section 4 predictions. We use the same `target_filter` as in `calibration_matrix.ipynb` but *without* `sim_modifier` to match that notebook's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-13 11:59:46,739 - INFO - Processing clone 1/3 (cols 0-11998, 50 unique states)...\n",
      "2026-02-13 11:59:47,871 - INFO - Processing clone 2/3 (cols 11999-23997, 50 unique states)...\n",
      "2026-02-13 11:59:49,000 - INFO - Processing clone 3/3 (cols 23998-35996, 50 unique states)...\n",
      "2026-02-13 11:59:50,123 - INFO - Assembling matrix from 3 clones...\n",
      "2026-02-13 11:59:50,124 - INFO - Matrix: 538 targets x 35997 cols, 14946 nnz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (538, 35997)\n",
      "Non-zero entries: 14,946\n",
      "Density: 0.000772\n"
     ]
    }
   ],
   "source": [
    "builder = UnifiedMatrixBuilder(\n",
    "    db_uri=db_uri,\n",
    "    time_period=2024,\n",
    "    dataset_path=dataset_path,\n",
    ")\n",
    "\n",
    "targets_df, X_sparse, target_names = builder.build_matrix(\n",
    "    geography,\n",
    "    sim,\n",
    "    target_filter={\"domain_variables\": [\"snap\"]},\n",
    ")\n",
    "\n",
    "print(f\"Matrix shape: {X_sparse.shape}\")\n",
    "print(f\"Non-zero entries: {X_sparse.nnz:,}\")\n",
    "print(f\"Density: {X_sparse.nnz / (X_sparse.shape[0] * X_sparse.shape[1]):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example household non-zero pattern across clones:\n",
      "\n",
      "Clone 0 (TX, CD 4829): 0 non-zero rows\n",
      "Clone 1 (IL, CD 1708): 0 non-zero rows\n",
      "Clone 2 (FL, CD 1220): 3 non-zero rows\n",
      "  row 3: household_count (geo=12): 1.00\n",
      "  row 54: snap (geo=12): 70.08\n",
      "  row 130: household_count (geo=1220): 1.00\n"
     ]
    }
   ],
   "source": [
    "print(f\"Example household non-zero pattern across clones:\\n\")\n",
    "for c in range(N_CLONES):\n",
    "    col = c * n_records + record_idx\n",
    "    col_vec = X_sparse[:, col]\n",
    "    nz_rows = col_vec.nonzero()[0]\n",
    "    state = int(geography.state_fips[col])\n",
    "    cd = geography.cd_geoid[col]\n",
    "    abbr = STATE_CODES.get(state, \"??\")\n",
    "    print(f\"Clone {c} ({abbr}, CD {cd}): {len(nz_rows)} non-zero rows\")\n",
    "    for r in nz_rows:\n",
    "        row = targets_df.iloc[r]\n",
    "        print(\n",
    "            f\"  row {r}: {row['variable']} \"\n",
    "            f\"(geo={row['geographic_id']}): \"\n",
    "            f\"{X_sparse[r, col]:.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Section 7: From Weights to Datasets\n",
    "\n",
    "`create_sparse_cd_stacked_dataset` takes calibrated weights and builds an h5 file with only the non-zero-weight households, reindexed per CD. Internally it does its own state-swap simulation — loading the base dataset, assigning `state_fips` for the target CD's state, and recalculating benefits from scratch. This means SNAP values in the output reflect the destination state's rules (e.g., a $70 SNAP household from ME may get $0 under AK rules).\n",
    "\n",
    "**Format gap:** The calibration produces weights in clone layout `(n_records * n_clones,)` where each clone maps to one specific CD via the `GeographyAssignment`. The stacked dataset builder expects CD layout `(n_cds * n_households,)` where every CD has a weight slot for every household. Converting between these — accumulating clone weights into their assigned CDs — is a separate step not yet implemented. The demo below constructs artificial CD-layout weights directly to show how the builder works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension mismatch:\n",
      "  Calibration output: (11999 * 3,) = 35,997 (clone layout)\n",
      "  Stacked builder expects: (436 * 11999,) = 5,231,564 (CD layout)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimension mismatch:\")\n",
    "print(\n",
    "    f\"  Calibration output: ({n_records} * {N_CLONES},) \"\n",
    "    f\"= {n_records * N_CLONES:,} (clone layout)\"\n",
    ")\n",
    "\n",
    "all_cds = get_all_cds_from_database(db_uri)\n",
    "n_cds = len(all_cds)\n",
    "print(\n",
    "    f\"  Stacked builder expects: ({n_cds} * {n_records},) \"\n",
    "    f\"= {n_cds * n_records:,} (CD layout)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight vector: 23,998 entries (2 CDs x 11,999 HH)\n",
      "Non-zero weights: 277\n",
      "Example HH weight in CD 3701: 2.5\n",
      "Example HH weight in CD 201: 3.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "demo_cds = [\"3701\", \"201\"]\n",
    "n_demo_cds = len(demo_cds)\n",
    "\n",
    "w = (\n",
    "    np.random.default_rng(42)\n",
    "    .binomial(n=1, p=0.01, size=n_demo_cds * n_records)\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "# Seed our example household into both CDs\n",
    "cd_idx_3701 = demo_cds.index(\"3701\")\n",
    "w[cd_idx_3701 * n_records + record_idx] = 2.5\n",
    "\n",
    "cd_idx_201 = demo_cds.index(\"201\")\n",
    "w[cd_idx_201 * n_records + record_idx] = 3.5\n",
    "\n",
    "output_dir = \"calibration_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"results.h5\")\n",
    "\n",
    "print(\n",
    "    f\"Weight vector: {len(w):,} entries \"\n",
    "    f\"({n_demo_cds} CDs x {n_records:,} HH)\"\n",
    ")\n",
    "print(f\"Non-zero weights: {(w > 0).sum()}\")\n",
    "print(\n",
    "    f\"Example HH weight in CD 3701: {w[cd_idx_3701 * n_records + record_idx]}\"\n",
    ")\n",
    "print(f\"Example HH weight in CD 201: {w[cd_idx_201 * n_records + record_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subset of 2 CDs: 3701, 201...\n",
      "Output path: calibration_output/results.h5\n",
      "\n",
      "Original dataset has 11,999 households\n",
      "Extracted weights for 2 CDs from full weight matrix\n",
      "Total active household-CD pairs: 277\n",
      "Total weight in W matrix: 281\n",
      "Processing CD 201 (2/2)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-13 12:00:01,947 - INFO - HTTP Request: GET https://huggingface.co/api/models/policyengine/policyengine-us-data \"HTTP/1.1 200 OK\"\n",
      "2026-02-13 12:00:02,000 - INFO - HTTP Request: HEAD https://huggingface.co/policyengine/policyengine-us-data/resolve/main/enhanced_cps_2024.h5 \"HTTP/1.1 302 Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining 2 CD DataFrames...\n",
      "Total households across all CDs: 277\n",
      "Combined DataFrame shape: (714, 222)\n",
      "\n",
      "Reindexing all entity IDs using 25k ranges per CD...\n",
      "  Created 277 unique households across 2 CDs\n",
      "  Reindexing persons using 25k ranges...\n",
      "  Reindexing tax units...\n",
      "  Reindexing SPM units...\n",
      "  Reindexing marital units...\n",
      "  Reindexing families...\n",
      "  Final persons: 714\n",
      "  Final households: 277\n",
      "  Final tax units: 373\n",
      "  Final SPM units: 291\n",
      "  Final marital units: 574\n",
      "  Final families: 309\n",
      "\n",
      "Weights in combined_df AFTER reindexing:\n",
      "  HH weight sum: 0.00M\n",
      "  Person weight sum: 0.00M\n",
      "  Ratio: 1.00\n",
      "\n",
      "Overflow check:\n",
      "  Max person ID after reindexing: 5,025,329\n",
      "  Max person ID × 100: 502,532,900\n",
      "  int32 max: 2,147,483,647\n",
      "  ✓ No overflow risk!\n",
      "\n",
      "Creating Dataset from combined DataFrame...\n",
      "Building simulation from Dataset...\n",
      "\n",
      "Saving to calibration_output/results.h5...\n",
      "Found 175 input variables to save\n",
      "Variables saved: 218\n",
      "Variables skipped: 3763\n",
      "Sparse CD-stacked dataset saved successfully!\n",
      "Household mapping saved to calibration_output/mappings/results_household_mapping.csv\n",
      "\n",
      "Verifying saved file...\n",
      "  Final households: 277\n",
      "  Final persons: 714\n",
      "  Total population (from household weights): 281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'calibration_output/results.h5'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_sparse_cd_stacked_dataset(\n",
    "    w,\n",
    "    demo_cds,\n",
    "    cd_subset=demo_cds,\n",
    "    dataset_path=dataset_path,\n",
    "    output_path=output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked dataset: 277 households\n",
      "\n",
      "Example household (original_id=654) in mapping:\n",
      "\n",
      " new_household_id  original_household_id  congressional_district  state_fips\n",
      "                2                    654                     201           2\n",
      "            25000                    654                    3701          37\n",
      "\n",
      "In stacked dataset:\n",
      "\n",
      " household_id  congressional_district_geoid  household_weight  state_fips      snap\n",
      "            2                           201               3.5           2  0.000000\n",
      "        25000                          3701               2.5          37 70.080002\n"
     ]
    }
   ],
   "source": [
    "sim_after = Microsimulation(dataset=f\"./{output_path}\")\n",
    "hh_after_df = pd.DataFrame(\n",
    "    sim_after.calculate_dataframe(\n",
    "        [\n",
    "            \"household_id\",\n",
    "            \"congressional_district_geoid\",\n",
    "            \"household_weight\",\n",
    "            \"state_fips\",\n",
    "            \"snap\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "print(f\"Stacked dataset: {len(hh_after_df)} households\\n\")\n",
    "\n",
    "mapping_df = pd.read_csv(\n",
    "    f\"{output_dir}/mappings/results_household_mapping.csv\"\n",
    ")\n",
    "example_mapping = mapping_df.loc[\n",
    "    mapping_df.original_household_id == example_hh_id\n",
    "]\n",
    "print(f\"Example household (original_id={example_hh_id}) \" f\"in mapping:\\n\")\n",
    "print(example_mapping.to_string(index=False))\n",
    "\n",
    "new_ids = example_mapping.new_household_id\n",
    "print(f\"\\nIn stacked dataset:\\n\")\n",
    "print(\n",
    "    hh_after_df.loc[hh_after_df.household_id.isin(new_ids)].to_string(\n",
    "        index=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned up calibration_output/\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(output_dir)\n",
    "print(f\"Cleaned up {output_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The clone-based calibration pipeline has six stages:\n",
    "\n",
    "1. **Clone + assign geography** — `assign_random_geography()` creates N copies of each CPS record, each with a population-weighted random census block.\n",
    "2. **Simulate** — `_simulate_clone()` sets each clone's `state_fips` and recalculates state-dependent benefits.\n",
    "3. **Geographic masking** — `state_to_cols` / `cd_to_cols` restrict each target row to geographically relevant columns.\n",
    "4. **Re-randomize takeup** — `rerandomize_takeup()` draws new takeup per census block, breaking the fixed-takeup assumption.\n",
    "5. **Build matrix** — `UnifiedMatrixBuilder.build_matrix()` assembles the sparse CSR matrix from all clones.\n",
    "6. **Stacked datasets** — `create_sparse_cd_stacked_dataset()` converts calibrated weights into CD-level h5 files.\n",
    "\n",
    "For matrix diagnostics (row/column anatomy, target groups, sparsity analysis), see [calibration_matrix.ipynb](calibration_matrix.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
