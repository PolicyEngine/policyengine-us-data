{"version":"1","records":[{"hierarchy":{"lvl1":"Abstract"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Abstract"},"content":"We present a methodology for creating enhanced microsimulation datasets by combining the Current Population Survey (CPS) with the IRS Public Use File (PUF). Our two-stage approach uses quantile regression forests to impute 72 tax variables from the PUF onto CPS records, preserving distributional characteristics while maintaining household composition and member relationships. The imputation process alone does not guarantee consistency with official statistics, necessitating a reweighting step to align the combined dataset with known population totals and administrative benchmarks. We apply a reweighting algorithm that calibrates the dataset to over 7,000 targets from six sources: IRS Statistics of Income, Census population projections, Congressional Budget Office program estimates, Treasury expenditure data, Joint Committee on Taxation tax expenditure estimates, and healthcare spending patterns. The reweighting employs dropout-regularized gradient descent optimization to ensure consistency with administrative benchmarks. Validation shows the enhanced dataset reduces error in key tax components by [TO BE CALCULATED]% relative to the baseline CPS. The dataset maintains the CPS’s demographic detail and geographic granularity while incorporating tax reporting data from administrative sources. We release the enhanced dataset, source code, and documentation to support policy analysis.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Appendix"},"type":"lvl1","url":"/appendix","position":0},{"hierarchy":{"lvl1":"Appendix"},"content":"","type":"content","url":"/appendix","position":1},{"hierarchy":{"lvl1":"Appendix","lvl2":"Appendix A: Implementation Code"},"type":"lvl2","url":"/appendix#appendix-a-implementation-code","position":2},{"hierarchy":{"lvl1":"Appendix","lvl2":"Appendix A: Implementation Code"},"content":"","type":"content","url":"/appendix#appendix-a-implementation-code","position":3},{"hierarchy":{"lvl1":"Appendix","lvl3":"A.1 Quantile Regression Forest Implementation","lvl2":"Appendix A: Implementation Code"},"type":"lvl3","url":"/appendix#a-1-quantile-regression-forest-implementation","position":4},{"hierarchy":{"lvl1":"Appendix","lvl3":"A.1 Quantile Regression Forest Implementation","lvl2":"Appendix A: Implementation Code"},"content":"The following code demonstrates the implementation of Quantile Regression Forests for variable imputation:from quantile_forest import RandomForestQuantileRegressor\n\nqrf = RandomForestQuantileRegressor(\n    n_estimators=100,\n    min_samples_leaf=1,\n    random_state=0\n)","type":"content","url":"/appendix#a-1-quantile-regression-forest-implementation","position":5},{"hierarchy":{"lvl1":"Appendix","lvl3":"A.2 PyTorch Optimization for Reweighting","lvl2":"Appendix A: Implementation Code"},"type":"lvl3","url":"/appendix#a-2-pytorch-optimization-for-reweighting","position":6},{"hierarchy":{"lvl1":"Appendix","lvl3":"A.2 PyTorch Optimization for Reweighting","lvl2":"Appendix A: Implementation Code"},"content":"The reweighting optimization uses PyTorch for gradient-based optimization:import torch\n\n# Initialize with log of original weights\nlog_weights = torch.log(original_weights)\nlog_weights.requires_grad = True\n\n# Adam optimizer\noptimizer = torch.optim.Adam([log_weights], lr=0.1)\n\n# Optimization loop\nfor iteration in range(5000):\n    weights = torch.exp(log_weights)\n    achieved = weights @ loss_matrix\n    relative_errors = (achieved - targets) / targets\n    loss = torch.mean(relative_errors ** 2)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()","type":"content","url":"/appendix#a-2-pytorch-optimization-for-reweighting","position":7},{"hierarchy":{"lvl1":"Appendix","lvl2":"Appendix B: Tables"},"type":"lvl2","url":"/appendix#appendix-b-tables","position":8},{"hierarchy":{"lvl1":"Appendix","lvl2":"Appendix B: Tables"},"content":"","type":"content","url":"/appendix#appendix-b-tables","position":9},{"hierarchy":{"lvl1":"Appendix","lvl3":"Table A1: Complete List of Imputed Variables","lvl2":"Appendix B: Tables"},"type":"lvl3","url":"/appendix#table-a1-complete-list-of-imputed-variables","position":10},{"hierarchy":{"lvl1":"Appendix","lvl3":"Table A1: Complete List of Imputed Variables","lvl2":"Appendix B: Tables"},"content":"[TO BE GENERATED - Complete list of 72 imputed variables from PUF organized by category]","type":"content","url":"/appendix#table-a1-complete-list-of-imputed-variables","position":11},{"hierarchy":{"lvl1":"Background"},"type":"lvl1","url":"/background","position":0},{"hierarchy":{"lvl1":"Background"},"content":"","type":"content","url":"/background","position":1},{"hierarchy":{"lvl1":"Background","lvl2":"The Microsimulation Landscape"},"type":"lvl2","url":"/background#the-microsimulation-landscape","position":2},{"hierarchy":{"lvl1":"Background","lvl2":"The Microsimulation Landscape"},"content":"Tax and benefit microsimulation models play a role in policy analysis by projecting the distributional and revenue impacts of proposed reforms. Institutions maintaining these models include government agencies like the Congressional Budget Office (CBO), Joint Committee on Taxation (JCT), and Treasury’s Office of Tax Analysis (OTA), as well as non-governmental organizations including the Urban-Brookings Tax Policy Center (TPC), Tax Foundation, Penn Wharton Budget Model (PWBM), Institute on Taxation and Economic Policy (ITEP), Yale Budget Lab, and the open-source Policy Simulation Library (PSL). Each model serves specific institutional needs but faces common data challenges.\n\nThe core challenges these models face stem from the tradeoff between data comprehensiveness and accessibility. Administrative tax data provides income reporting but lacks the household context that models need to analyze benefit programs and family-level impacts \n\nSabelhaus & Johnson, 2020. Survey data captures household relationships and program participation but suffers from income underreporting that worsens at higher income levels \n\nMeyer et al., 2021. The need to protect taxpayer privacy limits data availability because administrators cannot publicly release microdata.","type":"content","url":"/background#the-microsimulation-landscape","position":3},{"hierarchy":{"lvl1":"Background","lvl2":"Data Enhancement Approaches"},"type":"lvl2","url":"/background#data-enhancement-approaches","position":4},{"hierarchy":{"lvl1":"Background","lvl2":"Data Enhancement Approaches"},"content":"Different microsimulation models use various approaches to enhance their underlying data:\n\nGovernment models (CBO, JCT, Treasury) have access to confidential administrative data but cannot share their enhanced microdata. Non-governmental models work with public data, leading to various enhancement strategies. Some organizations use proprietary extracts of tax returns, while others enhance survey data with various methods.\n\nOur enhanced dataset provides an open-source methodology with state identifiers and calibration to state-level targets. This enables analysis of federal-state tax interactions. Researchers can use the dataset with PolicyEngine or other microsimulation models.\n\nThe open-source nature promotes methodological transparency. The modular design allows researchers to substitute alternative imputation or calibration methods while maintaining the overall framework. Regular updates as new CPS and administrative data become available ensure the dataset remains current.","type":"content","url":"/background#data-enhancement-approaches","position":5},{"hierarchy":{"lvl1":"Conclusion"},"type":"lvl1","url":"/conclusion","position":0},{"hierarchy":{"lvl1":"Conclusion"},"content":"We present a methodology for creating enhanced microsimulation datasets that combine the strengths of survey and administrative data sources. The Enhanced CPS dataset demonstrates that careful application of modern statistical methods can substantially improve the data available for policy analysis.","type":"content","url":"/conclusion","position":1},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Summary of Contributions"},"type":"lvl2","url":"/conclusion#summary-of-contributions","position":2},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Summary of Contributions"},"content":"Our work makes several key contributions:\n\nMethodological Innovation: The use of Quantile Regression Forests for imputation preserves distributional characteristics while maintaining computational efficiency. The large-scale calibration to 7,000+ targets pushes the boundaries of survey data enhancement.\n\nPractical Tools: We provide open-source implementations that enable researchers to apply, modify, and extend these methods. The modular design facilitates experimentation with alternative approaches.\n\nValidated Dataset: The Enhanced CPS itself serves as a public good for the research community, enabling studies that would otherwise require restricted data access.\n\nReproducible Research: All code, data, and documentation are publicly available, supporting reproducibility and collaborative improvement.","type":"content","url":"/conclusion#summary-of-contributions","position":3},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Key Findings"},"type":"lvl2","url":"/conclusion#key-findings","position":4},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Key Findings"},"content":"The validation results demonstrate that combining survey and administrative data through principled statistical methods can achieve:\n\nImproved income distribution representation\n\nBetter alignment with program participation totals\n\nMaintained demographic and geographic detail\n\nSuitable accuracy for policy simulation\n\nWhile no dataset perfectly represents the full population, the Enhanced CPS provides a pragmatic balance of accuracy, detail, and accessibility.","type":"content","url":"/conclusion#key-findings","position":5},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Implications for Policy Analysis"},"type":"lvl2","url":"/conclusion#implications-for-policy-analysis","position":6},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Implications for Policy Analysis"},"content":"Enhanced microdata availability creates immediate implications for policy analysis. More accurate representation of high incomes enables better analysis of progressive tax reforms and revenue estimates. Researchers can now analyze tax and transfer policies jointly rather than in isolation. Geographic identifiers enable subnational policy analysis not possible with administrative tax data alone. Finally, household structure allows examination of policy impacts across family types and income levels.","type":"content","url":"/conclusion#implications-for-policy-analysis","position":7},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Broader Implications"},"type":"lvl2","url":"/conclusion#broader-implications","position":8},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Broader Implications"},"content":"Beyond the specific dataset, this work demonstrates several broader principles. Combining multiple data sources can overcome individual limitations, showing the value of data integration. Making methods and data publicly available accelerates research progress and demonstrates open science benefits. While perfect data may never exist, pragmatic enhancements can substantially improve analysis capabilities. Furthermore, open-source approaches enable community contributions and continuous improvement.","type":"content","url":"/conclusion#broader-implications","position":9},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Limitations and Future Work"},"type":"lvl2","url":"/conclusion#limitations-and-future-work","position":10},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Limitations and Future Work"},"content":"We acknowledge important limitations including temporal inconsistency between data sources, imputation model assumptions, calibration trade-offs, and validation challenges. Future work should address these through more recent administrative data, enhanced imputation methods, additional validation exercises, and uncertainty quantification.","type":"content","url":"/conclusion#limitations-and-future-work","position":11},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Call to Action"},"type":"lvl2","url":"/conclusion#call-to-action","position":12},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Call to Action"},"content":"We encourage researchers to apply the Enhanced CPS to policy questions where combined demographic and tax detail adds value, compare findings with other data sources and contribute validation results, leverage the open-source nature to make methodological enhancements, and document use cases, limitations discovered, and suggested improvements.","type":"content","url":"/conclusion#call-to-action","position":13},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Final Thoughts"},"type":"lvl2","url":"/conclusion#final-thoughts","position":14},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Final Thoughts"},"content":"The Enhanced CPS represents one approach to a fundamental challenge in microsimulation: the need for comprehensive, accurate microdata. While not perfect, it demonstrates that substantial improvements are possible through careful methodology and open collaboration.\n\nAs data availability evolves and methods advance, this work contributes to a future where policy analysis rests on increasingly solid empirical foundations. Our ultimate goal remains better informed policy decisions that improve social welfare.\n\nThe enhanced dataset, complete documentation, and all source code are available at \n\nhttps://​github​.com​/PolicyEngine​/policyengine​-us​-data.","type":"content","url":"/conclusion#final-thoughts","position":15},{"hierarchy":{"lvl1":"Data Sources"},"type":"lvl1","url":"/data","position":0},{"hierarchy":{"lvl1":"Data Sources"},"content":"Our methodology combines two primary data sources with calibration targets from administrative sources.","type":"content","url":"/data","position":1},{"hierarchy":{"lvl1":"Data Sources","lvl2":"Primary Data Sources"},"type":"lvl2","url":"/data#primary-data-sources","position":2},{"hierarchy":{"lvl1":"Data Sources","lvl2":"Primary Data Sources"},"content":"","type":"content","url":"/data#primary-data-sources","position":3},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Current Population Survey (CPS)","lvl2":"Primary Data Sources"},"type":"lvl3","url":"/data#current-population-survey-cps","position":4},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Current Population Survey (CPS)","lvl2":"Primary Data Sources"},"content":"The Current Population Survey Annual Social and Economic Supplement (ASEC) serves as our base dataset. The Census Bureau and Bureau of Labor Statistics jointly conduct the CPS ASEC, surveying households annually.\n\nThe CPS provides features for microsimulation modeling. It offers a representative sample of US households with demographic information including age, education, race, and employment status. The survey captures family and household relationships through relationship codes that allow reconstruction of tax units and benefit units. Geographic identifiers down to the state level enable subnational policy analysis. The survey includes questions about program participation in transfer programs like SNAP, Medicaid, and housing assistance. The survey collects income data by source, distinguishing between wages, self-employment, interest, dividends, and transfers.\n\nThe CPS faces limitations that necessitate enhancement. Income underreporting is severe at high income levels \n\nRothbaum & Bee, 2021. The survey provides limited tax detail, lacking information on itemized deductions, tax credits, and capital gains realizations. The Census Bureau topcodes high income values to protect confidentiality. The survey’s focus on cash income means it misses non-cash compensation like employer-provided health insurance premiums.","type":"content","url":"/data#current-population-survey-cps","position":5},{"hierarchy":{"lvl1":"Data Sources","lvl3":"IRS Public Use File (PUF)","lvl2":"Primary Data Sources"},"type":"lvl3","url":"/data#irs-public-use-file-puf","position":6},{"hierarchy":{"lvl1":"Data Sources","lvl3":"IRS Public Use File (PUF)","lvl2":"Primary Data Sources"},"content":"The IRS Statistics of Income Public Use File contains tax return information that the IRS draws from a stratified sample of individual income tax returns.\n\nThe PUF provides tax-related variables drawn from filed tax returns. It provides breakdowns of income by source including wages, interest, dividends, capital gains, business income, and retirement distributions. The file includes itemized deductions such as mortgage interest, state and local taxes, and charitable contributions. The file includes tax credits that filers claim, from the earned income tax credit to education credits. The stratified sampling design oversamples high-income returns. Sampling weights allow researchers to produce population-representative estimates.\n\nThe PUF has limitations for policy analysis. The file contains minimal demographic information, limited to filing status and exemptions claimed. The IRS removes geographic identifiers to protect taxpayer privacy, which prevents state-level analysis. The population excludes non-filers. The PUF lacks household structure, preventing analysis of how tax policies interact with transfer programs that operate at the household level.","type":"content","url":"/data#irs-public-use-file-puf","position":7},{"hierarchy":{"lvl1":"Data Sources","lvl2":"Additional Data Sources for Imputation"},"type":"lvl2","url":"/data#additional-data-sources-for-imputation","position":8},{"hierarchy":{"lvl1":"Data Sources","lvl2":"Additional Data Sources for Imputation"},"content":"Beyond the PUF, we incorporate data from three additional surveys to impute specific variables missing from the CPS:","type":"content","url":"/data#additional-data-sources-for-imputation","position":9},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Survey of Income and Program Participation (SIPP)","lvl2":"Additional Data Sources for Imputation"},"type":"lvl3","url":"/data#survey-of-income-and-program-participation-sipp","position":10},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Survey of Income and Program Participation (SIPP)","lvl2":"Additional Data Sources for Imputation"},"content":"The SIPP provides income and program participation data. We use SIPP primarily to impute tip income through a Quantile Regression Forest model trained on SIPP data, using employment income, age, and household composition as predictors.","type":"content","url":"/data#survey-of-income-and-program-participation-sipp","position":11},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Survey of Consumer Finances (SCF)","lvl2":"Additional Data Sources for Imputation"},"type":"lvl3","url":"/data#survey-of-consumer-finances-scf","position":12},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Survey of Consumer Finances (SCF)","lvl2":"Additional Data Sources for Imputation"},"content":"The SCF provides wealth and debt information that we use to impute several financial variables missing from the CPS. We match auto loan balances based on household demographics and income, then calculate interest on auto loans from these imputed balances. Additionally, we impute various net worth components and other wealth measures not available in CPS. The SCF imputation uses their reference person definition to ensure proper matching.","type":"content","url":"/data#survey-of-consumer-finances-scf","position":13},{"hierarchy":{"lvl1":"Data Sources","lvl3":"American Community Survey (ACS)","lvl2":"Additional Data Sources for Imputation"},"type":"lvl3","url":"/data#american-community-survey-acs","position":14},{"hierarchy":{"lvl1":"Data Sources","lvl3":"American Community Survey (ACS)","lvl2":"Additional Data Sources for Imputation"},"content":"The ACS provides housing and geographic data that supplements the CPS housing information. For homeowners, we impute property taxes based on state of residence, household income, and demographic characteristics. We also impute rent values for specific tenure types where CPS data is incomplete, along with additional housing characteristics not captured in the CPS. These imputations use Quantile Regression Forests to preserve distributional characteristics while accounting for household heterogeneity.","type":"content","url":"/data#american-community-survey-acs","position":15},{"hierarchy":{"lvl1":"Data Sources","lvl2":"Calibration Data Sources"},"type":"lvl2","url":"/data#calibration-data-sources","position":16},{"hierarchy":{"lvl1":"Data Sources","lvl2":"Calibration Data Sources"},"content":"The calibration process uses targets from six administrative sources:","type":"content","url":"/data#calibration-data-sources","position":17},{"hierarchy":{"lvl1":"Data Sources","lvl3":"IRS Statistics of Income (SOI)","lvl2":"Calibration Data Sources"},"type":"lvl3","url":"/data#irs-statistics-of-income-soi","position":18},{"hierarchy":{"lvl1":"Data Sources","lvl3":"IRS Statistics of Income (SOI)","lvl2":"Calibration Data Sources"},"content":"The IRS SOI provides tax return aggregates by income level, filing status, and geography. These include counts of returns, aggregate income by source, deduction amounts, and credit utilization.","type":"content","url":"/data#irs-statistics-of-income-soi","position":19},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Census Population Estimates","lvl2":"Calibration Data Sources"},"type":"lvl3","url":"/data#census-population-estimates","position":20},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Census Population Estimates","lvl2":"Calibration Data Sources"},"content":"Census provides population counts by age, state, and other demographic characteristics.","type":"content","url":"/data#census-population-estimates","position":21},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Congressional Budget Office","lvl2":"Calibration Data Sources"},"type":"lvl3","url":"/data#congressional-budget-office","position":22},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Congressional Budget Office","lvl2":"Calibration Data Sources"},"content":"CBO provides projections for program participation and spending, including SNAP benefits, unemployment compensation, and tax revenues.","type":"content","url":"/data#congressional-budget-office","position":23},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Joint Committee on Taxation","lvl2":"Calibration Data Sources"},"type":"lvl3","url":"/data#joint-committee-on-taxation","position":24},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Joint Committee on Taxation","lvl2":"Calibration Data Sources"},"content":"JCT provides estimates of tax expenditures for major deductions and credits.","type":"content","url":"/data#joint-committee-on-taxation","position":25},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Healthcare Spending Data","lvl2":"Calibration Data Sources"},"type":"lvl3","url":"/data#healthcare-spending-data","position":26},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Healthcare Spending Data","lvl2":"Calibration Data Sources"},"content":"Various sources provide data on health insurance premiums, Medicare costs, and medical spending by age group.","type":"content","url":"/data#healthcare-spending-data","position":27},{"hierarchy":{"lvl1":"Data Sources","lvl3":"State Administrative Data","lvl2":"Calibration Data Sources"},"type":"lvl3","url":"/data#state-administrative-data","position":28},{"hierarchy":{"lvl1":"Data Sources","lvl3":"State Administrative Data","lvl2":"Calibration Data Sources"},"content":"State-level program participation and spending data from various state agencies.","type":"content","url":"/data#state-administrative-data","position":29},{"hierarchy":{"lvl1":"Data Sources","lvl2":"Data Access and Documentation"},"type":"lvl2","url":"/data#data-access-and-documentation","position":30},{"hierarchy":{"lvl1":"Data Sources","lvl2":"Data Access and Documentation"},"content":"The enhanced dataset is publicly available through Hugging Face at \n\nhttps://​huggingface​.co​/datasets​/PolicyEngine​/policyengine​-us​-data. We distribute the data as HDF5 files compatible with PolicyEngine and other microsimulation frameworks, with new releases accompanying each CPS vintage.\n\nWe maintain complete documentation of variable definitions, imputation procedures, and calibration targets in the project repository.","type":"content","url":"/data#data-access-and-documentation","position":31},{"hierarchy":{"lvl1":"Discussion"},"type":"lvl1","url":"/discussion","position":0},{"hierarchy":{"lvl1":"Discussion"},"content":"We examine the strengths, limitations, and potential applications of the Enhanced CPS dataset, along with directions for future development.","type":"content","url":"/discussion","position":1},{"hierarchy":{"lvl1":"Discussion","lvl2":"Strengths"},"type":"lvl2","url":"/discussion#strengths","position":2},{"hierarchy":{"lvl1":"Discussion","lvl2":"Strengths"},"content":"","type":"content","url":"/discussion#strengths","position":3},{"hierarchy":{"lvl1":"Discussion","lvl3":"Comprehensive Coverage","lvl2":"Strengths"},"type":"lvl3","url":"/discussion#comprehensive-coverage","position":4},{"hierarchy":{"lvl1":"Discussion","lvl3":"Comprehensive Coverage","lvl2":"Strengths"},"content":"The Enhanced CPS uniquely combines:\n\nDemographic detail from the CPS including state identifiers\n\nTax precision from IRS administrative data\n\nCalibration to contemporary official statistics\n\nOpen-source availability for research use\n\nThis combination enables analyses that would be difficult or impossible with existing public datasets alone.","type":"content","url":"/discussion#comprehensive-coverage","position":5},{"hierarchy":{"lvl1":"Discussion","lvl3":"Methodological Contributions","lvl2":"Strengths"},"type":"lvl3","url":"/discussion#methodological-contributions","position":6},{"hierarchy":{"lvl1":"Discussion","lvl3":"Methodological Contributions","lvl2":"Strengths"},"content":"The use of Quantile Regression Forests for imputation represents an advance over traditional matching methods:\n\nPreserves full conditional distributions\n\nCaptures non-linear relationships\n\nMaintains realistic variable correlations\n\nAllows uncertainty quantification\n\nThe large-scale calibration to 7,000+ targets ensures consistency with administrative benchmarks across multiple dimensions simultaneously.","type":"content","url":"/discussion#methodological-contributions","position":7},{"hierarchy":{"lvl1":"Discussion","lvl3":"Practical Advantages","lvl2":"Strengths"},"type":"lvl3","url":"/discussion#practical-advantages","position":8},{"hierarchy":{"lvl1":"Discussion","lvl3":"Practical Advantages","lvl2":"Strengths"},"content":"For policy analysis, the dataset offers state-level geographic detail enabling subnational analysis, household structure for distributional studies, tax detail for revenue estimation, program participation for benefit analysis, and recent data calibrated to current totals.","type":"content","url":"/discussion#practical-advantages","position":9},{"hierarchy":{"lvl1":"Discussion","lvl2":"Limitations"},"type":"lvl2","url":"/discussion#limitations","position":10},{"hierarchy":{"lvl1":"Discussion","lvl2":"Limitations"},"content":"","type":"content","url":"/discussion#limitations","position":11},{"hierarchy":{"lvl1":"Discussion","lvl3":"Temporal Inconsistency","lvl2":"Limitations"},"type":"lvl3","url":"/discussion#temporal-inconsistency","position":12},{"hierarchy":{"lvl1":"Discussion","lvl3":"Temporal Inconsistency","lvl2":"Limitations"},"content":"The temporal gap between data sources presents a limitation, with 2015 PUF data imputed onto 2024 CPS creating a nine-year gap in underlying populations. This gap means demographic shifts are not fully captured, and tax law changes since 2015 are not reflected in the imputed variables.\n\nWhile we uprate dollar amounts and calibration partially addresses this, we may not reflect fundamental demographic changes.","type":"content","url":"/discussion#temporal-inconsistency","position":13},{"hierarchy":{"lvl1":"Discussion","lvl3":"Imputation Assumptions","lvl2":"Limitations"},"type":"lvl3","url":"/discussion#imputation-assumptions","position":14},{"hierarchy":{"lvl1":"Discussion","lvl3":"Imputation Assumptions","lvl2":"Limitations"},"content":"The QRF imputation assumes that relationships between demographics and tax variables remain stable, seven predictors sufficiently capture variation, the PUF represents the tax-filing population well, and missing data patterns are ignorable.\n\nThese assumptions may not hold perfectly, particularly for subpopulations that the PUF underrepresents.","type":"content","url":"/discussion#imputation-assumptions","position":15},{"hierarchy":{"lvl1":"Discussion","lvl3":"Calibration Trade-offs","lvl2":"Limitations"},"type":"lvl3","url":"/discussion#calibration-trade-offs","position":16},{"hierarchy":{"lvl1":"Discussion","lvl3":"Calibration Trade-offs","lvl2":"Limitations"},"content":"With 7,000+ targets, perfect fit to all benchmarks is impossible. The optimization must balance competing objectives across target types, the relative importance of different statistics, stability of resulting weights, and preservation of household relationships.\n\nUsers should consult validation metrics for targets most relevant to their analysis.","type":"content","url":"/discussion#calibration-trade-offs","position":17},{"hierarchy":{"lvl1":"Discussion","lvl2":"Applications"},"type":"lvl2","url":"/discussion#applications","position":18},{"hierarchy":{"lvl1":"Discussion","lvl2":"Applications"},"content":"","type":"content","url":"/discussion#applications","position":19},{"hierarchy":{"lvl1":"Discussion","lvl3":"Tax Policy Analysis","lvl2":"Applications"},"type":"lvl3","url":"/discussion#tax-policy-analysis","position":20},{"hierarchy":{"lvl1":"Discussion","lvl3":"Tax Policy Analysis","lvl2":"Applications"},"content":"The dataset excels at analyzing federal tax reforms through accurate income distribution at high incomes, detailed deduction and credit information, state identifiers for SALT analysis, and household structure for family-based policies.","type":"content","url":"/discussion#tax-policy-analysis","position":21},{"hierarchy":{"lvl1":"Discussion","lvl3":"State and Local Analysis","lvl2":"Applications"},"type":"lvl3","url":"/discussion#state-and-local-analysis","position":22},{"hierarchy":{"lvl1":"Discussion","lvl3":"State and Local Analysis","lvl2":"Applications"},"content":"Unlike the PUF, the Enhanced CPS enables state-level studies including state income tax modeling, geographic variation in federal policies, state-specific program interactions, and regional economic impacts.","type":"content","url":"/discussion#state-and-local-analysis","position":23},{"hierarchy":{"lvl1":"Discussion","lvl3":"Integrated Policy Analysis","lvl2":"Applications"},"type":"lvl3","url":"/discussion#integrated-policy-analysis","position":24},{"hierarchy":{"lvl1":"Discussion","lvl3":"Integrated Policy Analysis","lvl2":"Applications"},"content":"The combination of tax and transfer data supports analysis of universal basic income proposals, earned income tax credit expansions, childcare and family benefit reforms, and healthcare subsidy design.","type":"content","url":"/discussion#integrated-policy-analysis","position":25},{"hierarchy":{"lvl1":"Discussion","lvl3":"Microsimulation Model Development","lvl2":"Applications"},"type":"lvl3","url":"/discussion#microsimulation-model-development","position":26},{"hierarchy":{"lvl1":"Discussion","lvl3":"Microsimulation Model Development","lvl2":"Applications"},"content":"As the foundation for PolicyEngine US, the dataset demonstrates how enhanced microdata improve model capabilities through more accurate baseline distributions, better behavioral response modeling, improved validation against benchmarks, and enhanced credibility of results.","type":"content","url":"/discussion#microsimulation-model-development","position":27},{"hierarchy":{"lvl1":"Discussion","lvl2":"Comparison with Alternatives"},"type":"lvl2","url":"/discussion#comparison-with-alternatives","position":28},{"hierarchy":{"lvl1":"Discussion","lvl2":"Comparison with Alternatives"},"content":"","type":"content","url":"/discussion#comparison-with-alternatives","position":29},{"hierarchy":{"lvl1":"Discussion","lvl3":"Versus Synthetic Data","lvl2":"Comparison with Alternatives"},"type":"lvl3","url":"/discussion#versus-synthetic-data","position":30},{"hierarchy":{"lvl1":"Discussion","lvl3":"Versus Synthetic Data","lvl2":"Comparison with Alternatives"},"content":"Unlike fully synthetic datasets, our approach preserves actual survey responses where possible, imputes only missing tax variables, maintains household relationships, and provides transparent methodology.","type":"content","url":"/discussion#versus-synthetic-data","position":31},{"hierarchy":{"lvl1":"Discussion","lvl3":"Versus Administrative Data","lvl2":"Comparison with Alternatives"},"type":"lvl3","url":"/discussion#versus-administrative-data","position":32},{"hierarchy":{"lvl1":"Discussion","lvl3":"Versus Administrative Data","lvl2":"Comparison with Alternatives"},"content":"While not replacing restricted administrative data, the Enhanced CPS offers public availability, household structure, geographic detail, integration with survey content, and no access restrictions.","type":"content","url":"/discussion#versus-administrative-data","position":33},{"hierarchy":{"lvl1":"Discussion","lvl3":"Versus Other Matching Approaches","lvl2":"Comparison with Alternatives"},"type":"lvl3","url":"/discussion#versus-other-matching-approaches","position":34},{"hierarchy":{"lvl1":"Discussion","lvl3":"Versus Other Matching Approaches","lvl2":"Comparison with Alternatives"},"content":"Compared to traditional statistical matching, QRF better preserves distributions, large-scale calibration ensures consistency, open-source implementation enables replication, and modular design allows improvements.","type":"content","url":"/discussion#versus-other-matching-approaches","position":35},{"hierarchy":{"lvl1":"Discussion","lvl2":"Future Directions"},"type":"lvl2","url":"/discussion#future-directions","position":36},{"hierarchy":{"lvl1":"Discussion","lvl2":"Future Directions"},"content":"","type":"content","url":"/discussion#future-directions","position":37},{"hierarchy":{"lvl1":"Discussion","lvl3":"Methodological Enhancements","lvl2":"Future Directions"},"type":"lvl3","url":"/discussion#methodological-enhancements","position":38},{"hierarchy":{"lvl1":"Discussion","lvl3":"Methodological Enhancements","lvl2":"Future Directions"},"content":"Potential improvements include incorporating additional predictors for imputation, using more recent administrative data when available, developing time-series consistency methods, and adding uncertainty quantification.","type":"content","url":"/discussion#methodological-enhancements","position":39},{"hierarchy":{"lvl1":"Discussion","lvl3":"Additional Data Integration","lvl2":"Future Directions"},"type":"lvl3","url":"/discussion#additional-data-integration","position":40},{"hierarchy":{"lvl1":"Discussion","lvl3":"Additional Data Integration","lvl2":"Future Directions"},"content":"We could incorporate in future versions state tax return data, program administrative records, consumer expenditure information, and health insurance claims data.","type":"content","url":"/discussion#additional-data-integration","position":41},{"hierarchy":{"lvl1":"Discussion","lvl3":"Model Development","lvl2":"Future Directions"},"type":"lvl3","url":"/discussion#model-development","position":42},{"hierarchy":{"lvl1":"Discussion","lvl3":"Model Development","lvl2":"Future Directions"},"content":"We could extend the framework to dynamic microsimulation over time, behavioral response estimation, geographic mobility modeling, and life-cycle analysis.","type":"content","url":"/discussion#model-development","position":43},{"hierarchy":{"lvl1":"Discussion","lvl3":"International Applications","lvl2":"Future Directions"},"type":"lvl3","url":"/discussion#international-applications","position":44},{"hierarchy":{"lvl1":"Discussion","lvl3":"International Applications","lvl2":"Future Directions"},"content":"Researchers could adapt the methodology for other countries facing similar data availability challenges, need for tax-benefit integration, open-source implementation requirements, and cross-national comparison needs.","type":"content","url":"/discussion#international-applications","position":45},{"hierarchy":{"lvl1":"Discussion","lvl2":"Conclusion for Researchers"},"type":"lvl2","url":"/discussion#conclusion-for-researchers","position":46},{"hierarchy":{"lvl1":"Discussion","lvl2":"Conclusion for Researchers"},"content":"The Enhanced CPS provides a valuable resource for policy analysis, though users should understand the limitations (particularly temporal inconsistency), validate results against external benchmarks, consider sensitivity to methodological choices, and contribute improvements to the open-source project.\n\nThe dataset represents a pragmatic solution to data limitations. It enables analyses that advance our understanding of tax and transfer policy impacts while we await improved data access.","type":"content","url":"/discussion#conclusion-for-researchers","position":47},{"hierarchy":{"lvl1":"Introduction"},"type":"lvl1","url":"/introduction","position":0},{"hierarchy":{"lvl1":"Introduction"},"content":"Microsimulation models require high-quality microdata that accurately represents both demographic characteristics and economic outcomes. The ideal dataset would combine the demographic richness and household structure of surveys with the income precision of administrative tax records. However, publicly available datasets typically excel in one dimension while lacking in the other.\n\nThe Current Population Survey (CPS) Annual Social and Economic Supplement provides detailed household demographics, family relationships, and program participation data for a representative sample of US households. However, it suffers from well-documented income underreporting, particularly at the top of the distribution. The IRS Public Use File (PUF) contains accurate tax return information but lacks household structure, demographic detail, and state identifiers needed for comprehensive policy analysis.\n\nThis paper presents a methodology for creating an Enhanced CPS dataset that combines the strengths of both sources. Through a two-stage enhancement process—imputation followed by reweighting—we create a dataset suitable for analyzing both tax and transfer policies at federal and state levels.","type":"content","url":"/introduction","position":1},{"hierarchy":{"lvl1":"Introduction","lvl2":"Related Work"},"type":"lvl2","url":"/introduction#related-work","position":2},{"hierarchy":{"lvl1":"Introduction","lvl2":"Related Work"},"content":"Researchers have developed several approaches to address the limitations of survey data for microsimulation. Researchers have long used statistical matching techniques to combine datasets. \n\nRadner (1978) pioneered exact matching methods for combining survey and administrative data, while \n\nRodgers (1984) developed statistical matching based on common variables. More recently, \n\nD'Orazio et al. (2006) provided a comprehensive framework for modern statistical matching methods.\n\nEconomic researchers address dataset limitations through various strategies. The Congressional Budget Office combines CPS data with tax return information through statistical matching \n\nCongressional Budget Office, 2022. The Tax Policy Center creates synthetic datasets by statistically matching the CPS to a subset of tax returns \n\nRohaly et al., 2005. However, these approaches often sacrifice either demographic detail or tax precision, which limits their utility for comprehensive policy analysis.\n\nStatistical agencies and researchers employ reweighting methods to align survey data with administrative totals. The Luxembourg Income Study uses calibration to improve cross-national comparability \n\nGornick & Jäntti, 2013. The Urban-Brookings Tax Policy Center employs reweighting in their microsimulation model but relies on proprietary data that cannot be shared publicly \n\nKhitatrakun et al., 2016.\n\nOur approach differs from previous efforts in three key ways. First, we employ quantile regression forests to preserve distributional characteristics during imputation, improving upon traditional hot-deck and regression-based methods that may distort variable relationships. We conduct robustness checks comparing QRF performance to gradient boosting and neural network approaches, finding QRF provides the best balance of accuracy and interpretability. Second, we calibrate to over 7,000 targets from multiple administrative sources, far exceeding the scope of previous calibration efforts which typically use fewer than 100 targets. Third, we provide a fully open-source implementation enabling reproducibility and collaborative improvement, addressing the transparency limitations of existing proprietary models.","type":"content","url":"/introduction#related-work","position":3},{"hierarchy":{"lvl1":"Introduction","lvl2":"Contributions"},"type":"lvl2","url":"/introduction#contributions","position":4},{"hierarchy":{"lvl1":"Introduction","lvl2":"Contributions"},"content":"This paper makes three main contributions to the economic and public policy literature. Methodologically, we demonstrate how quantile regression forests can effectively impute detailed tax variables while preserving their joint distribution and relationship to demographics. This advances the statistical matching literature by showing how modern machine learning methods can overcome limitations of traditional hot-deck and parametric approaches. The preservation of distributional characteristics is particularly important for tax policy analysis where outcomes often depend on complex interactions between income sources and household characteristics.\n\nOur empirical contribution involves creating and validating a publicly available enhanced dataset that addresses longstanding data limitations in microsimulation modeling. By combining the demographic richness of the CPS with the tax precision of the PUF, we enable analyses that were previously infeasible with public data. The dataset’s calibration to over 7,000 administrative targets ensures consistency with official statistics across multiple dimensions simultaneously.\n\nFrom a practical perspective, we provide open-source tools and comprehensive documentation that enable researchers to apply these methods, modify the approach, or build upon our work. This transparency contrasts with existing proprietary models and supports reproducible research. Government agencies could use our framework to enhance their own microsimulation capabilities, while academic researchers gain access to data suitable for analyzing distributional impacts of tax and transfer policies. The modular design allows incremental improvements as new data sources become available.\n\nWe organize the remainder of this paper as follows. Section 2 describes our data sources including the primary datasets and calibration targets. Section 3 details the enhancement methodology including both the imputation and reweighting stages. Section 4 presents validation results comparing performance across datasets. Section 5 discusses limitations, applications, and future directions. Section 6 concludes with implications for policy analysis.","type":"content","url":"/introduction#contributions","position":5},{"hierarchy":{"lvl1":"Methodology"},"type":"lvl1","url":"/methodology","position":0},{"hierarchy":{"lvl1":"Methodology"},"content":"We create the Enhanced CPS dataset through a two-stage process: imputation followed by reweighting. The imputation stage creates a copy of the CPS and uses Quantile Regression Forests to impute tax variables from the PUF onto this copy, creating the Extended CPS. The reweighting stage then optimizes household weights to match administrative targets, producing the Enhanced CPS with weights calibrated to statistics.graph TD\n    subgraph src[\"Source Datasets\"]\n        CPS[\"CPS ASEC\"]:::data\n        PUF[\"IRS PUF\"]:::data\n        SIPP[\"SIPP\"]:::data\n        SCF[\"SCF\"]:::data\n        ACS[\"ACS\"]:::data\n    end\n    \n    Age(\"Age all to target year\"):::process\n    \n    subgraph aged[\"Aged Datasets\"]\n        AgedCPS[\"Aged CPS\"]:::data\n        AgedPUF[\"Aged PUF\"]:::data\n        AgedSIPP[\"Aged SIPP\"]:::data\n        AgedSCF[\"Aged SCF\"]:::data\n        AgedACS[\"Aged ACS\"]:::data\n    end\n    \n    ImpOther(\"Impute SIPP/SCF/ACS variables to CPS\"):::process\n    UpdatedCPS[\"CPS with additional vars\"]:::data\n    \n    Clone(\"Clone CPS\"):::process\n    QRF(\"Train QRF\"):::process\n    \n    Copy1[\"CPS Copy 1: Missing PUF variables filled from PUF\"]:::data\n    Copy2[\"CPS Copy 2: Existing variables replaced from PUF\"]:::data\n    \n    Impute(\"Apply QRF to impute variables\"):::process\n    \n    Concat(\"Concatenate both copies\"):::process\n    \n    Extended[\"Extended CPS - 2x households\"]:::data\n    \n    Targets{{\"Administrative Targets - 7000+\"}}:::data\n    \n    Reweight(\"Reweight Optimization\"):::process\n    \n    Enhanced{{\"Enhanced CPS - Final Dataset\"}}:::output\n    \n    CPS --> Age\n    PUF --> Age\n    SIPP --> Age\n    SCF --> Age\n    ACS --> Age\n    \n    Age --> AgedCPS\n    Age --> AgedPUF\n    Age --> AgedSIPP\n    Age --> AgedSCF\n    Age --> AgedACS\n    \n    AgedSIPP --> ImpOther\n    AgedSCF --> ImpOther\n    AgedACS --> ImpOther\n    AgedCPS --> ImpOther\n    AgedPUF --> QRF\n    \n    ImpOther --> UpdatedCPS\n    UpdatedCPS --> Clone\n    \n    Clone --> Copy1\n    Clone --> Copy2\n    \n    QRF --> Impute\n    Copy1 --> Impute\n    Copy2 --> Impute\n    \n    Impute --> Concat\n    Concat --> Extended\n    \n    Extended --> Reweight\n    Targets --> Reweight\n    Reweight --> Enhanced\n    \n    classDef data fill:#2C6496,stroke:#2C6496,color:#FFFFFF\n    classDef process fill:#39C6C0,stroke:#2C6496,color:#FFFFFF\n    classDef output fill:#5091CC,stroke:#2C6496,color:#FFFFFF","type":"content","url":"/methodology","position":1},{"hierarchy":{"lvl1":"Methodology","lvl2":"Stage 1: Variable Imputation"},"type":"lvl2","url":"/methodology#stage-1-variable-imputation","position":2},{"hierarchy":{"lvl1":"Methodology","lvl2":"Stage 1: Variable Imputation"},"content":"The imputation process begins by aging both the CPS and PUF datasets to the target year, then creating a copy of the aged CPS dataset. This allows us to preserve the original CPS structure while adding imputed tax variables.","type":"content","url":"/methodology#stage-1-variable-imputation","position":3},{"hierarchy":{"lvl1":"Methodology","lvl3":"Data Aging","lvl2":"Stage 1: Variable Imputation"},"type":"lvl3","url":"/methodology#data-aging","position":4},{"hierarchy":{"lvl1":"Methodology","lvl3":"Data Aging","lvl2":"Stage 1: Variable Imputation"},"content":"We age all datasets (CPS, PUF, SIPP, SCF, and ACS) to the target year using population growth factors and income growth indices for input variables only.\n\nWe strip out calculated values like taxes and benefits from the source datasets. We recalculate these only after assembling all inputs.\n\nThis ensures that the imputation models are trained and applied on contemporaneous data.","type":"content","url":"/methodology#data-aging","position":5},{"hierarchy":{"lvl1":"Methodology","lvl3":"Data Cloning Approach","lvl2":"Stage 1: Variable Imputation"},"type":"lvl3","url":"/methodology#data-cloning-approach","position":6},{"hierarchy":{"lvl1":"Methodology","lvl3":"Data Cloning Approach","lvl2":"Stage 1: Variable Imputation"},"content":"We clone the aged CPS dataset to create two versions. The first copy retains original CPS values but fills in variables that don’t exist in CPS with imputed values from the PUF, such as mortgage interest deduction and charitable contributions. The second copy replaces existing CPS income variables with imputed values from the PUF, including wages and salaries, self-employment income, and partnership/S-corp income.\n\nThis dual approach ensures that variables not collected in CPS are added from the PUF, while variables collected in CPS but with measurement error are replaced with more accurate PUF values. Most importantly, household structure and relationships are preserved in both copies.","type":"content","url":"/methodology#data-cloning-approach","position":7},{"hierarchy":{"lvl1":"Methodology","lvl3":"Quantile Regression Forests","lvl2":"Stage 1: Variable Imputation"},"type":"lvl3","url":"/methodology#quantile-regression-forests","position":8},{"hierarchy":{"lvl1":"Methodology","lvl3":"Quantile Regression Forests","lvl2":"Stage 1: Variable Imputation"},"content":"Quantile Regression Forests (QRF) is an extension of random forests that estimates conditional quantiles rather than conditional means. QRF builds an ensemble of decision trees on the training data and stores all observations in leaf nodes rather than just their means. This enables estimation of any quantile of the conditional distribution at prediction time.","type":"content","url":"/methodology#quantile-regression-forests","position":9},{"hierarchy":{"lvl1":"Methodology","lvl4":"QRF Sampling Process","lvl3":"Quantile Regression Forests","lvl2":"Stage 1: Variable Imputation"},"type":"lvl4","url":"/methodology#qrf-sampling-process","position":10},{"hierarchy":{"lvl1":"Methodology","lvl4":"QRF Sampling Process","lvl3":"Quantile Regression Forests","lvl2":"Stage 1: Variable Imputation"},"content":"The key innovation of QRF for imputation is the ability to sample from the conditional distribution rather than using point estimates. The process works as follows:\n\nTrain the model: QRF estimates multiple conditional quantiles (e.g., 10 quantiles from 0 to 1)\n\nGenerate random quantiles: For each CPS record, draw a random quantile from a Beta distribution\n\nSelect imputed value: Use the randomly selected quantile to extract a value from the conditional distribution\n\nThis approach preserves realistic variation and captures conditional tails. For example, a young worker might have low wages most of the time but occasionally have high wages. QRF captures this by allowing the imputation to sometimes draw from the upper tail of the conditional distribution, thus maintaining realistic inequality within demographic groups.","type":"content","url":"/methodology#qrf-sampling-process","position":11},{"hierarchy":{"lvl1":"Methodology","lvl3":"Implementation","lvl2":"Stage 1: Variable Imputation"},"type":"lvl3","url":"/methodology#implementation","position":12},{"hierarchy":{"lvl1":"Methodology","lvl3":"Implementation","lvl2":"Stage 1: Variable Imputation"},"content":"The implementation uses the quantile-forest package, which provides scikit-learn compatible QRF implementation. The aged PUF is subsampled for training efficiency.","type":"content","url":"/methodology#implementation","position":13},{"hierarchy":{"lvl1":"Methodology","lvl3":"Predictor Variables","lvl2":"Stage 1: Variable Imputation"},"type":"lvl3","url":"/methodology#predictor-variables","position":14},{"hierarchy":{"lvl1":"Methodology","lvl3":"Predictor Variables","lvl2":"Stage 1: Variable Imputation"},"content":"Both imputations use the same seven demographic variables available in both datasets: age of the person, gender indicator, tax unit filing status (joint or separate), number of dependents in the tax unit, and tax unit role indicators (head, spouse, or dependent).\n\nThese demographic predictors capture key determinants of income and tax variables while being reliably measured in both datasets.","type":"content","url":"/methodology#predictor-variables","position":15},{"hierarchy":{"lvl1":"Methodology","lvl3":"Imputed Variables","lvl2":"Stage 1: Variable Imputation"},"type":"lvl3","url":"/methodology#imputed-variables","position":16},{"hierarchy":{"lvl1":"Methodology","lvl3":"Imputed Variables","lvl2":"Stage 1: Variable Imputation"},"content":"The process imputes tax-related variables from the PUF in two ways:\n\nFor CPS Copy 1, we add variables that are missing in CPS, including mortgage interest deduction, charitable contributions (both cash and non-cash), state and local tax deductions, medical expense deductions, and foreign tax credit. We also impute various tax credits such as child care, education, and energy credits, along with capital gains (both short and long term), dividend income (qualified and non-qualified), and other itemized deductions and adjustments.\n\nFor CPS Copy 2, we replace existing CPS income variables with more accurate PUF values, including partnership and S-corp income, interest deduction, unreimbursed business employee expenses, pre-tax contributions, W-2 wages from qualified business, self-employed pension contributions, and charitable cash donations.\n\nWe concatenate these two CPS copies to create the Extended CPS, effectively doubling the dataset size.","type":"content","url":"/methodology#imputed-variables","position":17},{"hierarchy":{"lvl1":"Methodology","lvl3":"Additional Imputations","lvl2":"Stage 1: Variable Imputation"},"type":"lvl3","url":"/methodology#additional-imputations","position":18},{"hierarchy":{"lvl1":"Methodology","lvl3":"Additional Imputations","lvl2":"Stage 1: Variable Imputation"},"content":"Beyond PUF tax variables, we impute variables from three other data sources:\n\nFrom the Survey of Income and Program Participation (SIPP), we impute tip income using predictors including employment income, age, number of children under 18, and number of children under 6.\n\nFrom the Survey of Consumer Finances (SCF), we match auto loan balances based on household demographics and income, then calculate interest on auto loans from these imputed balances. We also impute various net worth components and wealth measures not available in CPS.\n\nFrom the American Community Survey (ACS), we impute property taxes for homeowners based on state of residence, household income, and demographic characteristics. We also impute rent values for specific tenure types where CPS data is incomplete, along with additional housing-related variables.","type":"content","url":"/methodology#additional-imputations","position":19},{"hierarchy":{"lvl1":"Methodology","lvl3":"Example: Tip Income Imputation","lvl2":"Stage 1: Variable Imputation"},"type":"lvl3","url":"/methodology#example-tip-income-imputation","position":20},{"hierarchy":{"lvl1":"Methodology","lvl3":"Example: Tip Income Imputation","lvl2":"Stage 1: Variable Imputation"},"content":"To illustrate how QRF preserves conditional distributions, consider tip income imputation. The training data from SIPP contains workers with employment income and tip income. For a worker with predictors of 30,000 employment income, age 25, and no children, QRF finds that similar workers in SIPP have a conditional distribution ranging from 0 at the 10th percentile (no tips) to 2,000 at the median, 8,000 at the 90th percentile, and 15,000 at the 99th percentile. If the random quantile drawn is 0.85, the imputed tip income would be approximately 6,500. This approach ensures that some similar workers receive no tips while others receive substantial tips, preserving realistic variation.","type":"content","url":"/methodology#example-tip-income-imputation","position":21},{"hierarchy":{"lvl1":"Methodology","lvl2":"Stage 2: Reweighting"},"type":"lvl2","url":"/methodology#stage-2-reweighting","position":22},{"hierarchy":{"lvl1":"Methodology","lvl2":"Stage 2: Reweighting"},"content":"","type":"content","url":"/methodology#stage-2-reweighting","position":23},{"hierarchy":{"lvl1":"Methodology","lvl3":"Problem Formulation","lvl2":"Stage 2: Reweighting"},"type":"lvl3","url":"/methodology#problem-formulation","position":24},{"hierarchy":{"lvl1":"Methodology","lvl3":"Problem Formulation","lvl2":"Stage 2: Reweighting"},"content":"The reweighting stage adjusts household weights to ensure the enhanced dataset matches administrative totals. We optimize log-transformed weights given a loss matrix containing households’ contributions to targets and a target vector of statistics to minimize mean squared relative error. The log transformation ensures positive weights while allowing unconstrained optimization.","type":"content","url":"/methodology#problem-formulation","position":25},{"hierarchy":{"lvl1":"Methodology","lvl3":"Optimization","lvl2":"Stage 2: Reweighting"},"type":"lvl3","url":"/methodology#optimization","position":26},{"hierarchy":{"lvl1":"Methodology","lvl3":"Optimization","lvl2":"Stage 2: Reweighting"},"content":"We use PyTorch for gradient-based optimization with the Adam optimizer. The implementation uses log-transformed weights to ensure positivity constraints are satisfied throughout the optimization process.","type":"content","url":"/methodology#optimization","position":27},{"hierarchy":{"lvl1":"Methodology","lvl3":"Dropout Regularization","lvl2":"Stage 2: Reweighting"},"type":"lvl3","url":"/methodology#dropout-regularization","position":28},{"hierarchy":{"lvl1":"Methodology","lvl3":"Dropout Regularization","lvl2":"Stage 2: Reweighting"},"content":"To prevent overfitting to calibration targets, we apply dropout during optimization. We randomly mask weights each iteration and replace them with the mean of unmasked weights. This helps ensure that no single household receives excessive weight in matching targets.","type":"content","url":"/methodology#dropout-regularization","position":29},{"hierarchy":{"lvl1":"Methodology","lvl3":"Calibration Targets","lvl2":"Stage 2: Reweighting"},"type":"lvl3","url":"/methodology#calibration-targets","position":30},{"hierarchy":{"lvl1":"Methodology","lvl3":"Calibration Targets","lvl2":"Stage 2: Reweighting"},"content":"The loss matrix includes targets from six sources:\n\nIRS SOI: Income by AGI bracket and filing status, counts of returns by category, aggregate income totals by source, deduction and credit utilization rates\n\nCensus: Population by single year of age, state total populations, demographic distributions\n\nCBO/Treasury: SNAP participation and benefits, SSI recipient counts, EITC claims by family size, total federal revenues\n\nJCT: State and local taxes, charitable contributions, mortgage interest, medical expenses\n\nHealthcare: Health insurance premiums, Medicare Part B premiums, medical expenses by age\n\nOther: State program participation, income distributions by geography, local area statistics","type":"content","url":"/methodology#calibration-targets","position":31},{"hierarchy":{"lvl1":"Methodology","lvl3":"Tax and Benefit Calculations","lvl2":"Stage 2: Reweighting"},"type":"lvl3","url":"/methodology#tax-and-benefit-calculations","position":32},{"hierarchy":{"lvl1":"Methodology","lvl3":"Tax and Benefit Calculations","lvl2":"Stage 2: Reweighting"},"content":"The calibration process incorporates tax and benefit calculations through PolicyEngine’s microsimulation capabilities. This ensures that the reweighted dataset reflects income distributions and the interactions between tax liabilities and benefit eligibility.","type":"content","url":"/methodology#tax-and-benefit-calculations","position":33},{"hierarchy":{"lvl1":"Methodology","lvl3":"Convergence","lvl2":"Stage 2: Reweighting"},"type":"lvl3","url":"/methodology#convergence","position":34},{"hierarchy":{"lvl1":"Methodology","lvl3":"Convergence","lvl2":"Stage 2: Reweighting"},"content":"The optimization converges within iterations. We monitor convergence through the loss value trajectory, weight stability across iterations, and target achievement rates.","type":"content","url":"/methodology#convergence","position":35},{"hierarchy":{"lvl1":"Methodology","lvl2":"Validation"},"type":"lvl2","url":"/methodology#validation","position":36},{"hierarchy":{"lvl1":"Methodology","lvl2":"Validation"},"content":"","type":"content","url":"/methodology#validation","position":37},{"hierarchy":{"lvl1":"Methodology","lvl3":"Cross-Validation","lvl2":"Validation"},"type":"lvl3","url":"/methodology#cross-validation","position":38},{"hierarchy":{"lvl1":"Methodology","lvl3":"Cross-Validation","lvl2":"Validation"},"content":"We validate the methodology through three approaches: cross-validation on calibration targets, testing stability across multiple random seeds, and validating imputation quality through out-of-sample prediction on held-out records from source datasets.","type":"content","url":"/methodology#cross-validation","position":39},{"hierarchy":{"lvl1":"Methodology","lvl3":"Quality Checks","lvl2":"Validation"},"type":"lvl3","url":"/methodology#quality-checks","position":40},{"hierarchy":{"lvl1":"Methodology","lvl3":"Quality Checks","lvl2":"Validation"},"content":"Quality checks ensure data integrity. Weights remain positive after optimization. We check weight magnitudes to ensure no single household receives excessive influence on aggregate statistics. Household structures remain intact, with all members of a household receiving the same weight adjustment factor.","type":"content","url":"/methodology#quality-checks","position":41},{"hierarchy":{"lvl1":"Methodology","lvl2":"Implementation"},"type":"lvl2","url":"/methodology#implementation-1","position":42},{"hierarchy":{"lvl1":"Methodology","lvl2":"Implementation"},"content":"The implementation is available at:\n\n\nhttps://​github​.com​/PolicyEngine​/policyengine​-us​-data\n\nKey files:\n\npolicyengine_us_data/datasets/cps/extended_cps.py - Imputation stage\n\npolicyengine_us_data/datasets/cps/enhanced_cps.py - Reweighting stage\n\npolicyengine_us_data/utils/loss.py - Loss matrix construction","type":"content","url":"/methodology#implementation-1","position":43}]}