{"version":"1","records":[{"hierarchy":{"lvl1":"Abstract"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Abstract"},"content":"We present a methodology for creating enhanced microsimulation datasets by combining the\nCurrent Population Survey (CPS) with the IRS Public Use File (PUF). Our approach uses\nquantile regression forests to impute 67 tax variables from the PUF onto CPS records,\npreserving distributional characteristics while maintaining household composition and member\nrelationships. The imputation process alone does not guarantee consistency with official\nstatistics, necessitating a reweighting step to align the combined dataset with known\npopulation totals and administrative benchmarks. We apply a reweighting algorithm that calibrates the dataset to 2,813 targets from the IRS Statistics of Income, Census population projections, Congressional Budget Office benefit program estimates, Treasury expenditure data, Joint Committee on Taxation tax expenditure estimates, healthcare spending patterns, and other benefit program costs. The reweighting employs dropout-regularized gradient descent optimization to ensure consistency with administrative benchmarks. The dataset maintains the CPS’s demographic detail and geographic granularity while\nincorporating tax reporting data from administrative sources. We release the enhanced\ndataset, source code, and documentation to support policy analysis.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Appendix"},"type":"lvl1","url":"/appendix","position":0},{"hierarchy":{"lvl1":"Appendix"},"content":"","type":"content","url":"/appendix","position":1},{"hierarchy":{"lvl1":"Appendix","lvl2":"Appendix A: Implementation Code"},"type":"lvl2","url":"/appendix#appendix-a-implementation-code","position":2},{"hierarchy":{"lvl1":"Appendix","lvl2":"Appendix A: Implementation Code"},"content":"","type":"content","url":"/appendix#appendix-a-implementation-code","position":3},{"hierarchy":{"lvl1":"Appendix","lvl3":"A.1 Quantile Regression Forest Implementation","lvl2":"Appendix A: Implementation Code"},"type":"lvl3","url":"/appendix#a-1-quantile-regression-forest-implementation","position":4},{"hierarchy":{"lvl1":"Appendix","lvl3":"A.1 Quantile Regression Forest Implementation","lvl2":"Appendix A: Implementation Code"},"content":"The following code demonstrates the implementation of Quantile Regression Forests for variable imputation:from quantile_forest import RandomForestQuantileRegressor\n\nqrf = RandomForestQuantileRegressor(\n    n_estimators=100,\n    min_samples_leaf=1,\n    random_state=0\n)","type":"content","url":"/appendix#a-1-quantile-regression-forest-implementation","position":5},{"hierarchy":{"lvl1":"Appendix","lvl3":"A.2 PyTorch Optimization for Reweighting","lvl2":"Appendix A: Implementation Code"},"type":"lvl3","url":"/appendix#a-2-pytorch-optimization-for-reweighting","position":6},{"hierarchy":{"lvl1":"Appendix","lvl3":"A.2 PyTorch Optimization for Reweighting","lvl2":"Appendix A: Implementation Code"},"content":"The reweighting optimization uses PyTorch for gradient-based optimization:import torch\n\n# Initialize with log of original weights\nlog_weights = torch.log(original_weights)\nlog_weights.requires_grad = True\n\n# Adam optimizer\noptimizer = torch.optim.Adam([log_weights], lr=0.1)\n\n# Optimization loop\nfor iteration in range(5000):\n    weights = torch.exp(log_weights)\n    achieved = weights @ loss_matrix\n    relative_errors = (achieved - targets) / targets\n    loss = torch.mean(relative_errors ** 2)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()","type":"content","url":"/appendix#a-2-pytorch-optimization-for-reweighting","position":7},{"hierarchy":{"lvl1":"Appendix","lvl2":"Appendix B: Tables"},"type":"lvl2","url":"/appendix#appendix-b-tables","position":8},{"hierarchy":{"lvl1":"Appendix","lvl2":"Appendix B: Tables"},"content":"","type":"content","url":"/appendix#appendix-b-tables","position":9},{"hierarchy":{"lvl1":"Appendix","lvl3":"Table A1: Complete List of Imputed Variables","lvl2":"Appendix B: Tables"},"type":"lvl3","url":"/appendix#table-a1-complete-list-of-imputed-variables","position":10},{"hierarchy":{"lvl1":"Appendix","lvl3":"Table A1: Complete List of Imputed Variables","lvl2":"Appendix B: Tables"},"content":"","type":"content","url":"/appendix#table-a1-complete-list-of-imputed-variables","position":11},{"hierarchy":{"lvl1":"Appendix","lvl4":"Variables Imputed from IRS Public Use File (67 variables)","lvl3":"Table A1: Complete List of Imputed Variables","lvl2":"Appendix B: Tables"},"type":"lvl4","url":"/appendix#variables-imputed-from-irs-public-use-file-67-variables","position":12},{"hierarchy":{"lvl1":"Appendix","lvl4":"Variables Imputed from IRS Public Use File (67 variables)","lvl3":"Table A1: Complete List of Imputed Variables","lvl2":"Appendix B: Tables"},"content":"Income Variables:\n\nemployment_income\n\npartnership_s_corp_income\n\nsocial_security\n\ntaxable_pension_income\n\ntax_exempt_pension_income\n\nlong_term_capital_gains\n\nshort_term_capital_gains\n\ntaxable_ira_distributions\n\nself_employment_income\n\nqualified_dividend_income\n\nnon_qualified_dividend_income\n\nrental_income\n\ntaxable_unemployment_compensation\n\ntaxable_interest_income\n\ntax_exempt_interest_income\n\nestate_income\n\nmiscellaneous_income\n\nfarm_income\n\nalimony_income\n\nfarm_rent_income\n\nnon_sch_d_capital_gains\n\nlong_term_capital_gains_on_collectibles\n\nunrecaptured_section_1250_gain\n\nsalt_refund_income\n\nDeductions and Adjustments:\n\ninterest_deduction\n\nunreimbursed_business_employee_expenses\n\npre_tax_contributions\n\ncharitable_cash_donations\n\nself_employed_pension_contribution_ald\n\ndomestic_production_ald\n\nself_employed_health_insurance_ald\n\ncharitable_non_cash_donations\n\nalimony_expense\n\nhealth_savings_account_ald\n\nstudent_loan_interest\n\ninvestment_income_elected_form_4952\n\nearly_withdrawal_penalty\n\neducator_expense\n\ndeductible_mortgage_interest\n\nTax Credits:\n\ncdcc_relevant_expenses\n\nforeign_tax_credit\n\namerican_opportunity_credit\n\ngeneral_business_credit\n\nenergy_efficient_home_improvement_credit\n\namt_foreign_tax_credit\n\nexcess_withheld_payroll_tax\n\nsavers_credit\n\nprior_year_minimum_tax_credit\n\nother_credits\n\nQualified Business Income Variables:\n\nw2_wages_from_qualified_business\n\nunadjusted_basis_qualified_property\n\nbusiness_is_sstb\n\nqualified_reit_and_ptp_income\n\nqualified_bdc_income\n\nfarm_operations_income\n\nestate_income_would_be_qualified\n\nfarm_operations_income_would_be_qualified\n\nfarm_rent_income_would_be_qualified\n\npartnership_s_corp_income_would_be_qualified\n\nrental_income_would_be_qualified\n\nself_employment_income_would_be_qualified\n\nOther Tax Variables:\n\ntraditional_ira_contributions\n\nqualified_tuition_expenses\n\ncasualty_loss\n\nunreported_payroll_tax\n\nrecapture_of_investment_credit","type":"content","url":"/appendix#variables-imputed-from-irs-public-use-file-67-variables","position":13},{"hierarchy":{"lvl1":"Appendix","lvl4":"Variables Imputed from Survey of Income and Program Participation (1 variable)","lvl3":"Table A1: Complete List of Imputed Variables","lvl2":"Appendix B: Tables"},"type":"lvl4","url":"/appendix#variables-imputed-from-survey-of-income-and-program-participation-1-variable","position":14},{"hierarchy":{"lvl1":"Appendix","lvl4":"Variables Imputed from Survey of Income and Program Participation (1 variable)","lvl3":"Table A1: Complete List of Imputed Variables","lvl2":"Appendix B: Tables"},"content":"tip_income","type":"content","url":"/appendix#variables-imputed-from-survey-of-income-and-program-participation-1-variable","position":15},{"hierarchy":{"lvl1":"Appendix","lvl4":"Variables Imputed from Survey of Consumer Finances (3 variables)","lvl3":"Table A1: Complete List of Imputed Variables","lvl2":"Appendix B: Tables"},"type":"lvl4","url":"/appendix#variables-imputed-from-survey-of-consumer-finances-3-variables","position":16},{"hierarchy":{"lvl1":"Appendix","lvl4":"Variables Imputed from Survey of Consumer Finances (3 variables)","lvl3":"Table A1: Complete List of Imputed Variables","lvl2":"Appendix B: Tables"},"content":"networth\n\nauto_loan_balance\n\nauto_loan_interest","type":"content","url":"/appendix#variables-imputed-from-survey-of-consumer-finances-3-variables","position":17},{"hierarchy":{"lvl1":"Appendix","lvl4":"Variables Imputed from American Community Survey (2 variables)","lvl3":"Table A1: Complete List of Imputed Variables","lvl2":"Appendix B: Tables"},"type":"lvl4","url":"/appendix#variables-imputed-from-american-community-survey-2-variables","position":18},{"hierarchy":{"lvl1":"Appendix","lvl4":"Variables Imputed from American Community Survey (2 variables)","lvl3":"Table A1: Complete List of Imputed Variables","lvl2":"Appendix B: Tables"},"content":"rent\n\nreal_estate_taxes","type":"content","url":"/appendix#variables-imputed-from-american-community-survey-2-variables","position":19},{"hierarchy":{"lvl1":"Background"},"type":"lvl1","url":"/background","position":0},{"hierarchy":{"lvl1":"Background"},"content":"","type":"content","url":"/background","position":1},{"hierarchy":{"lvl1":"Background","lvl2":"The Microsimulation Landscape"},"type":"lvl2","url":"/background#the-microsimulation-landscape","position":2},{"hierarchy":{"lvl1":"Background","lvl2":"The Microsimulation Landscape"},"content":"Tax and benefit microsimulation models play a role in policy analysis by projecting the distributional and revenue impacts of proposed reforms. Institutions maintaining these models include government agencies like the Congressional Budget Office (CBO), Joint Committee on Taxation (JCT), and Treasury’s Office of Tax Analysis (OTA), as well as non-governmental organizations including the Urban-Brookings Tax Policy Center (TPC), Tax Foundation, Penn Wharton Budget Model (PWBM), Institute on Taxation and Economic Policy (ITEP), Yale Budget Lab, and the open-source Policy Simulation Library (PSL). Each model serves specific institutional needs but faces common data challenges.\n\nThe core challenges these models face stem from the tradeoff between data comprehensiveness and accessibility. Administrative tax data provides income reporting but lacks the household context that models need to analyze benefit programs and family-level impacts \n\nSabelhaus & Johnson, 2020. Survey data captures household relationships and program participation but suffers from income underreporting that worsens at higher income levels \n\nMeyer et al., 2021. The need to protect taxpayer privacy limits data availability because administrators cannot publicly release microdata.","type":"content","url":"/background#the-microsimulation-landscape","position":3},{"hierarchy":{"lvl1":"Background","lvl2":"Data Enhancement Approaches"},"type":"lvl2","url":"/background#data-enhancement-approaches","position":4},{"hierarchy":{"lvl1":"Background","lvl2":"Data Enhancement Approaches"},"content":"Different microsimulation models use various approaches to enhance their underlying data:\n\nGovernment models (CBO, JCT, Treasury) have access to confidential administrative data but cannot share their enhanced microdata. Non-governmental models work with public data, leading to various enhancement strategies. Some organizations use proprietary extracts of tax returns, while others enhance survey data with various methods.\n\nOur enhanced dataset provides an open-source methodology with state identifiers and calibration to state-level targets. This enables analysis of federal-state tax interactions. Researchers can use the dataset with PolicyEngine or other microsimulation models.\n\nThe open-source nature promotes methodological transparency. The modular design allows researchers to substitute alternative imputation or calibration methods while maintaining the overall framework. Regular updates as new CPS and administrative data become available ensure the dataset remains current.","type":"content","url":"/background#data-enhancement-approaches","position":5},{"hierarchy":{"lvl1":"Conclusion"},"type":"lvl1","url":"/conclusion","position":0},{"hierarchy":{"lvl1":"Conclusion"},"content":"We present a methodology for creating enhanced microsimulation datasets that combine the strengths of survey and administrative data sources. The Enhanced CPS dataset demonstrates that careful application of modern statistical methods can substantially improve the data available for policy analysis.","type":"content","url":"/conclusion","position":1},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Summary of Contributions"},"type":"lvl2","url":"/conclusion#summary-of-contributions","position":2},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Summary of Contributions"},"content":"Our work makes several key contributions:\n\nMethodological Innovation: The use of Quantile Regression Forests for imputation preserves distributional characteristics while maintaining computational efficiency. The large-scale calibration to 2,813 targets pushes the boundaries of survey data enhancement.\n\nPractical Tools: We provide open-source implementations that enable researchers to apply, modify, and extend these methods. The modular design facilitates experimentation with alternative approaches.\n\nValidated Dataset: The Enhanced CPS itself serves as a public good for the research community, enabling studies that would otherwise require restricted data access.\n\nReproducible Research: All code, data, and documentation are publicly available, supporting reproducibility and collaborative improvement.","type":"content","url":"/conclusion#summary-of-contributions","position":3},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Key Findings"},"type":"lvl2","url":"/conclusion#key-findings","position":4},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Key Findings"},"content":"The validation results demonstrate that combining survey and administrative data through principled statistical methods can achieve:\n\nImproved income distribution representation\n\nBetter alignment with program participation totals\n\nMaintained demographic and geographic detail\n\nSuitable accuracy for policy simulation\n\nWhile no dataset perfectly represents the full population, the Enhanced CPS provides a pragmatic balance of accuracy, detail, and accessibility.","type":"content","url":"/conclusion#key-findings","position":5},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Implications for Policy Analysis"},"type":"lvl2","url":"/conclusion#implications-for-policy-analysis","position":6},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Implications for Policy Analysis"},"content":"Enhanced microdata availability creates immediate implications for policy analysis. More accurate representation of high incomes enables better analysis of progressive tax reforms and revenue estimates. Researchers can now analyze tax and transfer policies jointly rather than in isolation. Geographic identifiers enable subnational policy analysis not possible with administrative tax data alone. Finally, household structure allows examination of policy impacts across family types and income levels.","type":"content","url":"/conclusion#implications-for-policy-analysis","position":7},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Broader Implications"},"type":"lvl2","url":"/conclusion#broader-implications","position":8},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Broader Implications"},"content":"Beyond the specific dataset, this work demonstrates several broader principles. Combining multiple data sources can overcome individual limitations, showing the value of data integration. Making methods and data publicly available accelerates research progress and demonstrates open science benefits. While perfect data may never exist, pragmatic enhancements can substantially improve analysis capabilities. Furthermore, open-source approaches enable community contributions and continuous improvement.","type":"content","url":"/conclusion#broader-implications","position":9},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Limitations and Future Work"},"type":"lvl2","url":"/conclusion#limitations-and-future-work","position":10},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Limitations and Future Work"},"content":"We acknowledge important limitations including temporal inconsistency between data sources, imputation model assumptions, calibration trade-offs, and validation challenges. Future work should address these through more recent administrative data, enhanced imputation methods, additional validation exercises, and uncertainty quantification.","type":"content","url":"/conclusion#limitations-and-future-work","position":11},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Call to Action"},"type":"lvl2","url":"/conclusion#call-to-action","position":12},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Call to Action"},"content":"We encourage researchers to apply the Enhanced CPS to policy questions where combined demographic and tax detail adds value, compare findings with other data sources and contribute validation results, leverage the open-source nature to make methodological enhancements, and document use cases, limitations discovered, and suggested improvements.","type":"content","url":"/conclusion#call-to-action","position":13},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Final Thoughts"},"type":"lvl2","url":"/conclusion#final-thoughts","position":14},{"hierarchy":{"lvl1":"Conclusion","lvl2":"Final Thoughts"},"content":"The Enhanced CPS represents one approach to a fundamental challenge in microsimulation: the need for comprehensive, accurate microdata. While not perfect, it demonstrates that substantial improvements are possible through careful methodology and open collaboration.\n\nAs data availability evolves and methods advance, this work contributes to a future where policy analysis rests on increasingly solid empirical foundations. Our ultimate goal remains better informed policy decisions that improve social welfare.\n\nThe enhanced dataset, complete documentation, and all source code are available at \n\nhttps://​github​.com​/PolicyEngine​/policyengine​-us​-data.","type":"content","url":"/conclusion#final-thoughts","position":15},{"hierarchy":{"lvl1":"Data Sources"},"type":"lvl1","url":"/data","position":0},{"hierarchy":{"lvl1":"Data Sources"},"content":"Our methodology combines two primary data sources with calibration targets from administrative sources.","type":"content","url":"/data","position":1},{"hierarchy":{"lvl1":"Data Sources","lvl2":"Primary Data Sources"},"type":"lvl2","url":"/data#primary-data-sources","position":2},{"hierarchy":{"lvl1":"Data Sources","lvl2":"Primary Data Sources"},"content":"","type":"content","url":"/data#primary-data-sources","position":3},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Current Population Survey (CPS)","lvl2":"Primary Data Sources"},"type":"lvl3","url":"/data#current-population-survey-cps","position":4},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Current Population Survey (CPS)","lvl2":"Primary Data Sources"},"content":"The Current Population Survey Annual Social and Economic Supplement (ASEC) serves as our base dataset. The Census Bureau and Bureau of Labor Statistics jointly conduct the CPS ASEC, surveying households annually.\n\nThe CPS provides features for microsimulation modeling. It offers a representative sample of US households with demographic information including age, education, race, and employment status. The survey captures family and household relationships through relationship codes that allow reconstruction of tax units and benefit units. Geographic identifiers down to the state level enable subnational policy analysis. The survey includes questions about program participation in transfer programs like SNAP, Medicaid, and housing assistance. The survey collects income data by source, distinguishing between wages, self-employment, interest, dividends, and transfers.\n\nThe CPS faces limitations that necessitate enhancement. Income underreporting is severe at high income levels \n\nRothbaum & Bee, 2021. The survey provides limited tax detail, lacking information on itemized deductions, tax credits, and capital gains realizations. The Census Bureau topcodes high income values to protect confidentiality. The survey’s focus on cash income means it misses non-cash compensation like employer-provided health insurance premiums.","type":"content","url":"/data#current-population-survey-cps","position":5},{"hierarchy":{"lvl1":"Data Sources","lvl3":"IRS Public Use File (PUF)","lvl2":"Primary Data Sources"},"type":"lvl3","url":"/data#irs-public-use-file-puf","position":6},{"hierarchy":{"lvl1":"Data Sources","lvl3":"IRS Public Use File (PUF)","lvl2":"Primary Data Sources"},"content":"The IRS Statistics of Income Public Use File contains tax return information that the IRS draws from a stratified sample of individual income tax returns.\n\nThe PUF provides tax-related variables drawn from filed tax returns. It provides breakdowns of income by source including wages, interest, dividends, capital gains, business income, and retirement distributions. The file includes itemized deductions such as mortgage interest, state and local taxes, and charitable contributions. The file includes tax credits that filers claim, from the earned income tax credit to education credits. The stratified sampling design oversamples high-income returns. Sampling weights allow researchers to produce population-representative estimates.\n\nThe PUF has limitations for policy analysis. The file contains minimal demographic information, limited to filing status and exemptions claimed. The IRS removes geographic identifiers to protect taxpayer privacy, which prevents state-level analysis. The population excludes non-filers. The PUF lacks household structure, preventing analysis of how tax policies interact with transfer programs that operate at the household level.","type":"content","url":"/data#irs-public-use-file-puf","position":7},{"hierarchy":{"lvl1":"Data Sources","lvl2":"Additional Data Sources for Imputation"},"type":"lvl2","url":"/data#additional-data-sources-for-imputation","position":8},{"hierarchy":{"lvl1":"Data Sources","lvl2":"Additional Data Sources for Imputation"},"content":"Beyond the PUF, we incorporate data from three additional surveys to impute specific variables missing from the CPS:","type":"content","url":"/data#additional-data-sources-for-imputation","position":9},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Survey of Income and Program Participation (SIPP)","lvl2":"Additional Data Sources for Imputation"},"type":"lvl3","url":"/data#survey-of-income-and-program-participation-sipp","position":10},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Survey of Income and Program Participation (SIPP)","lvl2":"Additional Data Sources for Imputation"},"content":"The SIPP provides income and program participation data. We use SIPP primarily to impute tip income through a Quantile Regression Forest model trained on SIPP data, using employment income, age, and household composition as predictors.","type":"content","url":"/data#survey-of-income-and-program-participation-sipp","position":11},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Survey of Consumer Finances (SCF)","lvl2":"Additional Data Sources for Imputation"},"type":"lvl3","url":"/data#survey-of-consumer-finances-scf","position":12},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Survey of Consumer Finances (SCF)","lvl2":"Additional Data Sources for Imputation"},"content":"The SCF provides wealth and debt information that we use to impute several financial variables missing from the CPS. We match auto loan balances based on household demographics and income, then calculate interest on auto loans from these imputed balances. Additionally, we impute various net worth components and other wealth measures not available in CPS. The SCF imputation uses their reference person definition to ensure proper matching.","type":"content","url":"/data#survey-of-consumer-finances-scf","position":13},{"hierarchy":{"lvl1":"Data Sources","lvl3":"American Community Survey (ACS)","lvl2":"Additional Data Sources for Imputation"},"type":"lvl3","url":"/data#american-community-survey-acs","position":14},{"hierarchy":{"lvl1":"Data Sources","lvl3":"American Community Survey (ACS)","lvl2":"Additional Data Sources for Imputation"},"content":"The ACS provides housing and geographic data that supplements the CPS housing information. For homeowners, we impute property taxes based on state of residence, household income, and demographic characteristics. We also impute rent values for specific tenure types where CPS data is incomplete, along with additional housing characteristics not captured in the CPS. These imputations use Quantile Regression Forests to preserve distributional characteristics while accounting for household heterogeneity.","type":"content","url":"/data#american-community-survey-acs","position":15},{"hierarchy":{"lvl1":"Data Sources","lvl2":"Calibration Data Sources"},"type":"lvl2","url":"/data#calibration-data-sources","position":16},{"hierarchy":{"lvl1":"Data Sources","lvl2":"Calibration Data Sources"},"content":"The calibration process uses targets from six administrative sources:","type":"content","url":"/data#calibration-data-sources","position":17},{"hierarchy":{"lvl1":"Data Sources","lvl3":"IRS Statistics of Income (SOI)","lvl2":"Calibration Data Sources"},"type":"lvl3","url":"/data#irs-statistics-of-income-soi","position":18},{"hierarchy":{"lvl1":"Data Sources","lvl3":"IRS Statistics of Income (SOI)","lvl2":"Calibration Data Sources"},"content":"The IRS SOI provides tax return aggregates by income level, filing status, and geography. These include counts of returns, aggregate income by source, deduction amounts, and credit utilization.","type":"content","url":"/data#irs-statistics-of-income-soi","position":19},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Census Population Estimates","lvl2":"Calibration Data Sources"},"type":"lvl3","url":"/data#census-population-estimates","position":20},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Census Population Estimates","lvl2":"Calibration Data Sources"},"content":"Census provides population counts by age, state, and other demographic characteristics.","type":"content","url":"/data#census-population-estimates","position":21},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Congressional Budget Office","lvl2":"Calibration Data Sources"},"type":"lvl3","url":"/data#congressional-budget-office","position":22},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Congressional Budget Office","lvl2":"Calibration Data Sources"},"content":"CBO provides projections for program participation and spending, including SNAP benefits, unemployment compensation, and tax revenues.","type":"content","url":"/data#congressional-budget-office","position":23},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Joint Committee on Taxation","lvl2":"Calibration Data Sources"},"type":"lvl3","url":"/data#joint-committee-on-taxation","position":24},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Joint Committee on Taxation","lvl2":"Calibration Data Sources"},"content":"JCT provides estimates of tax expenditures for major deductions and credits.","type":"content","url":"/data#joint-committee-on-taxation","position":25},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Healthcare Spending Data","lvl2":"Calibration Data Sources"},"type":"lvl3","url":"/data#healthcare-spending-data","position":26},{"hierarchy":{"lvl1":"Data Sources","lvl3":"Healthcare Spending Data","lvl2":"Calibration Data Sources"},"content":"Various sources provide data on health insurance premiums, Medicare costs, and medical spending by age group.","type":"content","url":"/data#healthcare-spending-data","position":27},{"hierarchy":{"lvl1":"Data Sources","lvl3":"State Administrative Data","lvl2":"Calibration Data Sources"},"type":"lvl3","url":"/data#state-administrative-data","position":28},{"hierarchy":{"lvl1":"Data Sources","lvl3":"State Administrative Data","lvl2":"Calibration Data Sources"},"content":"State-level program participation and spending data from various state agencies.","type":"content","url":"/data#state-administrative-data","position":29},{"hierarchy":{"lvl1":"Data Sources","lvl2":"Data Access and Documentation"},"type":"lvl2","url":"/data#data-access-and-documentation","position":30},{"hierarchy":{"lvl1":"Data Sources","lvl2":"Data Access and Documentation"},"content":"The enhanced dataset is publicly available through Hugging Face at \n\nhttps://​huggingface​.co​/datasets​/PolicyEngine​/policyengine​-us​-data. We distribute the data as HDF5 files compatible with PolicyEngine and other microsimulation frameworks, with new releases accompanying each CPS vintage.\n\nWe maintain complete documentation of variable definitions, imputation procedures, and calibration targets in the project repository.","type":"content","url":"/data#data-access-and-documentation","position":31},{"hierarchy":{"lvl1":"Discussion"},"type":"lvl1","url":"/discussion","position":0},{"hierarchy":{"lvl1":"Discussion"},"content":"We examine the strengths, limitations, and potential applications of the Enhanced CPS dataset, along with directions for future development.","type":"content","url":"/discussion","position":1},{"hierarchy":{"lvl1":"Discussion","lvl2":"Strengths"},"type":"lvl2","url":"/discussion#strengths","position":2},{"hierarchy":{"lvl1":"Discussion","lvl2":"Strengths"},"content":"","type":"content","url":"/discussion#strengths","position":3},{"hierarchy":{"lvl1":"Discussion","lvl3":"Comprehensive Coverage","lvl2":"Strengths"},"type":"lvl3","url":"/discussion#comprehensive-coverage","position":4},{"hierarchy":{"lvl1":"Discussion","lvl3":"Comprehensive Coverage","lvl2":"Strengths"},"content":"The Enhanced CPS uniquely combines:\n\nDemographic detail from the CPS including state identifiers\n\nTax precision from IRS administrative data\n\nCalibration to contemporary official statistics\n\nOpen-source availability for research use\n\nThis combination enables analyses that would be difficult or impossible with existing public datasets alone.","type":"content","url":"/discussion#comprehensive-coverage","position":5},{"hierarchy":{"lvl1":"Discussion","lvl3":"Methodological Contributions","lvl2":"Strengths"},"type":"lvl3","url":"/discussion#methodological-contributions","position":6},{"hierarchy":{"lvl1":"Discussion","lvl3":"Methodological Contributions","lvl2":"Strengths"},"content":"The use of Quantile Regression Forests for imputation represents an advance over traditional matching methods:\n\nPreserves full conditional distributions\n\nCaptures non-linear relationships\n\nMaintains realistic variable correlations\n\nAllows uncertainty quantification\n\nThe large-scale calibration to 2,813 targets ensures consistency with administrative benchmarks across multiple dimensions simultaneously.","type":"content","url":"/discussion#methodological-contributions","position":7},{"hierarchy":{"lvl1":"Discussion","lvl3":"Practical Advantages","lvl2":"Strengths"},"type":"lvl3","url":"/discussion#practical-advantages","position":8},{"hierarchy":{"lvl1":"Discussion","lvl3":"Practical Advantages","lvl2":"Strengths"},"content":"For policy analysis, the dataset offers several key features: state-level geographic detail for subnational analysis, household structure for distributional studies, tax detail for revenue estimation, program participation for benefit analysis, and calibration to current administrative totals.","type":"content","url":"/discussion#practical-advantages","position":9},{"hierarchy":{"lvl1":"Discussion","lvl2":"Limitations"},"type":"lvl2","url":"/discussion#limitations","position":10},{"hierarchy":{"lvl1":"Discussion","lvl2":"Limitations"},"content":"","type":"content","url":"/discussion#limitations","position":11},{"hierarchy":{"lvl1":"Discussion","lvl3":"Temporal Inconsistency","lvl2":"Limitations"},"type":"lvl3","url":"/discussion#temporal-inconsistency","position":12},{"hierarchy":{"lvl1":"Discussion","lvl3":"Temporal Inconsistency","lvl2":"Limitations"},"content":"The temporal gap between data sources presents a limitation, with 2015 PUF data imputed onto 2024 CPS creating a nine-year gap in underlying populations. This gap means demographic shifts are not fully captured, and tax law changes since 2015 are not reflected in the imputed variables.\n\nWhile we uprate dollar amounts and calibration partially addresses this, we may not reflect fundamental demographic changes.","type":"content","url":"/discussion#temporal-inconsistency","position":13},{"hierarchy":{"lvl1":"Discussion","lvl3":"Imputation Assumptions","lvl2":"Limitations"},"type":"lvl3","url":"/discussion#imputation-assumptions","position":14},{"hierarchy":{"lvl1":"Discussion","lvl3":"Imputation Assumptions","lvl2":"Limitations"},"content":"The QRF imputation assumes that relationships between demographics and tax variables remain stable, seven predictors sufficiently capture variation, the PUF represents the tax-filing population well, and missing data patterns are ignorable.\n\nThese assumptions may not hold perfectly, particularly for subpopulations that the PUF underrepresents.","type":"content","url":"/discussion#imputation-assumptions","position":15},{"hierarchy":{"lvl1":"Discussion","lvl3":"Calibration Trade-offs","lvl2":"Limitations"},"type":"lvl3","url":"/discussion#calibration-trade-offs","position":16},{"hierarchy":{"lvl1":"Discussion","lvl3":"Calibration Trade-offs","lvl2":"Limitations"},"content":"With 2,813 targets, perfect fit to all benchmarks is impossible. The optimization must balance competing objectives across target types, the relative importance of different statistics, stability of resulting weights, and preservation of household relationships.\n\nUsers should consult validation metrics for targets most relevant to their analysis.","type":"content","url":"/discussion#calibration-trade-offs","position":17},{"hierarchy":{"lvl1":"Discussion","lvl2":"Applications"},"type":"lvl2","url":"/discussion#applications","position":18},{"hierarchy":{"lvl1":"Discussion","lvl2":"Applications"},"content":"","type":"content","url":"/discussion#applications","position":19},{"hierarchy":{"lvl1":"Discussion","lvl3":"Tax Policy Analysis","lvl2":"Applications"},"type":"lvl3","url":"/discussion#tax-policy-analysis","position":20},{"hierarchy":{"lvl1":"Discussion","lvl3":"Tax Policy Analysis","lvl2":"Applications"},"content":"The dataset excels at analyzing federal tax reforms through accurate income distribution at high incomes, detailed deduction and credit information, state identifiers for SALT analysis, and household structure for family-based policies.","type":"content","url":"/discussion#tax-policy-analysis","position":21},{"hierarchy":{"lvl1":"Discussion","lvl3":"State and Local Analysis","lvl2":"Applications"},"type":"lvl3","url":"/discussion#state-and-local-analysis","position":22},{"hierarchy":{"lvl1":"Discussion","lvl3":"State and Local Analysis","lvl2":"Applications"},"content":"Unlike the PUF, the Enhanced CPS enables state-level studies including state income tax modeling, geographic variation in federal policies, state-specific program interactions, and regional economic impacts.","type":"content","url":"/discussion#state-and-local-analysis","position":23},{"hierarchy":{"lvl1":"Discussion","lvl3":"Integrated Policy Analysis","lvl2":"Applications"},"type":"lvl3","url":"/discussion#integrated-policy-analysis","position":24},{"hierarchy":{"lvl1":"Discussion","lvl3":"Integrated Policy Analysis","lvl2":"Applications"},"content":"The combination of tax and transfer data supports analysis of universal basic income proposals, earned income tax credit expansions, childcare and family benefit reforms, and healthcare subsidy design.","type":"content","url":"/discussion#integrated-policy-analysis","position":25},{"hierarchy":{"lvl1":"Discussion","lvl3":"Microsimulation Model Development","lvl2":"Applications"},"type":"lvl3","url":"/discussion#microsimulation-model-development","position":26},{"hierarchy":{"lvl1":"Discussion","lvl3":"Microsimulation Model Development","lvl2":"Applications"},"content":"As the foundation for PolicyEngine US, the dataset demonstrates how enhanced microdata improve model capabilities through more accurate baseline distributions, better behavioral response modeling, improved validation against benchmarks, and enhanced credibility of results.","type":"content","url":"/discussion#microsimulation-model-development","position":27},{"hierarchy":{"lvl1":"Discussion","lvl2":"Comparison with Alternatives"},"type":"lvl2","url":"/discussion#comparison-with-alternatives","position":28},{"hierarchy":{"lvl1":"Discussion","lvl2":"Comparison with Alternatives"},"content":"","type":"content","url":"/discussion#comparison-with-alternatives","position":29},{"hierarchy":{"lvl1":"Discussion","lvl3":"Versus Synthetic Data","lvl2":"Comparison with Alternatives"},"type":"lvl3","url":"/discussion#versus-synthetic-data","position":30},{"hierarchy":{"lvl1":"Discussion","lvl3":"Versus Synthetic Data","lvl2":"Comparison with Alternatives"},"content":"Unlike fully synthetic datasets, our approach preserves actual survey responses where possible, imputes only missing tax variables, maintains household relationships, and provides transparent methodology.","type":"content","url":"/discussion#versus-synthetic-data","position":31},{"hierarchy":{"lvl1":"Discussion","lvl3":"Versus Administrative Data","lvl2":"Comparison with Alternatives"},"type":"lvl3","url":"/discussion#versus-administrative-data","position":32},{"hierarchy":{"lvl1":"Discussion","lvl3":"Versus Administrative Data","lvl2":"Comparison with Alternatives"},"content":"While not replacing restricted administrative data, the Enhanced CPS offers public availability, household structure, geographic detail, integration with survey content, and no access restrictions.","type":"content","url":"/discussion#versus-administrative-data","position":33},{"hierarchy":{"lvl1":"Discussion","lvl3":"Versus Other Matching Approaches","lvl2":"Comparison with Alternatives"},"type":"lvl3","url":"/discussion#versus-other-matching-approaches","position":34},{"hierarchy":{"lvl1":"Discussion","lvl3":"Versus Other Matching Approaches","lvl2":"Comparison with Alternatives"},"content":"Compared to traditional statistical matching, QRF better preserves distributions, large-scale calibration ensures consistency, open-source implementation enables replication, and modular design allows improvements.","type":"content","url":"/discussion#versus-other-matching-approaches","position":35},{"hierarchy":{"lvl1":"Discussion","lvl2":"Future Directions"},"type":"lvl2","url":"/discussion#future-directions","position":36},{"hierarchy":{"lvl1":"Discussion","lvl2":"Future Directions"},"content":"","type":"content","url":"/discussion#future-directions","position":37},{"hierarchy":{"lvl1":"Discussion","lvl3":"Methodological Enhancements","lvl2":"Future Directions"},"type":"lvl3","url":"/discussion#methodological-enhancements","position":38},{"hierarchy":{"lvl1":"Discussion","lvl3":"Methodological Enhancements","lvl2":"Future Directions"},"content":"Potential improvements include incorporating additional predictors for imputation, using more recent administrative data when available, developing time-series consistency methods, and adding uncertainty quantification.","type":"content","url":"/discussion#methodological-enhancements","position":39},{"hierarchy":{"lvl1":"Discussion","lvl3":"Additional Data Integration","lvl2":"Future Directions"},"type":"lvl3","url":"/discussion#additional-data-integration","position":40},{"hierarchy":{"lvl1":"Discussion","lvl3":"Additional Data Integration","lvl2":"Future Directions"},"content":"We could incorporate in future versions state tax return data, program administrative records, consumer expenditure information, and health insurance claims data.","type":"content","url":"/discussion#additional-data-integration","position":41},{"hierarchy":{"lvl1":"Discussion","lvl3":"Model Development","lvl2":"Future Directions"},"type":"lvl3","url":"/discussion#model-development","position":42},{"hierarchy":{"lvl1":"Discussion","lvl3":"Model Development","lvl2":"Future Directions"},"content":"We could extend the framework to dynamic microsimulation over time, behavioral response estimation, geographic mobility modeling, and life-cycle analysis.","type":"content","url":"/discussion#model-development","position":43},{"hierarchy":{"lvl1":"Discussion","lvl3":"International Applications","lvl2":"Future Directions"},"type":"lvl3","url":"/discussion#international-applications","position":44},{"hierarchy":{"lvl1":"Discussion","lvl3":"International Applications","lvl2":"Future Directions"},"content":"Researchers could adapt the methodology for other countries facing similar data availability challenges, need for tax-benefit integration, open-source implementation requirements, and cross-national comparison needs.","type":"content","url":"/discussion#international-applications","position":45},{"hierarchy":{"lvl1":"Discussion","lvl2":"Conclusion for Researchers"},"type":"lvl2","url":"/discussion#conclusion-for-researchers","position":46},{"hierarchy":{"lvl1":"Discussion","lvl2":"Conclusion for Researchers"},"content":"The Enhanced CPS provides a valuable resource for policy analysis, though users should understand the limitations (particularly temporal inconsistency), validate results against external benchmarks, consider sensitivity to methodological choices, and contribute improvements to the open-source project.\n\nThe dataset represents a pragmatic solution to data limitations. It enables analyses that advance our understanding of tax and transfer policy impacts while we await improved data access.","type":"content","url":"/discussion#conclusion-for-researchers","position":47},{"hierarchy":{"lvl1":"Introduction"},"type":"lvl1","url":"/introduction","position":0},{"hierarchy":{"lvl1":"Introduction"},"content":"Microsimulation models require high-quality microdata that accurately represent demographic characteristics and economic outcomes. The ideal dataset would combine the demographic richness and household structure of surveys with the income precision of administrative tax records. However, publicly available datasets typically excel in one dimension while lacking in the other.\n\nThe Current Population Survey (CPS) Annual Social and Economic Supplement provides detailed household demographics, family relationships, and program participation data for a representative sample of US households. However, it suffers from well-documented income underreporting, particularly at the top of the distribution. The IRS Public Use File (PUF) contains accurate tax return information but lacks household structure, demographic detail, and state identifiers needed for comprehensive policy analysis.\n\nThis paper presents a methodology for creating an Enhanced CPS dataset that combines the strengths of both sources. Through an enhancement process: imputation followed by reweighting, we create a dataset suitable for analyzing both tax and transfer policies at federal and state levels.","type":"content","url":"/introduction","position":1},{"hierarchy":{"lvl1":"Introduction","lvl2":"Related Work"},"type":"lvl2","url":"/introduction#related-work","position":2},{"hierarchy":{"lvl1":"Introduction","lvl2":"Related Work"},"content":"Researchers have developed several approaches to address the limitations of survey data for microsimulation. Researchers have long used statistical matching techniques to combine datasets. \n\nRadner (1978) pioneered exact matching methods for combining survey and administrative data, while \n\nRodgers (1984) developed statistical matching based on common variables. More recently, \n\nD'Orazio et al. (2006) provided a comprehensive framework for modern statistical matching methods.\n\nEconomic researchers address dataset limitations through various strategies. The Congressional Budget Office combines CPS data with tax return information through statistical matching \n\nCongressional Budget Office, 2022. The Tax Policy Center creates synthetic datasets by statistically matching the CPS to a subset of tax returns \n\nRohaly et al., 2005. However, these approaches often sacrifice either demographic detail or tax precision, which limits their utility for comprehensive policy analysis.\n\nStatistical agencies and researchers employ reweighting methods to align survey data with administrative totals. The Luxembourg Income Study uses calibration to improve cross-national comparability \n\nGornick & Jäntti, 2013. The Urban-Brookings Tax Policy Center employs reweighting in their microsimulation model but relies on proprietary data that cannot be shared publicly \n\nKhitatrakun et al., 2016.\n\nOur approach differs from previous efforts in three key ways. First, we employ quantile regression forests to preserve distributional characteristics during imputation, improving upon traditional hot-deck and regression-based methods that may distort variable relationships. We conduct robustness checks comparing QRF performance to gradient boosting and neural network approaches, finding QRF provides the best balance of accuracy and interpretability. Second, we calibrate to 2,813 targets from multiple administrative sources, far exceeding the scope of previous calibration efforts which typically use fewer than 100 targets. Third, we provide a fully open-source implementation enabling reproducibility and collaborative improvement, addressing the transparency limitations of existing proprietary models.","type":"content","url":"/introduction#related-work","position":3},{"hierarchy":{"lvl1":"Introduction","lvl2":"Contributions"},"type":"lvl2","url":"/introduction#contributions","position":4},{"hierarchy":{"lvl1":"Introduction","lvl2":"Contributions"},"content":"This paper makes three main contributions to the economic and public policy literature. Methodologically, we demonstrate how quantile regression forests can effectively impute detailed tax variables while preserving their joint distribution and relationship to demographics. This advances the statistical matching literature by showing how modern machine learning methods can overcome limitations of traditional hot-deck and parametric approaches. The preservation of distributional characteristics is particularly important for tax policy analysis where outcomes often depend on complex interactions between income sources and household characteristics.\n\nOur empirical contribution involves creating and validating a publicly available enhanced dataset that addresses longstanding data limitations in microsimulation modeling. By combining the demographic richness of the CPS with the tax precision of the PUF, we enable analyses that were previously infeasible with public data. The dataset’s calibration to 2,813 administrative targets ensures consistency with official statistics across multiple dimensions simultaneously.\n\nFrom a practical perspective, we provide open-source tools and comprehensive documentation that enable researchers to apply these methods, modify the approach, or build upon our work. This transparency contrasts with existing proprietary models and supports reproducible research. Government agencies could use our framework to enhance their own microsimulation capabilities, while academic researchers gain access to data suitable for analyzing distributional impacts of tax and transfer policies. The modular design allows incremental improvements as new data sources become available.\n\nWe organize the remainder of this paper as follows. Section 2 describes our data sources including the primary datasets and calibration targets. Section 3 details the enhancement methodology including both the imputation and reweighting stages. Section 4 presents validation results comparing performance across datasets. Section 5 discusses limitations, applications, and future directions. Section 6 concludes with implications for policy analysis.","type":"content","url":"/introduction#contributions","position":5},{"hierarchy":{"lvl1":"Local Area Calibration Setup"},"type":"lvl1","url":"/local-area-calibration-setup","position":0},{"hierarchy":{"lvl1":"Local Area Calibration Setup"},"content":"This notebook demonstrates the clone-based calibration pipeline: how raw CPS records become a calibration matrix and, ultimately, CD-level stacked datasets.\n\nThe paradigm shift from the old approach: instead of replicating every household into every congressional district, we clone each record N times and assign each clone a random census block drawn from a population-weighted distribution. Each clone inherits a state, CD, and block — and gets re-simulated under the rules of its assigned state.\n\nWe follow one household (record_idx=8629, household_id 128694, SNAP $18,396) through the entire pipeline:\n\nClone and assign geography\n\nSimulate under new state rules (_simulate_clone)\n\nGeographic column masking\n\nRe-randomize takeup per census block\n\nBuild the calibration matrix\n\nCreate stacked datasets from calibrated weights\n\nCompanion notebook: \n\ncalibration​_matrix​.ipynb covers the finished matrix — row/column anatomy, target groups, sparsity. This notebook covers the process that creates it and what happens after (stacked datasets).\n\nRequirements: policy_data.db, block_cd_distributions.csv.gz, and the stratified CPS h5 file in STORAGE_FOLDER.\n\n","type":"content","url":"/local-area-calibration-setup","position":1},{"hierarchy":{"lvl1":"Local Area Calibration Setup","lvl2":"Section 1: Setup & Configuration"},"type":"lvl2","url":"/local-area-calibration-setup#section-1-setup-configuration","position":2},{"hierarchy":{"lvl1":"Local Area Calibration Setup","lvl2":"Section 1: Setup & Configuration"},"content":"\n\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict\n\nfrom policyengine_us import Microsimulation\nfrom policyengine_us_data.storage import STORAGE_FOLDER\nfrom policyengine_us_data.calibration.clone_and_assign import (\n    assign_random_geography,\n    GeographyAssignment,\n    load_global_block_distribution,\n)\nfrom policyengine_us_data.calibration.unified_matrix_builder import (\n    UnifiedMatrixBuilder,\n)\nfrom policyengine_us_data.calibration.unified_calibration import (\n    rerandomize_takeup,\n    SIMPLE_TAKEUP_VARS,\n)\nfrom policyengine_us_data.utils.randomness import seeded_rng\nfrom policyengine_us_data.parameters import load_take_up_rate\nfrom policyengine_us_data.datasets.cps.local_area_calibration.calibration_utils import (\n    get_calculated_variables,\n    STATE_CODES,\n    get_all_cds_from_database,\n)\nfrom policyengine_us_data.datasets.cps.local_area_calibration.stacked_dataset_builder import (\n    create_sparse_cd_stacked_dataset,\n)\n\ndb_path = STORAGE_FOLDER / \"calibration\" / \"policy_data.db\"\ndb_uri = f\"sqlite:///{db_path}\"\ndataset_path = str(STORAGE_FOLDER / \"stratified_extended_cps_2024.h5\")\n\nN_CLONES = 3\nSEED = 42\n\n\n\nsim = Microsimulation(dataset=dataset_path)\nhh_ids = sim.calculate(\"household_id\", map_to=\"household\").values\nsnap_values = sim.calculate(\"snap\", map_to=\"household\").values\nn_records = len(hh_ids)\n\nrecord_idx = 8629  # High SNAP ($18k), lands in TX/PA/NY with seed=42\nexample_hh_id = hh_ids[record_idx]\nprint(f\"Base dataset: {n_records:,} households\")\nprint(\n    f\"Example household: record_idx={record_idx}, \"\n    f\"household_id={example_hh_id}, \"\n    f\"SNAP=${snap_values[record_idx]:,.2f}\"\n)\n\n\n\n","type":"content","url":"/local-area-calibration-setup#section-1-setup-configuration","position":3},{"hierarchy":{"lvl1":"Local Area Calibration Setup","lvl2":"Section 2: Geography Assignment"},"type":"lvl2","url":"/local-area-calibration-setup#section-2-geography-assignment","position":4},{"hierarchy":{"lvl1":"Local Area Calibration Setup","lvl2":"Section 2: Geography Assignment"},"content":"assign_random_geography creates n_records * n_clones total records, each assigned a random census block from a population-weighted distribution. State and CD are derived from the block GEOID. The result is a GeographyAssignment dataclass with arrays indexed as clone_idx * n_records + record_idx.\n\ngeography = assign_random_geography(n_records, n_clones=N_CLONES, seed=SEED)\nn_total = n_records * N_CLONES\n\nprint(f\"Total cloned records: {n_total:,}\")\nprint(f\"Unique states: {len(np.unique(geography.state_fips))}\")\nprint(f\"Unique CDs: {len(np.unique(geography.cd_geoid))}\")\nprint(f\"Unique blocks: {len(np.unique(geography.block_geoid))}\")\n\n\n\nprint(\n    f\"Example household (record_idx={record_idx}) across {N_CLONES} clones:\\n\"\n)\nrows = []\nfor c in range(N_CLONES):\n    col = c * n_records + record_idx\n    rows.append(\n        {\n            \"clone\": c,\n            \"col\": col,\n            \"state_fips\": geography.state_fips[col],\n            \"abbr\": STATE_CODES.get(geography.state_fips[col], \"??\"),\n            \"cd_geoid\": geography.cd_geoid[col],\n            \"block_geoid\": geography.block_geoid[col],\n        }\n    )\npd.DataFrame(rows)\n\n\n\n\n\nOne household, three parallel geographic identities. Each clone will be simulated under different state rules, producing different benefit amounts.\n\nNote: With only N_CLONES=3 (~36K total samples), small-population areas like DC may not appear in the random draw. The production pipeline uses N_CLONES=10, which covers all 51 state-equivalents and 436 CDs.\n\nblocks, cds, states, probs = load_global_block_distribution()\nprint(f\"Global block distribution: {len(blocks):,} blocks\")\nprint(f\"Top 5 states by total probability:\")\nstate_prob = pd.Series(probs, index=states).groupby(level=0).sum()\ntop5 = state_prob.nlargest(5)\nfor fips, p in top5.items():\n    print(f\"  {STATE_CODES.get(fips, '??')} ({fips}): {p:.3%}\")\n\n\n\n","type":"content","url":"/local-area-calibration-setup#section-2-geography-assignment","position":5},{"hierarchy":{"lvl1":"Local Area Calibration Setup","lvl2":"Section 3: Inside _simulate_clone — State-Swap"},"type":"lvl2","url":"/local-area-calibration-setup#section-3-inside-simulate-clone-state-swap","position":6},{"hierarchy":{"lvl1":"Local Area Calibration Setup","lvl2":"Section 3: Inside _simulate_clone — State-Swap"},"content":"For each clone, _simulate_clone does four things:\n\nCreates a fresh Microsimulation from the base dataset\n\nOverwrites state_fips with the clone’s assigned states\n\nOptionally calls a sim_modifier (e.g., takeup re-randomization)\n\nClears cached formulas via get_calculated_variables — preserving survey inputs and IDs while forcing recalculation of state-dependent variables like SNAP\n\nLet’s reproduce this manually for clone 0.\n\nclone_idx = 0\ncol_start = clone_idx * n_records\ncol_end = col_start + n_records\nclone_states = geography.state_fips[col_start:col_end]\n\nclone_sim = Microsimulation(dataset=dataset_path)\nclone_sim.set_input(\"state_fips\", 2024, clone_states.astype(np.int32))\nfor var in get_calculated_variables(clone_sim):\n    clone_sim.delete_arrays(var)\n\nnew_snap = clone_sim.calculate(\"snap\", map_to=\"household\").values\n\norig_state = sim.calculate(\"state_fips\", map_to=\"household\").values[record_idx]\nnew_state = clone_states[record_idx]\n\nprint(f\"Example household (record_idx={record_idx}):\")\nprint(\n    f\"  Original state: {STATE_CODES.get(int(orig_state), '??')} \"\n    f\"({int(orig_state)})\"\n)\nprint(\n    f\"  Clone 0 state:  {STATE_CODES.get(int(new_state), '??')} \"\n    f\"({int(new_state)})\"\n)\nprint(f\"  Original SNAP:  ${snap_values[record_idx]:,.2f}\")\nprint(f\"  Clone 0 SNAP:   ${new_snap[record_idx]:,.2f}\")\n\n\n\nprint(f\"SNAP for record_idx={record_idx} across all {N_CLONES} clones:\\n\")\nrows = []\nfor c in range(N_CLONES):\n    cs = geography.state_fips[c * n_records + record_idx]\n    s = Microsimulation(dataset=dataset_path)\n    s.set_input(\n        \"state_fips\",\n        2024,\n        geography.state_fips[c * n_records : (c + 1) * n_records].astype(\n            np.int32\n        ),\n    )\n    for var in get_calculated_variables(s):\n        s.delete_arrays(var)\n    clone_snap = s.calculate(\"snap\", map_to=\"household\").values\n    rows.append(\n        {\n            \"clone\": c,\n            \"state\": STATE_CODES.get(int(cs), \"??\"),\n            \"state_fips\": int(cs),\n            \"SNAP\": f\"${clone_snap[record_idx]:,.2f}\",\n        }\n    )\npd.DataFrame(rows)\n\n\n\n\n\nget_calculated_variables is selective: it identifies variables with formulas (state-dependent computations) while preserving survey-reported inputs and entity IDs. This is what allows the same demographic household to produce different benefit amounts under different state rules.\n\n","type":"content","url":"/local-area-calibration-setup#section-3-inside-simulate-clone-state-swap","position":7},{"hierarchy":{"lvl1":"Local Area Calibration Setup","lvl2":"Section 4: Geographic Column Masking"},"type":"lvl2","url":"/local-area-calibration-setup#section-4-geographic-column-masking","position":8},{"hierarchy":{"lvl1":"Local Area Calibration Setup","lvl2":"Section 4: Geographic Column Masking"},"content":"When assembling the calibration matrix, each target row only “sees” columns (clones) whose geography matches the target’s geography. This is implemented via state_to_cols and cd_to_cols dictionaries built from the GeographyAssignment.\n\nThis is step 3 of build_matrix — reproduced here for transparency.\n\nstate_col_lists = defaultdict(list)\ncd_col_lists = defaultdict(list)\nfor col in range(n_total):\n    state_col_lists[int(geography.state_fips[col])].append(col)\n    cd_col_lists[str(geography.cd_geoid[col])].append(col)\n\nstate_to_cols = {s: np.array(c) for s, c in state_col_lists.items()}\ncd_to_cols = {cd: np.array(c) for cd, c in cd_col_lists.items()}\n\nprint(f\"Unique states mapped: {len(state_to_cols)}\")\nprint(f\"Unique CDs mapped: {len(cd_to_cols)}\")\n\nstate_counts = {s: len(c) for s, c in state_to_cols.items()}\nsc_series = pd.Series(state_counts)\nprint(\n    f\"\\nColumns per state: min={sc_series.min()}, \"\n    f\"median={sc_series.median():.0f}, max={sc_series.max()}\"\n)\n\n\n\nprint(f\"Example household clone visibility:\\n\")\nfor c in range(N_CLONES):\n    col = c * n_records + record_idx\n    state = int(geography.state_fips[col])\n    cd = str(geography.cd_geoid[col])\n    abbr = STATE_CODES.get(state, \"??\")\n    print(f\"Clone {c} ({abbr}, CD {cd}):\")\n    print(\n        f\"  Visible to {abbr} state targets: \"\n        f\"col {col} in state_to_cols[{state}]? \"\n        f\"{col in state_to_cols.get(state, [])}\"\n    )\n    print(\n        f\"  Visible to CD {cd} targets: \"\n        f\"col {col} in cd_to_cols['{cd}']? \"\n        f\"{col in cd_to_cols.get(cd, [])}\"\n    )\n    # Check an unrelated state\n    print(\n        f\"  Visible to NC (37) targets: \" f\"{col in state_to_cols.get(37, [])}\"\n    )\n    print()\n\n\n\nThis is the mechanism behind the sparsity pattern in calibration_matrix.ipynb: a household clone assigned to TX can contribute to TX state targets and TX CD targets, but produces a zero entry for NC or AK targets. The matrix is sparse because each clone only intersects a small fraction of all geographic targets.\n\n","type":"content","url":"/local-area-calibration-setup#section-4-geographic-column-masking","position":9},{"hierarchy":{"lvl1":"Local Area Calibration Setup","lvl2":"Section 5: Takeup Re-randomization"},"type":"lvl2","url":"/local-area-calibration-setup#section-5-takeup-re-randomization","position":10},{"hierarchy":{"lvl1":"Local Area Calibration Setup","lvl2":"Section 5: Takeup Re-randomization"},"content":"The base CPS has fixed takeup decisions (e.g., “this household takes up SNAP”). But when we clone a household into different census blocks, each block should have independently drawn takeup — otherwise every clone of a SNAP-participating household would still participate, regardless of geography.\n\nrerandomize_takeup solves this: for each census block, it uses seeded_rng(variable_name, salt=block_geoid) to draw new takeup booleans. The seed is deterministic per (variable, block) pair, so results are reproducible.\n\nprint(f\"{len(SIMPLE_TAKEUP_VARS)} takeup variables:\\n\")\nfor spec in SIMPLE_TAKEUP_VARS:\n    rate_key = spec[\"rate_key\"]\n    if rate_key == \"voluntary_filing\":\n        rate = 0.05\n    else:\n        rate = load_take_up_rate(rate_key, 2024)\n    rate_str = (\n        f\"{rate:.2%}\"\n        if isinstance(rate, float)\n        else f\"dict ({len(rate)} entries)\"\n    )\n    print(\n        f\"  {spec['variable']:40s} \"\n        f\"entity={spec['entity']:10s} rate={rate_str}\"\n    )\n\n\n\nblock_a = \"482011234567890\"\nblock_b = \"170311234567890\"\nvar = \"takes_up_snap_if_eligible\"\n\nrng_a1 = seeded_rng(var, salt=block_a)\nrng_a2 = seeded_rng(var, salt=block_a)\nrng_b = seeded_rng(var, salt=block_b)\nrng_other = seeded_rng(\"takes_up_aca_if_eligible\", salt=block_a)\n\ndraws_a1 = rng_a1.random(5)\ndraws_a2 = rng_a2.random(5)\ndraws_b = rng_b.random(5)\ndraws_other = rng_other.random(5)\n\nprint(\"Same block + same var (reproducible):\")\nprint(f\"  {draws_a1}\")\nprint(f\"  {draws_a2}\")\nprint(f\"  Match: {np.allclose(draws_a1, draws_a2)}\")\nprint(f\"\\nDifferent block, same var:\")\nprint(f\"  {draws_b}\")\nprint(f\"  Match: {np.allclose(draws_a1, draws_b)}\")\nprint(f\"\\nSame block, different var:\")\nprint(f\"  {draws_other}\")\nprint(f\"  Match: {np.allclose(draws_a1, draws_other)}\")\n\n\n\ntest_sim = Microsimulation(dataset=dataset_path)\nclone_0_states = geography.state_fips[:n_records]\nclone_0_blocks = geography.block_geoid[:n_records]\ntest_sim.set_input(\"state_fips\", 2024, clone_0_states.astype(np.int32))\n\nbefore = {}\nfor spec in SIMPLE_TAKEUP_VARS:\n    v = spec[\"variable\"]\n    vals = test_sim.calculate(v, map_to=spec[\"entity\"]).values\n    before[v] = vals.mean()\n\nrerandomize_takeup(test_sim, clone_0_blocks, clone_0_states, 2024)\n\nprint(\"Takeup rates before/after re-randomization (clone 0):\\n\")\nfor spec in SIMPLE_TAKEUP_VARS:\n    v = spec[\"variable\"]\n    vals = test_sim.calculate(v, map_to=spec[\"entity\"]).values\n    after = vals.mean()\n    print(f\"  {v:40s} before={before[v]:.3%}  after={after:.3%}\")\n\n\n\nmedicaid_rates = load_take_up_rate(\"medicaid\", 2024)\nprint(\"Medicaid takeup rates (state-specific), first 10 states:\\n\")\nfor state, rate in sorted(medicaid_rates.items())[:10]:\n    print(f\"  {state}: {rate:.2%}\")\n\n\n\nIn the full pipeline, rerandomize_takeup is passed to build_matrix as a sim_modifier callback. For each clone, after state_fips is set but before formula caches are cleared, the callback draws new takeup booleans per census block. This means the same household in block A might take up SNAP while in block B it doesn’t — matching the statistical reality that takeup varies by geography.\n\n","type":"content","url":"/local-area-calibration-setup#section-5-takeup-re-randomization","position":11},{"hierarchy":{"lvl1":"Local Area Calibration Setup","lvl2":"Section 6: Matrix Build Verification"},"type":"lvl2","url":"/local-area-calibration-setup#section-6-matrix-build-verification","position":12},{"hierarchy":{"lvl1":"Local Area Calibration Setup","lvl2":"Section 6: Matrix Build Verification"},"content":"Let’s run the full build_matrix pipeline and verify the example household’s pattern matches our Section 4 predictions. We use the same target_filter as in calibration_matrix.ipynb but without sim_modifier to match that notebook’s output.\n\nbuilder = UnifiedMatrixBuilder(\n    db_uri=db_uri,\n    time_period=2024,\n    dataset_path=dataset_path,\n)\n\ntargets_df, X_sparse, target_names = builder.build_matrix(\n    geography,\n    sim,\n    target_filter={\"domain_variables\": [\"snap\"]},\n)\n\nprint(f\"Matrix shape: {X_sparse.shape}\")\nprint(f\"Non-zero entries: {X_sparse.nnz:,}\")\nprint(f\"Density: {X_sparse.nnz / (X_sparse.shape[0] * X_sparse.shape[1]):.6f}\")\n\n\n\n\n\nprint(f\"Example household non-zero pattern across clones:\\n\")\nfor c in range(N_CLONES):\n    col = c * n_records + record_idx\n    col_vec = X_sparse[:, col]\n    nz_rows = col_vec.nonzero()[0]\n    state = int(geography.state_fips[col])\n    cd = geography.cd_geoid[col]\n    abbr = STATE_CODES.get(state, \"??\")\n    print(f\"Clone {c} ({abbr}, CD {cd}): {len(nz_rows)} non-zero rows\")\n    for r in nz_rows:\n        row = targets_df.iloc[r]\n        print(\n            f\"  row {r}: {row['variable']} \"\n            f\"(geo={row['geographic_id']}): \"\n            f\"{X_sparse[r, col]:.2f}\"\n        )\n\n\n\n","type":"content","url":"/local-area-calibration-setup#section-6-matrix-build-verification","position":13},{"hierarchy":{"lvl1":"Local Area Calibration Setup","lvl2":"Section 7: From Weights to Datasets"},"type":"lvl2","url":"/local-area-calibration-setup#section-7-from-weights-to-datasets","position":14},{"hierarchy":{"lvl1":"Local Area Calibration Setup","lvl2":"Section 7: From Weights to Datasets"},"content":"create_sparse_cd_stacked_dataset takes calibrated weights and builds an h5 file with only the non-zero-weight households, reindexed per CD. Internally it does its own state-swap simulation — loading the base dataset, assigning state_fips for the target CD’s state, and recalculating benefits from scratch. This means SNAP values in the output reflect the destination state’s rules (e.g., a 70 SNAP household from ME may get 0 under AK rules).\n\nFormat gap: The calibration produces weights in clone layout (n_records * n_clones,) where each clone maps to one specific CD via the GeographyAssignment. The stacked dataset builder expects CD layout (n_cds * n_households,) where every CD has a weight slot for every household. Converting between these — accumulating clone weights into their assigned CDs — is a separate step not yet implemented. The demo below constructs artificial CD-layout weights directly to show how the builder works.\n\nprint(\"Dimension mismatch:\")\nprint(\n    f\"  Calibration output: ({n_records} * {N_CLONES},) \"\n    f\"= {n_records * N_CLONES:,} (clone layout)\"\n)\n\nall_cds = get_all_cds_from_database(db_uri)\nn_cds = len(all_cds)\nprint(\n    f\"  Stacked builder expects: ({n_cds} * {n_records},) \"\n    f\"= {n_cds * n_records:,} (CD layout)\"\n)\n\n\n\nimport os\n\ndemo_cds = [\"3701\", \"201\"]\nn_demo_cds = len(demo_cds)\n\nw = (\n    np.random.default_rng(42)\n    .binomial(n=1, p=0.01, size=n_demo_cds * n_records)\n    .astype(float)\n)\n\n# Seed our example household into both CDs\ncd_idx_3701 = demo_cds.index(\"3701\")\nw[cd_idx_3701 * n_records + record_idx] = 2.5\n\ncd_idx_201 = demo_cds.index(\"201\")\nw[cd_idx_201 * n_records + record_idx] = 3.5\n\noutput_dir = \"calibration_output\"\nos.makedirs(output_dir, exist_ok=True)\noutput_path = os.path.join(output_dir, \"results.h5\")\n\nprint(\n    f\"Weight vector: {len(w):,} entries \"\n    f\"({n_demo_cds} CDs x {n_records:,} HH)\"\n)\nprint(f\"Non-zero weights: {(w > 0).sum()}\")\nprint(\n    f\"Example HH weight in CD 3701: {w[cd_idx_3701 * n_records + record_idx]}\"\n)\nprint(f\"Example HH weight in CD 201: {w[cd_idx_201 * n_records + record_idx]}\")\n\n\n\ncreate_sparse_cd_stacked_dataset(\n    w,\n    demo_cds,\n    cd_subset=demo_cds,\n    dataset_path=dataset_path,\n    output_path=output_path,\n)\n\n\n\n\n\n\n\n\n\nsim_after = Microsimulation(dataset=f\"./{output_path}\")\nhh_after_df = pd.DataFrame(\n    sim_after.calculate_dataframe(\n        [\n            \"household_id\",\n            \"congressional_district_geoid\",\n            \"household_weight\",\n            \"state_fips\",\n            \"snap\",\n        ]\n    )\n)\nprint(f\"Stacked dataset: {len(hh_after_df)} households\\n\")\n\nmapping_df = pd.read_csv(\n    f\"{output_dir}/mappings/results_household_mapping.csv\"\n)\nexample_mapping = mapping_df.loc[\n    mapping_df.original_household_id == example_hh_id\n]\nprint(f\"Example household (original_id={example_hh_id}) \" f\"in mapping:\\n\")\nprint(example_mapping.to_string(index=False))\n\nnew_ids = example_mapping.new_household_id\nprint(f\"\\nIn stacked dataset:\\n\")\nprint(\n    hh_after_df.loc[hh_after_df.household_id.isin(new_ids)].to_string(\n        index=False\n    )\n)\n\n\n\nimport shutil\n\nshutil.rmtree(output_dir)\nprint(f\"Cleaned up {output_dir}/\")\n\n\n\n","type":"content","url":"/local-area-calibration-setup#section-7-from-weights-to-datasets","position":15},{"hierarchy":{"lvl1":"Local Area Calibration Setup","lvl2":"Summary"},"type":"lvl2","url":"/local-area-calibration-setup#summary","position":16},{"hierarchy":{"lvl1":"Local Area Calibration Setup","lvl2":"Summary"},"content":"The clone-based calibration pipeline has six stages:\n\nClone + assign geography — assign_random_geography() creates N copies of each CPS record, each with a population-weighted random census block.\n\nSimulate — _simulate_clone() sets each clone’s state_fips and recalculates state-dependent benefits.\n\nGeographic masking — state_to_cols / cd_to_cols restrict each target row to geographically relevant columns.\n\nRe-randomize takeup — rerandomize_takeup() draws new takeup per census block, breaking the fixed-takeup assumption.\n\nBuild matrix — UnifiedMatrixBuilder.build_matrix() assembles the sparse CSR matrix from all clones.\n\nStacked datasets — create_sparse_cd_stacked_dataset() converts calibrated weights into CD-level h5 files.\n\nFor matrix diagnostics (row/column anatomy, target groups, sparsity analysis), see \n\ncalibration​_matrix​.ipynb.","type":"content","url":"/local-area-calibration-setup#summary","position":17},{"hierarchy":{"lvl1":"Long Term Projections"},"type":"lvl1","url":"/long-term-projections","position":0},{"hierarchy":{"lvl1":"Long Term Projections"},"content":"","type":"content","url":"/long-term-projections","position":1},{"hierarchy":{"lvl1":"Long Term Projections","lvl2":"Integrating Economic Uprating with Demographic Reweighting"},"type":"lvl2","url":"/long-term-projections#integrating-economic-uprating-with-demographic-reweighting","position":2},{"hierarchy":{"lvl1":"Long Term Projections","lvl2":"Integrating Economic Uprating with Demographic Reweighting"},"content":"","type":"content","url":"/long-term-projections#integrating-economic-uprating-with-demographic-reweighting","position":3},{"hierarchy":{"lvl1":"Long Term Projections","lvl2":"Executive Summary"},"type":"lvl2","url":"/long-term-projections#executive-summary","position":4},{"hierarchy":{"lvl1":"Long Term Projections","lvl2":"Executive Summary"},"content":"This document outlines an innovative approach for projecting federal income tax revenue through 2100 that uniquely combines sophisticated economic microsimulation with demographic reweighting. By harmonizing PolicyEngine’s state-of-the-art tax modeling with Social Security Administration demographic projections, we can isolate and quantify the fiscal impact of population aging while preserving the full complexity of the tax code.","type":"content","url":"/long-term-projections#executive-summary","position":5},{"hierarchy":{"lvl1":"Long Term Projections","lvl2":"The Challenge"},"type":"lvl2","url":"/long-term-projections#the-challenge","position":6},{"hierarchy":{"lvl1":"Long Term Projections","lvl2":"The Challenge"},"content":"Projecting tax revenue over a 75-year horizon requires simultaneously modeling two distinct but interrelated dynamics:\n\nEconomic Evolution: How incomes, prices, and tax parameters change over time\n\nWage growth and income distribution shifts\n\nInflation affecting brackets and deductions\n\nLegislative changes and indexing rules\n\nBehavioral responses to tax policy\n\nDemographic Transformation: How the population structure evolves\n\nBaby boom generation aging through retirement\n\nDeclining birth rates reducing working-age population\n\nIncreasing longevity extending retirement duration\n\nShifting household composition patterns\n\nTraditional approaches typically sacrifice either economic sophistication (using simplified tax calculations) or demographic realism (holding age distributions constant). Our methodology preserves both.","type":"content","url":"/long-term-projections#the-challenge","position":7},{"hierarchy":{"lvl1":"Long Term Projections","lvl2":"Running Projections"},"type":"lvl2","url":"/long-term-projections#running-projections","position":8},{"hierarchy":{"lvl1":"Long Term Projections","lvl2":"Running Projections"},"content":"Run projections using run_household_projection.py:# Save calibrated datasets as .h5 files for each year\npython ../policyengine_us_data/datasets/cps/long_term/run_household_projection.py 2027 --greg --use-ss --use-payroll --save-h5\n\nArguments:\n\nEND_YEAR: Target year for projection (default: 2035)\n\n--greg: Use GREG calibration instead of IPF (optional)\n\n--use-ss: Include Social Security benefit totals as calibration target (requires --greg)\n\n--use-payroll: Include taxable payroll as calibration target (requires --greg)\n\n--save-h5: Save year-specific .h5 files to ./projected_datasets/ directory","type":"content","url":"/long-term-projections#running-projections","position":9},{"hierarchy":{"lvl1":"Long Term Projections","lvl2":"Data Sources"},"type":"lvl2","url":"/long-term-projections#data-sources","position":10},{"hierarchy":{"lvl1":"Long Term Projections","lvl2":"Data Sources"},"content":"The long-term projections use two key SSA datasets:\n\nSSA Population Projections (SSPopJul_TR2024.csv)\n\nSource: \n\nSSA 2024 Trustees Report - Single Year Age Demographic Projections\n\nContains age-specific population projections through 2100\n\nUsed for demographic reweighting to match future population structure\n\nSocial Security Cost Projections (social_security_aux.csv)\n\nSource: \n\nSSA 2025 Trustees Report, Table VI.G9\n\nContains OASDI benefit cost projections in CPI-indexed 2025 dollars\n\nUsed as calibration target in GREG method to ensure fiscal consistency","type":"content","url":"/long-term-projections#data-sources","position":11},{"hierarchy":{"lvl1":"Long Term Projections","lvl3":"Loading SSA Data","lvl2":"Data Sources"},"type":"lvl3","url":"/long-term-projections#loading-ssa-data","position":12},{"hierarchy":{"lvl1":"Long Term Projections","lvl3":"Loading SSA Data","lvl2":"Data Sources"},"content":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nfrom policyengine_us_data.storage import STORAGE_FOLDER\n\n# Load SSA population data\nssa_pop = pd.read_csv(STORAGE_FOLDER / 'SSPopJul_TR2024.csv')\nssa_pop.head()\n\n# Load Social Security auxiliary data\nss_aux = pd.read_csv(STORAGE_FOLDER / 'social_security_aux.csv')\nss_aux.head()","type":"content","url":"/long-term-projections#loading-ssa-data","position":13},{"hierarchy":{"lvl1":"Long Term Projections","lvl2":"Core Innovation"},"type":"lvl2","url":"/long-term-projections#core-innovation","position":14},{"hierarchy":{"lvl1":"Long Term Projections","lvl2":"Core Innovation"},"content":"Our approach operates in two complementary stages:","type":"content","url":"/long-term-projections#core-innovation","position":15},{"hierarchy":{"lvl1":"Long Term Projections","lvl3":"Stage 1: Economic Uprating","lvl2":"Core Innovation"},"type":"lvl3","url":"/long-term-projections#stage-1-economic-uprating","position":16},{"hierarchy":{"lvl1":"Long Term Projections","lvl3":"Stage 1: Economic Uprating","lvl2":"Core Innovation"},"content":"PolicyEngine’s microsimulation engine projects each household’s economic circumstances forward using:\n\nSophisticated Income Modeling\n\nThe system models 17 distinct income categories, each uprated according to its economic fundamentals:\n\nPrimary Categories with Specific Projections:\n\nEmployment income (wages) - follows CBO wage growth projections\n\nSelf-employment income - follows CBO business income projections\n\nCapital gains - follows CBO asset appreciation projections\n\nInterest income - follows CBO interest rate projections\n\nDividend income - follows CBO corporate profit projections\n\nPension income - follows CBO retirement income projections\n\nSocial Security - follows SSA COLA projections (available through 2100)","type":"content","url":"/long-term-projections#stage-1-economic-uprating","position":17},{"hierarchy":{"lvl1":"Long Term Projections","lvl3":"Stage 2: Demographic Reweighting","lvl2":"Core Innovation"},"type":"lvl3","url":"/long-term-projections#stage-2-demographic-reweighting","position":18},{"hierarchy":{"lvl1":"Long Term Projections","lvl3":"Stage 2: Demographic Reweighting","lvl2":"Core Innovation"},"content":"We offer two calibration methods for adjusting household weights to match SSA projections:\n\nMethod 1: Iterative Proportional Fitting (IPF)\n\nTraditional raking approach using Kullback-Leibler divergence\n\nIteratively adjusts weights to match marginal distributions\n\nRobust to specification and always produces non-negative weights\n\nDefault method for backward compatibility\n\nMethod 2: Generalized Regression (GREG) Calibration\n\nModern calibration using chi-squared distance minimization\n\nEnables simultaneous calibration to categorical AND continuous variables\n\nDirect solution via matrix operations (no iteration needed)\n\nRequired for incorporating Social Security benefit constraints","type":"content","url":"/long-term-projections#stage-2-demographic-reweighting","position":19},{"hierarchy":{"lvl1":"Long Term Projections","lvl2":"Demonstrating the Calibration Methods"},"type":"lvl2","url":"/long-term-projections#demonstrating-the-calibration-methods","position":20},{"hierarchy":{"lvl1":"Long Term Projections","lvl2":"Demonstrating the Calibration Methods"},"content":"from policyengine_us_data.datasets.cps.long_term.ssa_data import (\n    load_ssa_age_projections,\n    load_ssa_benefit_projections\n)\n\n# Get SSA population targets for a specific year\nyear = 2025\nage_targets = load_ssa_age_projections(end_year=year)\nprint(f\"\\nAge distribution targets for {year}:\")\nprint(f\"Shape: {age_targets.shape}\")\nprint(f\"Total population: {age_targets[:, 0].sum() / 1000:.1f}M\")\n\n# Get Social Security benefit target\nss_target = load_ssa_benefit_projections(year)\nprint(f\"\\nSocial Security benefit target for {year}: ${ss_target / 1e9:.1f}B\")","type":"content","url":"/long-term-projections#demonstrating-the-calibration-methods","position":21},{"hierarchy":{"lvl1":"Long Term Projections","lvl2":"PWBM Analysis: Eliminating Income Taxes on Social Security Benefits"},"type":"lvl2","url":"/long-term-projections#pwbm-analysis-eliminating-income-taxes-on-social-security-benefits","position":22},{"hierarchy":{"lvl1":"Long Term Projections","lvl2":"PWBM Analysis: Eliminating Income Taxes on Social Security Benefits"},"content":"Source: \n\nEliminating Income Taxes on Social Security Benefits (Penn Wharton Budget Model, February 10, 2025)","type":"content","url":"/long-term-projections#pwbm-analysis-eliminating-income-taxes-on-social-security-benefits","position":23},{"hierarchy":{"lvl1":"Long Term Projections","lvl3":"Policy Analyzed","lvl2":"PWBM Analysis: Eliminating Income Taxes on Social Security Benefits"},"type":"lvl3","url":"/long-term-projections#policy-analyzed","position":24},{"hierarchy":{"lvl1":"Long Term Projections","lvl3":"Policy Analyzed","lvl2":"PWBM Analysis: Eliminating Income Taxes on Social Security Benefits"},"content":"The Penn Wharton Budget Model (PWBM) analyzed a policy proposal to permanently eliminate all income taxes on Social Security benefits, effective January 1, 2025.","type":"content","url":"/long-term-projections#policy-analyzed","position":25},{"hierarchy":{"lvl1":"Long Term Projections","lvl3":"Key Findings","lvl2":"PWBM Analysis: Eliminating Income Taxes on Social Security Benefits"},"type":"lvl3","url":"/long-term-projections#key-findings","position":26},{"hierarchy":{"lvl1":"Long Term Projections","lvl3":"Key Findings","lvl2":"PWBM Analysis: Eliminating Income Taxes on Social Security Benefits"},"content":"Budgetary Impact: The policy is projected to reduce federal revenues by $1.45 trillion over the 10-year budget window (2025-2034). Over the long term, it is projected to increase federal debt by 7 percent by 2054, relative to the current baseline.\n\nMacroeconomic Impact: The analysis finds the policy would have negative long-term effects on the economy.\n\nIt reduces incentives for households to save for retirement and to work.\n\nThis leads to a smaller capital stock (projected to be 4.2% lower by 2054).\n\nThe smaller capital stock results in lower average wages (1.8% lower by 2054) and lower GDP (2.1% lower by 2054).\n\nConventional Distributional Impact (Your Table): The table you shared shows the annual “conventional” effects on household after-tax income.\n\nThe largest average dollar tax cuts go to households in the top 20 percent of the income distribution (quintiles 80-100%).\n\nThe largest relative gains (as a percentage of income) go to households in the fourth quintile (60-80%), who see a 1.6% increase in after-tax income by 2054.\n\nThe dollar amounts shown are in nominal dollars for each specified year, not adjusted to a single base year.\n\nDynamic (Lifetime) Impact: When analyzing the policy’s effects over a household’s entire lifetime, PWBM finds:\n\nThe policy primarily benefits high-income households who are nearing or in retirement.\n\nIt negatively impacts all households under the age of 30 and all future generations, who would experience a net welfare loss due to the long-term effects of lower wages and higher federal debt.","type":"content","url":"/long-term-projections#key-findings","position":27},{"hierarchy":{"lvl1":"Long Term Projections","lvl2":"PolicyEngine’s Analysis of Eliminating Income Taxes on Social Security Benefits"},"type":"lvl2","url":"/long-term-projections#policyengines-analysis-of-eliminating-income-taxes-on-social-security-benefits","position":28},{"hierarchy":{"lvl1":"Long Term Projections","lvl2":"PolicyEngine’s Analysis of Eliminating Income Taxes on Social Security Benefits"},"content":"import sys\nimport os\nimport pandas as pd\nimport numpy as np\nimport gc\n\nfrom policyengine_us import Microsimulation\nfrom policyengine_core.reforms import Reform\n\nWHARTON_BENCHMARKS = {\n    2026: {\n        'First quintile': {'tax_change': 0, 'pct_change': 0.0},\n        'Second quintile': {'tax_change': -15, 'pct_change': 0.0},\n        'Middle quintile': {'tax_change': -340, 'pct_change': 0.5},\n        'Fourth quintile': {'tax_change': -1135, 'pct_change': 1.1},\n        '80-90%': {'tax_change': -1625, 'pct_change': 1.0},\n        '90-95%': {'tax_change': -1590, 'pct_change': 0.7},\n        '95-99%': {'tax_change': -2020, 'pct_change': 0.5},\n        '99-99.9%': {'tax_change': -2205, 'pct_change': 0.2},\n        'Top 0.1%': {'tax_change': -2450, 'pct_change': 0.0},\n    },\n    2034: {\n        'First quintile': {'tax_change': 0, 'pct_change': 0.0},\n        'Second quintile': {'tax_change': -45, 'pct_change': 0.1},\n        'Middle quintile': {'tax_change': -615, 'pct_change': 0.8},\n        'Fourth quintile': {'tax_change': -1630, 'pct_change': 1.2},\n        '80-90%': {'tax_change': -2160, 'pct_change': 1.1},\n        '90-95%': {'tax_change': -2160, 'pct_change': 0.7},\n        '95-99%': {'tax_change': -2605, 'pct_change': 0.6},\n        '99-99.9%': {'tax_change': -2715, 'pct_change': 0.2},\n        'Top 0.1%': {'tax_change': -2970, 'pct_change': 0.0},\n    },\n    2054: {\n        'First quintile': {'tax_change': -5, 'pct_change': 0.0},\n        'Second quintile': {'tax_change': -275, 'pct_change': 0.3},\n        'Middle quintile': {'tax_change': -1730, 'pct_change': 1.3},\n        'Fourth quintile': {'tax_change': -3560, 'pct_change': 1.6},\n        '80-90%': {'tax_change': -4075, 'pct_change': 1.2},\n        '90-95%': {'tax_change': -4385, 'pct_change': 0.9},\n        '95-99%': {'tax_change': -4565, 'pct_change': 0.6},\n        '99-99.9%': {'tax_change': -4820, 'pct_change': 0.2},\n        'Top 0.1%': {'tax_change': -5080, 'pct_change': 0.0},\n    },\n}\n\ndef run_analysis(dataset_path, year, income_rank_var = \"household_net_income\"):\n    \"\"\"Run Option 1 analysis for given dataset and year\"\"\"\n\n    option1_reform = Reform.from_dict(\n        {\n            # Base rate parameters (0-50% bracket)\n            \"gov.irs.social_security.taxability.rate.base.benefit_cap\": {\n                \"2026-01-01.2100-12-31\": 0\n            },\n            \"gov.irs.social_security.taxability.rate.base.excess\": {\n                \"2026-01-01.2100-12-31\": 0\n            },\n            # Additional rate parameters (50-85% bracket)\n            \"gov.irs.social_security.taxability.rate.additional.benefit_cap\": {\n                \"2026-01-01.2100-12-31\": 0\n            },\n            \"gov.irs.social_security.taxability.rate.additional.bracket\": {\n                \"2026-01-01.2100-12-31\": 0\n            },\n            \"gov.irs.social_security.taxability.rate.additional.excess\": {\n                \"2026-01-01.2100-12-31\": 0\n            }\n        }, country_id=\"us\"\n    )\n    reform = Microsimulation(dataset=dataset_path, reform=option1_reform)\n\n    # Get household data\n    household_net_income_reform = reform.calculate(\"household_net_income\", period=year, map_to=\"household\")\n    household_agi_reform = reform.calculate(\"adjusted_gross_income\", period=year, map_to=\"household\")\n    income_tax_reform = reform.calculate(\"income_tax\", period=year, map_to=\"household\")\n\n    del reform\n    gc.collect()\n\n    print(f\"Loading dataset: {dataset_path}\")\n    baseline = Microsimulation(dataset=dataset_path)\n    household_weight = baseline.calculate(\"household_weight\", period=year)\n    household_net_income_baseline = baseline.calculate(\"household_net_income\", period=year, map_to=\"household\")\n    household_agi_baseline = baseline.calculate(\"adjusted_gross_income\", period=year, map_to=\"household\")\n    income_tax_baseline = baseline.calculate(\"income_tax\", period=year, map_to=\"household\")\n\n    # Calculate changes\n    tax_change = income_tax_reform - income_tax_baseline\n    income_change_pct = (\n        (household_net_income_reform - household_net_income_baseline) / household_net_income_baseline\n    ) * 100\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'household_net_income': household_net_income_baseline,\n        'weight': household_weight,\n        'tax_change': tax_change,\n        'income_change_pct': income_change_pct,\n        'income_rank_var': baseline.calculate(income_rank_var, year, map_to=\"household\")\n    })\n\n    # Calculate percentiles\n\n    print(f\"Ranking according to quantiles with: {income_rank_var}\")\n    df['income_percentile'] = df['income_rank_var'].rank(pct=True) * 100\n\n    # Assign income groups\n    def assign_income_group(percentile):\n        if percentile <= 20:\n            return 'First quintile'\n        elif percentile <= 40:\n            return 'Second quintile'\n        elif percentile <= 60:\n            return 'Middle quintile'\n        elif percentile <= 80:\n            return 'Fourth quintile'\n        elif percentile <= 90:\n            return '80-90%'\n        elif percentile <= 95:\n            return '90-95%'\n        elif percentile <= 99:\n            return '95-99%'\n        elif percentile <= 99.9:\n            return '99-99.9%'\n        else:\n            return 'Top 0.1%'\n\n    df['income_group'] = df['income_percentile'].apply(assign_income_group)\n\n    # Calculate aggregate revenue\n    revenue_impact = (income_tax_reform.sum() - income_tax_baseline.sum()) / 1e9\n\n    # Calculate by group\n    results = []\n    for group in ['First quintile', 'Second quintile', 'Middle quintile', 'Fourth quintile',\n                  '80-90%', '90-95%', '95-99%', '99-99.9%', 'Top 0.1%']:\n        group_data = df[df['income_group'] == group]\n        if len(group_data) == 0:\n            continue\n\n        total_weight = group_data['weight'].sum()\n        avg_tax_change = (group_data['tax_change'] * group_data['weight']).sum() / total_weight\n        avg_income_change_pct = (group_data['income_change_pct'] * group_data['weight']).sum() / total_weight\n\n        results.append({\n            'group': group,\n            'pe_tax_change': round(avg_tax_change),\n            'pe_pct_change': round(avg_income_change_pct, 1),\n        })\n\n    return pd.DataFrame(results), revenue_impact\n\ndef generate_comparison_table(pe_results, year):\n    \"\"\"Generate comparison table with Wharton benchmark\"\"\"\n\n    if year not in WHARTON_BENCHMARKS:\n        print(f\"Warning: No Wharton benchmark available for year {year}\")\n        return pe_results\n\n    wharton_data = WHARTON_BENCHMARKS[year]\n\n    comparison = []\n    for _, row in pe_results.iterrows():\n        group = row['group']\n        wharton = wharton_data.get(group, {'tax_change': None, 'pct_change': None})\n\n        pe_tax = row['pe_tax_change']\n        wh_tax = wharton['tax_change']\n\n        comparison.append({\n            'Income Group': group,\n            'PolicyEngine': f\"${pe_tax:,}\",\n            'Wharton': f\"${wh_tax:,}\" if wh_tax is not None else 'N/A',\n            'Difference': f\"${(pe_tax - wh_tax):,}\" if wh_tax is not None else 'N/A',\n            'PE %': f\"{row['pe_pct_change']}%\",\n            'Wharton %': f\"{wharton['pct_change']}%\" if wharton['pct_change'] is not None else 'N/A',\n        })\n\n    return pd.DataFrame(comparison)\n\n# Example usage:\n# dataset_path = 'hf://policyengine/test/2054.h5'\n# year = 2054\n# income_rank_variable = \"household_net_income\"\n# pe_results, revenue_impact = run_analysis(dataset_path, year, income_rank_variable)\n# comparison_table = generate_comparison_table(pe_results, year)","type":"content","url":"/long-term-projections#policyengines-analysis-of-eliminating-income-taxes-on-social-security-benefits","position":29},{"hierarchy":{"lvl1":"Methodology"},"type":"lvl1","url":"/methodology","position":0},{"hierarchy":{"lvl1":"Methodology"},"content":"We create the Enhanced CPS dataset through imputation followed by reweighting. The imputation stage creates a copy of the CPS and uses Quantile Regression Forests to impute tax variables from the PUF onto this copy, creating the Extended CPS. The reweighting stage then optimizes household weights to match administrative targets, producing the Enhanced CPS with weights calibrated to statistics.graph TD\n    subgraph src[\"Source Datasets\"]\n        CPS[\"CPS ASEC\"]:::data\n        PUF[\"IRS PUF\"]:::data\n        SIPP[\"SIPP\"]:::data\n        SCF[\"SCF\"]:::data\n        ACS[\"ACS\"]:::data\n    end\n    \n    Age(\"Age all to target year\"):::process\n    \n    subgraph aged[\"Aged Datasets\"]\n        AgedCPS[\"Aged CPS\"]:::data\n        AgedPUF[\"Aged PUF\"]:::data\n        AgedSIPP[\"Aged SIPP\"]:::data\n        AgedSCF[\"Aged SCF\"]:::data\n        AgedACS[\"Aged ACS\"]:::data\n    end\n    \n    ImpOther(\"Impute SIPP/SCF/ACS variables to CPS\"):::process\n    UpdatedCPS[\"CPS with additional vars\"]:::data\n    \n    Clone(\"Clone CPS\"):::process\n    QRF(\"Train QRF\"):::process\n    \n    Copy1[\"CPS Copy 1: Missing PUF variables filled from PUF\"]:::data\n    Copy2[\"CPS Copy 2: Existing variables replaced from PUF\"]:::data\n    \n    Impute(\"Apply QRF to impute variables\"):::process\n    \n    Concat(\"Concatenate both copies\"):::process\n    \n    Extended[\"Extended CPS - 2x households\"]:::data\n    \n    Targets{{\"Administrative Targets - 2,813\"}}:::data\n    \n    Reweight(\"Reweight Optimization\"):::process\n    \n    Enhanced{{\"Enhanced CPS - Final Dataset\"}}:::output\n    \n    CPS --> Age\n    PUF --> Age\n    SIPP --> Age\n    SCF --> Age\n    ACS --> Age\n    \n    Age --> AgedCPS\n    Age --> AgedPUF\n    Age --> AgedSIPP\n    Age --> AgedSCF\n    Age --> AgedACS\n    \n    AgedSIPP --> ImpOther\n    AgedSCF --> ImpOther\n    AgedACS --> ImpOther\n    AgedCPS --> ImpOther\n    AgedPUF --> QRF\n    \n    ImpOther --> UpdatedCPS\n    UpdatedCPS --> Clone\n    \n    Clone --> Copy1\n    Clone --> Copy2\n    \n    QRF --> Impute\n    Copy1 --> Impute\n    Copy2 --> Impute\n    \n    Impute --> Concat\n    Concat --> Extended\n    \n    Extended --> Reweight\n    Targets --> Reweight\n    Reweight --> Enhanced\n    \n    classDef data fill:#2C6496,stroke:#2C6496,color:#FFFFFF\n    classDef process fill:#39C6C0,stroke:#2C6496,color:#FFFFFF\n    classDef output fill:#5091CC,stroke:#2C6496,color:#FFFFFF","type":"content","url":"/methodology","position":1},{"hierarchy":{"lvl1":"Methodology","lvl2":"Stage 1: Variable Imputation"},"type":"lvl2","url":"/methodology#stage-1-variable-imputation","position":2},{"hierarchy":{"lvl1":"Methodology","lvl2":"Stage 1: Variable Imputation"},"content":"The imputation process begins by aging both the CPS and PUF datasets to the target year, then creating a copy of the aged CPS dataset. This allows us to preserve the original CPS structure while adding imputed tax variables.","type":"content","url":"/methodology#stage-1-variable-imputation","position":3},{"hierarchy":{"lvl1":"Methodology","lvl3":"Data Aging","lvl2":"Stage 1: Variable Imputation"},"type":"lvl3","url":"/methodology#data-aging","position":4},{"hierarchy":{"lvl1":"Methodology","lvl3":"Data Aging","lvl2":"Stage 1: Variable Imputation"},"content":"We age all datasets (CPS, PUF, SIPP, SCF, and ACS) to the target year using population growth factors and income growth indices for input variables only.\n\nWe strip out calculated values like taxes and benefits from the source datasets. We recalculate these only after assembling all inputs.\n\nThis ensures that the imputation models are trained and applied on contemporaneous data.","type":"content","url":"/methodology#data-aging","position":5},{"hierarchy":{"lvl1":"Methodology","lvl3":"Data Cloning Approach","lvl2":"Stage 1: Variable Imputation"},"type":"lvl3","url":"/methodology#data-cloning-approach","position":6},{"hierarchy":{"lvl1":"Methodology","lvl3":"Data Cloning Approach","lvl2":"Stage 1: Variable Imputation"},"content":"We clone the aged CPS dataset to create two versions. The first copy retains original CPS values but fills in variables that don’t exist in CPS with imputed values from the PUF, such as mortgage interest deduction and charitable contributions. The second copy replaces existing CPS income variables with imputed values from the PUF, including wages and salaries, self-employment income, and partnership/S-corp income.\n\nThis dual approach ensures that variables not collected in CPS are added from the PUF, while variables collected in CPS but with measurement error are replaced with more accurate PUF values. Most importantly, household structure and relationships are preserved in both copies.","type":"content","url":"/methodology#data-cloning-approach","position":7},{"hierarchy":{"lvl1":"Methodology","lvl3":"Quantile Regression Forests","lvl2":"Stage 1: Variable Imputation"},"type":"lvl3","url":"/methodology#quantile-regression-forests","position":8},{"hierarchy":{"lvl1":"Methodology","lvl3":"Quantile Regression Forests","lvl2":"Stage 1: Variable Imputation"},"content":"Quantile Regression Forests (QRF) is an extension of random forests that estimates conditional quantiles rather than conditional means. QRF builds an ensemble of decision trees on the training data and stores all observations in leaf nodes rather than just their means. This enables estimation of any quantile of the conditional distribution at prediction time.","type":"content","url":"/methodology#quantile-regression-forests","position":9},{"hierarchy":{"lvl1":"Methodology","lvl4":"QRF Sampling Process","lvl3":"Quantile Regression Forests","lvl2":"Stage 1: Variable Imputation"},"type":"lvl4","url":"/methodology#qrf-sampling-process","position":10},{"hierarchy":{"lvl1":"Methodology","lvl4":"QRF Sampling Process","lvl3":"Quantile Regression Forests","lvl2":"Stage 1: Variable Imputation"},"content":"The key innovation of QRF for imputation is the ability to sample from the conditional distribution rather than using point estimates. The process works as follows:\n\nTrain the model: QRF estimates multiple conditional quantiles (e.g., 10 quantiles from 0 to 1)\n\nGenerate random quantiles: For each CPS record, draw a random quantile from a Beta distribution\n\nSelect imputed value: Use the randomly selected quantile to extract a value from the conditional distribution\n\nThis approach preserves realistic variation and captures conditional tails. For example, a young worker might have low wages most of the time but occasionally have high wages. QRF captures this by allowing the imputation to sometimes draw from the upper tail of the conditional distribution, thus maintaining realistic inequality within demographic groups.","type":"content","url":"/methodology#qrf-sampling-process","position":11},{"hierarchy":{"lvl1":"Methodology","lvl3":"Implementation","lvl2":"Stage 1: Variable Imputation"},"type":"lvl3","url":"/methodology#implementation","position":12},{"hierarchy":{"lvl1":"Methodology","lvl3":"Implementation","lvl2":"Stage 1: Variable Imputation"},"content":"The implementation uses the quantile-forest package, which provides scikit-learn compatible QRF implementation. The aged PUF is subsampled for training efficiency.","type":"content","url":"/methodology#implementation","position":13},{"hierarchy":{"lvl1":"Methodology","lvl3":"Predictor Variables","lvl2":"Stage 1: Variable Imputation"},"type":"lvl3","url":"/methodology#predictor-variables","position":14},{"hierarchy":{"lvl1":"Methodology","lvl3":"Predictor Variables","lvl2":"Stage 1: Variable Imputation"},"content":"Both imputations use the same seven demographic variables available in both datasets: age of the person, gender indicator, tax unit filing status (joint or separate), number of dependents in the tax unit, and tax unit role indicators (head, spouse, or dependent).\n\nThese demographic predictors capture key determinants of income and tax variables while being reliably measured in both datasets.","type":"content","url":"/methodology#predictor-variables","position":15},{"hierarchy":{"lvl1":"Methodology","lvl3":"Imputed Variables","lvl2":"Stage 1: Variable Imputation"},"type":"lvl3","url":"/methodology#imputed-variables","position":16},{"hierarchy":{"lvl1":"Methodology","lvl3":"Imputed Variables","lvl2":"Stage 1: Variable Imputation"},"content":"The process imputes tax-related variables from the PUF in two ways:\n\nFor CPS Copy 1, we add variables that are missing in CPS, including mortgage interest deduction, charitable contributions (both cash and non-cash), state and local tax deductions, medical expense deductions, and foreign tax credit. We also impute various tax credits such as child care, education, and energy credits, along with capital gains (both short and long term), dividend income (qualified and non-qualified), and other itemized deductions and adjustments.\n\nFor CPS Copy 2, we replace existing CPS income variables with more accurate PUF values, including partnership and S-corp income, interest deduction, unreimbursed business employee expenses, pre-tax contributions, W-2 wages from qualified business, self-employed pension contributions, and charitable cash donations.\n\nWe concatenate these two CPS copies to create the Extended CPS, effectively doubling the dataset size.","type":"content","url":"/methodology#imputed-variables","position":17},{"hierarchy":{"lvl1":"Methodology","lvl3":"Additional Imputations","lvl2":"Stage 1: Variable Imputation"},"type":"lvl3","url":"/methodology#additional-imputations","position":18},{"hierarchy":{"lvl1":"Methodology","lvl3":"Additional Imputations","lvl2":"Stage 1: Variable Imputation"},"content":"Beyond PUF tax variables, we impute variables from three other data sources:\n\nFrom the Survey of Income and Program Participation (SIPP), we impute tip income using predictors including employment income, age, number of children under 18, and number of children under 6.\n\nFrom the Survey of Consumer Finances (SCF), we match auto loan balances based on household demographics and income, then calculate interest on auto loans from these imputed balances. We also impute various net worth components and wealth measures not available in CPS.\n\nFrom the American Community Survey (ACS), we impute property taxes for homeowners based on state of residence, household income, and demographic characteristics. We also impute rent values for specific tenure types where CPS data is incomplete, along with additional housing-related variables.","type":"content","url":"/methodology#additional-imputations","position":19},{"hierarchy":{"lvl1":"Methodology","lvl3":"Example: Tip Income Imputation","lvl2":"Stage 1: Variable Imputation"},"type":"lvl3","url":"/methodology#example-tip-income-imputation","position":20},{"hierarchy":{"lvl1":"Methodology","lvl3":"Example: Tip Income Imputation","lvl2":"Stage 1: Variable Imputation"},"content":"To illustrate how QRF preserves conditional distributions, consider tip income imputation. The training data from SIPP contains workers with employment income and tip income.\n\nFor a worker with the following characteristics:\n\nEmployment income: $30,000\n\nAge: 25\n\nNumber of children: 0\n\nQRF finds that similar workers in SIPP have a conditional distribution of tip income:\n\n10th percentile: $0 (no tips)\n\n50th percentile: $2,000\n\n90th percentile: $8,000\n\n99th percentile: $15,000\n\nIf the random quantile drawn is 0.85, the imputed tip income would be approximately $6,500. This approach ensures that some similar workers receive no tips while others receive substantial tips, preserving realistic variation.","type":"content","url":"/methodology#example-tip-income-imputation","position":21},{"hierarchy":{"lvl1":"Methodology","lvl2":"Stage 2: Reweighting"},"type":"lvl2","url":"/methodology#stage-2-reweighting","position":22},{"hierarchy":{"lvl1":"Methodology","lvl2":"Stage 2: Reweighting"},"content":"","type":"content","url":"/methodology#stage-2-reweighting","position":23},{"hierarchy":{"lvl1":"Methodology","lvl3":"Problem Formulation","lvl2":"Stage 2: Reweighting"},"type":"lvl3","url":"/methodology#problem-formulation","position":24},{"hierarchy":{"lvl1":"Methodology","lvl3":"Problem Formulation","lvl2":"Stage 2: Reweighting"},"content":"The reweighting stage adjusts household weights to ensure the enhanced dataset matches administrative totals. We optimize log-transformed weights given a loss matrix containing households’ contributions to targets and a target vector of statistics to minimize mean squared relative error. The log transformation ensures positive weights while allowing unconstrained optimization.","type":"content","url":"/methodology#problem-formulation","position":25},{"hierarchy":{"lvl1":"Methodology","lvl3":"Optimization","lvl2":"Stage 2: Reweighting"},"type":"lvl3","url":"/methodology#optimization","position":26},{"hierarchy":{"lvl1":"Methodology","lvl3":"Optimization","lvl2":"Stage 2: Reweighting"},"content":"We use PyTorch for gradient-based optimization with the Adam optimizer. The implementation uses log-transformed weights to ensure positivity constraints are satisfied throughout the optimization process.","type":"content","url":"/methodology#optimization","position":27},{"hierarchy":{"lvl1":"Methodology","lvl3":"Dropout Regularization","lvl2":"Stage 2: Reweighting"},"type":"lvl3","url":"/methodology#dropout-regularization","position":28},{"hierarchy":{"lvl1":"Methodology","lvl3":"Dropout Regularization","lvl2":"Stage 2: Reweighting"},"content":"To prevent overfitting to calibration targets, we apply dropout during optimization. We randomly mask weights each iteration and replace them with the mean of unmasked weights. This helps ensure that no single household receives excessive weight in matching targets.","type":"content","url":"/methodology#dropout-regularization","position":29},{"hierarchy":{"lvl1":"Methodology","lvl3":"Calibration Targets","lvl2":"Stage 2: Reweighting"},"type":"lvl3","url":"/methodology#calibration-targets","position":30},{"hierarchy":{"lvl1":"Methodology","lvl3":"Calibration Targets","lvl2":"Stage 2: Reweighting"},"content":"The loss matrix includes targets from six sources:\n\nIRS SOI: Income by AGI bracket and filing status, counts of returns by category, aggregate income totals by source, deduction and credit utilization rates\n\nCensus: Population by single year of age, state total populations, demographic distributions\n\nCBO/Treasury: SNAP participation and benefits, SSI recipient counts, EITC claims by family size, total federal revenues\n\nJCT: State and local taxes, charitable contributions, mortgage interest, medical expenses\n\nHealthcare: Health insurance premiums, Medicare Part B premiums, medical expenses by age\n\nOther: State program participation, income distributions by geography, local area statistics","type":"content","url":"/methodology#calibration-targets","position":31},{"hierarchy":{"lvl1":"Methodology","lvl3":"Tax and Benefit Calculations","lvl2":"Stage 2: Reweighting"},"type":"lvl3","url":"/methodology#tax-and-benefit-calculations","position":32},{"hierarchy":{"lvl1":"Methodology","lvl3":"Tax and Benefit Calculations","lvl2":"Stage 2: Reweighting"},"content":"The calibration process incorporates tax and benefit calculations through PolicyEngine’s microsimulation capabilities. This ensures that the reweighted dataset reflects income distributions and the interactions between tax liabilities and benefit eligibility.","type":"content","url":"/methodology#tax-and-benefit-calculations","position":33},{"hierarchy":{"lvl1":"Methodology","lvl3":"Convergence","lvl2":"Stage 2: Reweighting"},"type":"lvl3","url":"/methodology#convergence","position":34},{"hierarchy":{"lvl1":"Methodology","lvl3":"Convergence","lvl2":"Stage 2: Reweighting"},"content":"The optimization converges within 500 epochs. We monitor convergence through the loss value trajectory, weight stability across iterations, and target achievement rates.","type":"content","url":"/methodology#convergence","position":35},{"hierarchy":{"lvl1":"Methodology","lvl2":"Validation"},"type":"lvl2","url":"/methodology#validation","position":36},{"hierarchy":{"lvl1":"Methodology","lvl2":"Validation"},"content":"","type":"content","url":"/methodology#validation","position":37},{"hierarchy":{"lvl1":"Methodology","lvl3":"Cross-Validation","lvl2":"Validation"},"type":"lvl3","url":"/methodology#cross-validation","position":38},{"hierarchy":{"lvl1":"Methodology","lvl3":"Cross-Validation","lvl2":"Validation"},"content":"We validate the methodology through three approaches: cross-validation on calibration targets, testing stability across multiple random seeds, and validating imputation quality through out-of-sample prediction on held-out records from source datasets.","type":"content","url":"/methodology#cross-validation","position":39},{"hierarchy":{"lvl1":"Methodology","lvl3":"Quality Checks","lvl2":"Validation"},"type":"lvl3","url":"/methodology#quality-checks","position":40},{"hierarchy":{"lvl1":"Methodology","lvl3":"Quality Checks","lvl2":"Validation"},"content":"Quality checks ensure data integrity. Weights remain positive after optimization. We check weight magnitudes to ensure no single household receives excessive influence on aggregate statistics. Household structures remain intact, with all members of a household receiving the same weight adjustment factor.","type":"content","url":"/methodology#quality-checks","position":41},{"hierarchy":{"lvl1":"Methodology","lvl2":"Implementation"},"type":"lvl2","url":"/methodology#implementation-1","position":42},{"hierarchy":{"lvl1":"Methodology","lvl2":"Implementation"},"content":"The implementation is available at:\n\n\nhttps://​github​.com​/PolicyEngine​/policyengine​-us​-data\n\nKey files:\n\npolicyengine_us_data/datasets/cps/extended_cps.py - Imputation stage\n\npolicyengine_us_data/datasets/cps/enhanced_cps.py - Reweighting stage\n\npolicyengine_us_data/utils/loss.py - Loss matrix construction","type":"content","url":"/methodology#implementation-1","position":43}]}