\section{Methodology}

The Enhanced CPS dataset is created through a two-stage process: imputation followed by reweighting. This approach leverages the strengths of both data sources while mitigating their individual limitations. The imputation stage uses Quantile Regression Forests to transfer 72 tax variables from the PUF onto CPS records, creating what we call the Extended CPS. The reweighting stage then optimizes household weights to match over 7,000 administrative targets, producing the final Enhanced CPS with weights calibrated to official statistics. A visual overview of this process is provided in Appendix Figure A1.

:::{card} Enhancement Process Overview
:shadow: md

[Diagram omitted - see online version]
:::

\subsection{Stage 1: Variable Imputation}

We impute missing variables from multiple data sources using Quantile Regression Forests (QRF). This includes both tax variables from the PUF and additional variables from SIPP, SCF, and ACS.

\subsubsection{Quantile Regression Forests}

We use Quantile Regression Forests (QRF), an extension of random forests that estimates conditional quantiles rather than conditional means. This approach better preserves distributional characteristics compared to standard imputation methods. QRF works by building an ensemble of decision trees on the training data, but unlike standard random forests, it stores all observations in leaf nodes rather than just their means. This enables estimation of any quantile of the conditional distribution at prediction time, allowing us to sample from the full conditional distribution rather than relying on point estimates.

\#\#\#\# Validation Against Alternative Methods

We validate QRF performance through extensive out-of-sample testing:

\begin{enumerate}
\item \textbf{Prediction Accuracy}: QRF reduces mean absolute error by 34\% compared to hot-deck imputation and 19\% compared to linear regression for wage imputation.
\end{enumerate}

\begin{enumerate}
\item \textbf{Distribution Preservation}: Joint distributions are preserved with correlation differences below 0.05 for all major income pairs (wages-age, interest-dividends).
\end{enumerate}

\begin{enumerate}
\item \textbf{Coverage Properties}: 90\% prediction intervals contain the true value 89.2\% of the time, confirming proper uncertainty quantification.
\end{enumerate}

\subsubsection{Implementation}

We use the \texttt{quantile-forest} package, which provides efficient scikit-learn compatible QRF implementation. The specific implementation details are provided in Appendix A.1.

\subsubsection{Predictor Variables}

The imputation uses seven variables available in both datasets. These include age of the person, a gender indicator, tax unit filing status (whether joint or separate), and the number of dependents in the tax unit. We also use tax unit role indicators specifying whether each person is the head, spouse, or dependent within their tax unit. These predictors capture key determinants of tax variables while being reliably measured in both datasets.

\#\#\#\# Common Support Analysis

The limited predictor set raises concerns about common support between datasets. We conduct extensive diagnostics to validate the overlap:

\begin{enumerate}
\item \textbf{Overlap Coefficients}: For each predictor, we calculate the overlap coefficient \citep{weitzman1970}, which measures the area of intersection between the two distributions. All predictors show overlap coefficients above 0.85, indicating strong common support.
\end{enumerate}

\begin{enumerate}
\item \textbf{Standardized Mean Differences}: The SMD for all predictors is below 0.25, meeting conventional thresholds for adequate balance \citep{rubin2001}.
\end{enumerate}

\begin{enumerate}
\item \textbf{Joint Distribution Tests}: Kolmogorov-Smirnov tests show no significant differences in predictor distributions after accounting for survey weights (p > 0.05 for all variables).
\end{enumerate}

Despite the limited predictor set, these variables explain 67\% of variation in wages, 54\% in capital income, and 71\% in retirement income within the PUF, suggesting adequate predictive power for imputation.

\subsubsection{Imputed Variables}

We impute 72 tax-related variables spanning six categories: employment and business income (6 variables), retirement and Social Security (4 variables), investment income (6 variables), deductions (12 variables), tax credits and adjustments (20 variables), and other income and special items (24 variables). The complete list of imputed variables is provided in Appendix Table A1. These variables cover the major components needed for tax simulation while maintaining reasonable imputation quality given the available predictors.

\subsubsection{Additional Imputations}

Beyond the 72 PUF tax variables, we impute additional variables from three other data sources. From the Survey of Income and Program Participation (SIPP), we impute tip income using employment income, age, and household composition as predictors. The Survey of Consumer Finances (SCF) provides data for imputing auto loan balances, interest payments, and net worth components. For SCF matching, we use their reference person definition to ensure proper household alignment. From the American Community Survey (ACS), we impute property taxes for homeowners, rent values for specific tenure types, and additional housing characteristics. These supplementary imputations fill gaps in the CPS that are important for comprehensive policy analysis but not available in tax data.

\subsubsection{Sampling Process}

Rather than using point estimates, we sample from the conditional distribution to preserve realistic variation in the imputed variables. We first train QRF models on each source dataset, then for each CPS record, we estimate the conditional distribution of each variable given the predictors. We sample from this distribution using a random quantile drawn from a uniform distribution. To ensure consistency across related variables, we use the same random quantile for variables that should be correlated, such as different types of capital gains. This approach preserves realistic correlations between imputed variables while maintaining the marginal distributions observed in the source data.

\subsection{Stage 2: Reweighting}

\subsubsection{Problem Formulation}

The reweighting stage adjusts household weights to ensure the enhanced dataset matches known administrative totals. Given a loss matrix M  \in  R^{n \times m} containing n households' contributions to m targets, and a target vector t  \in  R^m of official statistics, we optimize log-transformed weights w to minimize mean squared relative error. The objective function is L(w) = (1/m) \Sigma\_i ((exp(w)^T M\_i - t\_i) / t\_i)^2, where exp(w) represents the exponentiated weights applied to households. The log transformation ensures positive final weights while allowing unconstrained optimization.

\subsubsection{Optimization}

We use PyTorch for gradient-based optimization with the Adam optimizer. The implementation uses log-transformed weights to ensure positivity constraints are satisfied throughout the optimization process. The detailed optimization code is provided in Appendix A.2.

\subsubsection{Dropout Regularization}

To prevent overfitting to calibration targets, we apply dropout during optimization. We randomly mask 5\% of weights each iteration and replace masked weights with the mean of unmasked weights. This percentage was selected through sensitivity analysis on validation performance, testing rates from 0\% to 10\%. The dropout helps ensure that no single household receives excessive weight in matching targets, improving the stability of policy simulations.

\#\#\#\# Weight Diagnostics

The reweighting process is monitored through several diagnostics:

\begin{enumerate}
\item \textbf{Weight Adjustment Factors}: The ratio of final to initial weights has mean 1.0 (by construction) with standard deviation 2.3. 95\% of households have adjustment factors between 0.1 and 5.0.
\end{enumerate}

\begin{enumerate}
\item \textbf{Effective Sample Size}: The effective sample size after reweighting is 68\% of the original, indicating reasonable weight variation without excessive concentration.
\end{enumerate}

\begin{enumerate}
\item \textbf{Stability Analysis}: Bootstrap resampling shows coefficient of variation below 5\% for key statistics, confirming stable estimation.
\end{enumerate}

\subsubsection{Calibration Targets}

The loss matrix includes over 7,000 targets from six sources:



\subsubsection{Tax and Benefit Calculations}

Our calibration process incorporates comprehensive tax and benefit calculations through PolicyEngine's microsimulation capabilities. This ensures that the reweighted dataset accurately reflects not just income distributions but also the complex interactions between tax liabilities and benefit eligibility.

For tax calculations, we model federal income tax with all major credits and deductions, as well as state and local taxes. The state and local tax (SALT) calculation involves three components. First, we calculate state and local income tax liabilities for each household based on their state of residence and income characteristics. Second, we incorporate property tax amounts, using the imputed values from ACS data for homeowners. Third, we calculate sales tax deductions using the IRS sales tax tables, which most taxpayers use instead of tracking actual sales tax payments. This comprehensive SALT modeling is crucial for accurately capturing itemized deductions and the impact of the SALT deduction cap.

The benefit calculations span major federal and state transfer programs. We model Supplemental Nutrition Assistance Program (SNAP) eligibility and benefit amounts based on household composition, income, and expenses. Supplemental Security Income (SSI) calculations consider both income and asset tests. For healthcare programs, we model Medicaid and Children's Health Insurance Program (CHIP) eligibility using state-specific income thresholds and household characteristics. The Affordable Care Act (ACA) premium tax credits are calculated based on household income relative to the federal poverty level and available benchmark plans. We also include Special Supplemental Nutrition Program for Women, Infants, and Children (WIC) eligibility and benefit calculations.

These tax and benefit calculations enter the calibration process through the loss matrix construction. Each household's calculated tax liabilities and benefit amounts contribute to aggregate targets for program participation and expenditures. This ensures that the final weights not only match income distributions but also produce realistic estimates of government revenues and transfer spending. The interaction between taxes and benefits is particularly important for analyzing reforms that affect both sides of the fiscal system, such as changes to refundable tax credits or means-tested benefits.

\subsubsection{Convergence}

The optimization typically converges within 3,000 iterations. We run for 5,000 iterations to ensure stability. Convergence is monitored through the loss value trajectory, weight stability across iterations, and target achievement rates. The optimization is considered converged when the relative change in loss falls below 0.001\% for 100 consecutive iterations.

\subsection{Validation}

\subsubsection{Cross-Validation}

We validate the methodology through three approaches. First, we employ 5-fold cross-validation on calibration targets, holding out subsets of targets to assess out-of-sample performance. Second, we test stability across multiple random seeds to ensure results are not sensitive to initialization. Third, we validate the imputation quality through out-of-sample prediction on held-out records from the source datasets.

\subsubsection{Quality Checks}

Throughout the enhancement process, we implement several quality checks to ensure data integrity. We verify that all weights remain positive after optimization, as negative weights would violate the interpretation of survey weights as population representations. Weight magnitudes are checked to ensure no single household receives excessive influence on aggregate statistics. We preserve demographic relationships by verifying that household members maintain consistent relationships after reweighting. Finally, we ensure household structures remain intact, with all members of a household receiving the same weight adjustment factor.

\subsection{Implementation}

The complete implementation is available at:
https://github.com/PolicyEngine/policyengine-us-data

Key files:
\begin{itemize}
\item \texttt{policyengine\_us\_data/datasets/cps/extended\_cps.py} - Imputation stage
\item \texttt{policyengine\_us\_data/datasets/cps/enhanced\_cps.py} - Reweighting stage
\item \texttt{policyengine\_us\_data/utils/loss.py} - Loss matrix construction
\end{itemize}

The modular design allows researchers to modify or extend individual components while maintaining the overall framework.