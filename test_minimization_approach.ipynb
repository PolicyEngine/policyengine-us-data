{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6dc9cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from policyengine_us_data.utils.minimize import minimize_dataset, random_sampling_minimization, candidate_loss_contribution\n",
    "from policyengine_us_data.storage import STORAGE_FOLDER\n",
    "from policyengine_us import Microsimulation\n",
    "from policyengine_us_data.datasets.cps.enhanced_cps import reweight, ExtendedCPS_2024\n",
    "from policyengine_us_data.utils import build_loss_matrix\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "bad_targets = [\n",
    "    \"nation/irs/adjusted gross income/total/AGI in 10k-15k/taxable/Head of Household\",\n",
    "    \"nation/irs/adjusted gross income/total/AGI in 15k-20k/taxable/Head of Household\",\n",
    "    \"nation/irs/adjusted gross income/total/AGI in 10k-15k/taxable/Married Filing Jointly/Surviving Spouse\",\n",
    "    \"nation/irs/adjusted gross income/total/AGI in 15k-20k/taxable/Married Filing Jointly/Surviving Spouse\",\n",
    "    \"nation/irs/count/count/AGI in 10k-15k/taxable/Head of Household\",\n",
    "    \"nation/irs/count/count/AGI in 15k-20k/taxable/Head of Household\",\n",
    "    \"nation/irs/count/count/AGI in 10k-15k/taxable/Married Filing Jointly/Surviving Spouse\",\n",
    "    \"nation/irs/count/count/AGI in 15k-20k/taxable/Married Filing Jointly/Surviving Spouse\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683fd57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of household entity in the dataset measured through household_weight:\n",
    "\n",
    "# Original ECPS 2024 dataset size: 41310\n",
    "# Through \"random_sampling_minimization\" with 0.5 of the dataset being pruned: 20655\n",
    "# Through \"random_sampling_minimization\" with 0.2 of the dataset being pruned: 33408\n",
    "# After minimization through \"candidate_loss_contribution\" and a 1.0 max error change: 20655 \n",
    "# After minimization through \"candidate_loss_contribution\" and a 0.001 max error change: 24786"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99994d3",
   "metadata": {},
   "source": [
    "# Enhanced_CPS_2024.py Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db975ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  3.00it/s, loss=9.1e-5, loss_rel_change=-0.809] \n",
      "100%|██████████| 10/10 [00:03<00:00,  2.96it/s, loss=0.000181, loss_rel_change=-0.679]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.98it/s, loss=0.00108, loss_rel_change=-0.273]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.59it/s, loss=0.0101, loss_rel_change=-0.0377]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.46it/s, loss=0.1, loss_rel_change=-0.00391]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.52it/s, loss=0.000191, loss_rel_change=-0.672]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.89it/s, loss=0.00116, loss_rel_change=-0.274]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.57it/s, loss=0.00978, loss_rel_change=-0.166]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.34it/s, loss=0.0881, loss_rel_change=-0.22]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.55it/s, loss=0.866, loss_rel_change=-0.23]\n",
      "100%|██████████| 10/10 [00:03<00:00,  3.31it/s, loss=9.12e-5, loss_rel_change=-0.812]\n",
      "100%|██████████| 10/10 [00:03<00:00,  3.26it/s, loss=0.00018, loss_rel_change=-0.687]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.47it/s, loss=0.00108, loss_rel_change=-0.263]\n",
      "100%|██████████| 10/10 [00:03<00:00,  3.21it/s, loss=0.0101, loss_rel_change=-0.0373]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.37it/s, loss=0.1, loss_rel_change=-0.00383]\n",
      "100%|██████████| 10/10 [00:03<00:00,  3.28it/s, loss=0.00389, loss_rel_change=-0.875]\n",
      "100%|██████████| 10/10 [00:03<00:00,  3.17it/s, loss=0.0328, loss_rel_change=-0.894]\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.72it/s, loss=0.321, loss_rel_change=-0.896]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.49it/s, loss=3.21, loss_rel_change=-0.896]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.37it/s, loss=32.1, loss_rel_change=-0.896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CALIBRATION LOG DEBUG ===\n",
      "File path: /Users/elenacura/Desktop/PolicyEngine/policyengine-us-data/policyengine_us_data/storage/enhanced_cps_2024.h5\n",
      "Epoch: 0\n",
      "Number of households: 41310\n",
      "Total weight: 12764381616743.21\n",
      "Weight range: 0.54 to 1303728.75\n",
      "Loss matrix shape: (41310, 2813)\n",
      "Number of targets: 2813\n",
      "After filtering bad targets:\n",
      "Loss matrix clean shape: (41310, 2805)\n",
      "Number of clean targets: 2805\n",
      "Estimates shape: (2805,)\n",
      "Estimates sum: 324584770671300.88\n",
      "First 3 estimates: nation/irs/adjusted gross income/total/AGI in -inf-inf/taxable/All    1.498784e+13\n",
      "nation/irs/adjusted gross income/total/AGI in 10k-15k/taxable/All     1.609638e+10\n",
      "nation/irs/adjusted gross income/total/AGI in 15k-20k/taxable/All     6.707770e+10\n",
      "dtype: float64\n",
      "First 3 targets: [1.62972204e+13 1.68634879e+10 6.76819729e+10]\n",
      "Mean absolute error: 17235490830.73\n",
      "Mean relative error: 0.0997\n",
      "=== END DEBUG ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:38<00:00,  2.54it/s, loss=3.62e-5, loss_rel_change=-0.301]\n",
      "100%|██████████| 250/250 [01:35<00:00,  2.62it/s, loss=3.58e-5, loss_rel_change=-0.294]\n",
      "100%|██████████| 250/250 [01:33<00:00,  2.68it/s, loss=3.34e-5, loss_rel_change=-0.376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight relative change: 99.95%\n",
      "Re-calibrating final selected households...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m         output_path \u001b[38;5;241m=\u001b[39m STORAGE_FOLDER \u001b[38;5;241m/\u001b[39m approach \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_enhanced_cps_2024_minimised.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m         output_path\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 92\u001b[0m         \u001b[43mminimise_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m            \u001b[49m\u001b[43mminimization_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminimization_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget_fractions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m params \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_rel_change_max\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n",
      "File \u001b[0;32m~/Desktop/PolicyEngine/policyengine-us-data/policyengine_us_data/utils/minimise.py:430\u001b[0m, in \u001b[0;36mminimise_dataset\u001b[0;34m(dataset, output_path, minimization_function, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# Re-calibrate the final selected households to hit targets\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-calibrating final selected households...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 430\u001b[0m calibrated_weights \u001b[38;5;241m=\u001b[39m \u001b[43mreweight\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_matrix_clean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Convert to numpy array\u001b[39;49;00m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets_clean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduced epochs for faster processing\u001b[39;49;00m\n\u001b[1;32m    435\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m sim\u001b[38;5;241m.\u001b[39mset_input(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhousehold_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2024\u001b[39m, calibrated_weights)\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal calibration completed successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/PolicyEngine/policyengine-us-data/policyengine_us_data/datasets/cps/enhanced_cps.py:47\u001b[0m, in \u001b[0;36mreweight\u001b[0;34m(original_weights, loss_matrix, targets_array, dropout_rate, epochs, log_path, penalty_approach, penalty_weight)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreweight\u001b[39m(\n\u001b[1;32m     38\u001b[0m     original_weights,\n\u001b[1;32m     39\u001b[0m     loss_matrix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     penalty_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     46\u001b[0m ):\n\u001b[0;32m---> 47\u001b[0m     target_names \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mloss_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m)\n\u001b[1;32m     48\u001b[0m     is_national \u001b[38;5;241m=\u001b[39m loss_matrix\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnation/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m     loss_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(loss_matrix\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "## ALL TESTS\n",
    "\n",
    "## For L1 and L0 penalty approaches which are integrated into the enhanced CPS dataset creation\n",
    "input_dataset = ExtendedCPS_2024\n",
    "\n",
    "approaches = [\"l0_sigmoid\", \"l0_log\", \"l0_exp\", \"l1\"]\n",
    "penalty_weights = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "\n",
    "def get_output_path(approach, file_name):\n",
    "    output_path = STORAGE_FOLDER / approach / file_name\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    return output_path\n",
    "\n",
    "results = []\n",
    "\n",
    "for approach in approaches:\n",
    "            for penalty_weight in penalty_weights:\n",
    "                # Storing files in correct locations\n",
    "                cal_log_name = f\"calibration_log_{approach}_{penalty_weight}.csv\"\n",
    "                h5_name = f\"enhanced_cps_2024_{approach}_{penalty_weight}_minimised.h5\"\n",
    "                cal_log_path = get_output_path(approach, cal_log_name)\n",
    "                h5_path = get_output_path(approach, h5_name)\n",
    "\n",
    "                sim = Microsimulation(dataset=input_dataset)\n",
    "                data = sim.dataset.load_dataset()\n",
    "                data[\"household_weight\"] = {}\n",
    "                original_weights = sim.calculate(\"household_weight\")\n",
    "                original_weights = original_weights.values + np.random.normal(\n",
    "                    1, 0.1, len(original_weights)\n",
    "                )\n",
    "                for year in range(2024, 2025):\n",
    "                    loss_matrix, targets_array = build_loss_matrix(\n",
    "                        input_dataset, year\n",
    "                    )\n",
    "\n",
    "                    bad_mask = loss_matrix.columns.isin(bad_targets)\n",
    "                    keep_mask_bool = ~bad_mask\n",
    "                    keep_idx = np.where(keep_mask_bool)[0]\n",
    "                    loss_matrix_clean = loss_matrix.iloc[:, keep_idx]\n",
    "                    targets_array_clean = targets_array[keep_idx]\n",
    "                    assert loss_matrix_clean.shape[1] == targets_array_clean.size\n",
    "\n",
    "                    optimised_weights = reweight(\n",
    "                        original_weights,\n",
    "                        loss_matrix_clean,\n",
    "                        targets_array_clean,\n",
    "                        log_path=cal_log_path, \n",
    "                        penalty_approach=approach,\n",
    "                        penalty_weight=penalty_weight, \n",
    "                        epochs=250,  # Reduced epochs for faster processing\n",
    "                    )\n",
    "                    data[\"household_weight\"][year] = optimised_weights\n",
    "\n",
    "                # Save to HDF5 file\n",
    "                with h5py.File(h5_path, \"w\") as f:\n",
    "                    for variable, values in data.items():\n",
    "                        for year, value in values.items():\n",
    "                            f.create_dataset(f\"{variable}/{year}\", data=value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ff392d",
   "metadata": {},
   "source": [
    "# Minimize.py approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For approaches external to the reweighting approach implemented in enhanced CPS dataset creation\n",
    "\n",
    "files = [\n",
    "        STORAGE_FOLDER / \"enhanced_cps_2024.h5\",\n",
    "    ]\n",
    "\n",
    "approaches = {\n",
    "        \"random_sampling_minimization\": random_sampling_minimization,\n",
    "        \"candidate_loss_contribution\": candidate_loss_contribution,\n",
    "}\n",
    "\n",
    "optional_params = {\n",
    "        \"random_sampling_minimization\": {\n",
    "            \"target_fractions\": [0.5, 0.6]#, 0.7, 0.8, 0.9],  # fractions of the dataset to keep\n",
    "        },\n",
    "        \"candidate_loss_contribution\": {\n",
    "            \"loss_rel_change_max\": [0.001, 0.0001]#, 0.00001, 0.000001, 0.0000001] # maximum relative change in loss\n",
    "        }\n",
    "}\n",
    "\n",
    "for approach, function in approaches.items():\n",
    "    minimization_function = function\n",
    "    # other minimization function approach is \"random_sampling_minimization\", for which you can specify the tolerance for loss relative change.\n",
    "\n",
    "    for params, values in optional_params[approach].items():\n",
    "        for value in values:\n",
    "            if params == \"target_fractions\":\n",
    "                for file in files:\n",
    "                    output_path = STORAGE_FOLDER / approach / f\"{value}_enhanced_cps_2024_minimised.h5\"\n",
    "                    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    minimize_dataset(\n",
    "                        file,\n",
    "                        output_path,\n",
    "                        minimization_function=minimization_function, \n",
    "                        target_fractions=[value]\n",
    "                    )\n",
    "            elif params == \"loss_rel_change_max\":\n",
    "                for file in files:\n",
    "                    output_path = STORAGE_FOLDER / approach / f\"{value}_enhanced_cps_2024_minimised.h5\"\n",
    "                    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    minimize_dataset(\n",
    "                        file,\n",
    "                        output_path,\n",
    "                        minimization_function=minimization_function, \n",
    "                        loss_rel_change_max=value\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ea957",
   "metadata": {},
   "source": [
    "### (Temporary) Cleaning of data (removing weights smaller than epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88df261",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this should go in the enhanced_cps_2024.py file, because household removal doesn't happen there\n",
    "# Need to check Ben's PR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b0fe2e",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "225debd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strategy</th>\n",
       "      <th>parameter</th>\n",
       "      <th>dataset_size</th>\n",
       "      <th>total_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>41310</td>\n",
       "      <td>0.0069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  strategy parameter  dataset_size  total_loss\n",
       "0     none      none         41310      0.0069"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Creating scoring of loss\n",
    "Creating dataframe to store regularization results\n",
    "\"\"\"\n",
    "\n",
    "# Calculate quality categories\n",
    "def loss_score(calibration_log):\n",
    "    excellent_count = (\n",
    "        calibration_log[\"rel_abs_error\"] < 0.05).sum()  # < 5% error\n",
    "    good_count = (\n",
    "        (calibration_log[\"rel_abs_error\"] >= 0.05)\n",
    "        & (calibration_log[\"rel_abs_error\"] < 0.20)).sum()  # 5-20% error\n",
    "    total_targets = len(calibration_log)\n",
    "    # Calculate quality score\n",
    "    quality_score = (excellent_count * 100 + good_count * 75) / total_targets\n",
    "    return quality_score\n",
    "\n",
    "\n",
    "\n",
    "# Initial dataframe setup\n",
    "reg_results_df = pd.DataFrame({\n",
    "    'strategy': ['none'],\n",
    "    'parameter': ['none'],\n",
    "    'dataset_size': [41310],\n",
    "    'total_loss': [6.9e-3]\n",
    "})\n",
    "\n",
    "def add_result(df, strategy, parameter, dataset_size, total_loss):\n",
    "    new_rows = pd.DataFrame({\n",
    "        'strategy': strategy,        \n",
    "        'parameter': parameter,      \n",
    "        'dataset_size': [dataset_size],\n",
    "        'total_loss': [total_loss]\n",
    "    })\n",
    "    return pd.concat([reg_results_df, new_rows], ignore_index=True)\n",
    "\n",
    "# Example usage\n",
    "#reg_results_df = add_result(reg_results_df, ['L1', 'L2'], ['0.001','0.002'], [35000, 4000], [7.2e-3, 7.2e-3])\n",
    "reg_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7bb3ef3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strategy</th>\n",
       "      <th>parameter</th>\n",
       "      <th>dataset_size</th>\n",
       "      <th>total_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>41310</td>\n",
       "      <td>0.0069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>l0_sigmoid</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41310</td>\n",
       "      <td>0.0069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>l0_sigmoid</td>\n",
       "      <td>0.1</td>\n",
       "      <td>41310</td>\n",
       "      <td>39.2959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>l0_sigmoid</td>\n",
       "      <td>0.01</td>\n",
       "      <td>41310</td>\n",
       "      <td>39.2959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>l0_sigmoid</td>\n",
       "      <td>0.001</td>\n",
       "      <td>41310</td>\n",
       "      <td>39.2959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>l0_sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>41310</td>\n",
       "      <td>39.2959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>l0_sigmoid</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>41310</td>\n",
       "      <td>39.2959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>l0_log</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41310</td>\n",
       "      <td>0.0069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>l0_log</td>\n",
       "      <td>0.1</td>\n",
       "      <td>41310</td>\n",
       "      <td>39.2959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>l0_log</td>\n",
       "      <td>0.01</td>\n",
       "      <td>41310</td>\n",
       "      <td>39.2959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>l0_log</td>\n",
       "      <td>0.001</td>\n",
       "      <td>41310</td>\n",
       "      <td>39.2959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>l0_log</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>41310</td>\n",
       "      <td>39.2959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>l0_log</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>41310</td>\n",
       "      <td>39.2959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>l0_exp</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41310</td>\n",
       "      <td>0.0069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>l0_exp</td>\n",
       "      <td>0.1</td>\n",
       "      <td>41310</td>\n",
       "      <td>39.2959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>l0_exp</td>\n",
       "      <td>0.01</td>\n",
       "      <td>41310</td>\n",
       "      <td>39.2959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>l0_exp</td>\n",
       "      <td>0.001</td>\n",
       "      <td>41310</td>\n",
       "      <td>39.2959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>l0_exp</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>41310</td>\n",
       "      <td>39.2959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>l0_exp</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>41310</td>\n",
       "      <td>39.2959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>l1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41310</td>\n",
       "      <td>0.0069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>l1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>41310</td>\n",
       "      <td>39.2959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>l1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>41310</td>\n",
       "      <td>39.2959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>l1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>41310</td>\n",
       "      <td>39.2959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>l1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>41310</td>\n",
       "      <td>39.2959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>l1</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>41310</td>\n",
       "      <td>39.2959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      strategy parameter  dataset_size  total_loss\n",
       "0         none      none         41310      0.0069\n",
       "1   l0_sigmoid       1.0         41310      0.0069\n",
       "2   l0_sigmoid       0.1         41310     39.2959\n",
       "3   l0_sigmoid      0.01         41310     39.2959\n",
       "4   l0_sigmoid     0.001         41310     39.2959\n",
       "5   l0_sigmoid    0.0001         41310     39.2959\n",
       "6   l0_sigmoid   0.00001         41310     39.2959\n",
       "7       l0_log       1.0         41310      0.0069\n",
       "8       l0_log       0.1         41310     39.2959\n",
       "9       l0_log      0.01         41310     39.2959\n",
       "10      l0_log     0.001         41310     39.2959\n",
       "11      l0_log    0.0001         41310     39.2959\n",
       "12      l0_log   0.00001         41310     39.2959\n",
       "13      l0_exp       1.0         41310      0.0069\n",
       "14      l0_exp       0.1         41310     39.2959\n",
       "15      l0_exp      0.01         41310     39.2959\n",
       "16      l0_exp     0.001         41310     39.2959\n",
       "17      l0_exp    0.0001         41310     39.2959\n",
       "18      l0_exp   0.00001         41310     39.2959\n",
       "19          l1       1.0         41310      0.0069\n",
       "20          l1       0.1         41310     39.2959\n",
       "21          l1      0.01         41310     39.2959\n",
       "22          l1     0.001         41310     39.2959\n",
       "23          l1    0.0001         41310     39.2959\n",
       "24          l1   0.00001         41310     39.2959"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pulling values from created calibration_log.csv and .h5 files to populate the line plot dataframe\n",
    "\n",
    "( I need to pull the strategy (folder name), parameter (from file title??), dataset size (from length of .h5 file), and total loss (from sum of loss column in calibration_log_file.csv))\n",
    "\n",
    "approaches = [\"l0_exp\", \"l1\"] \n",
    "penalty_weights = [1e-2, 1e-1]\n",
    "\"\"\"\n",
    "approaches = [\"l0_sigmoid\", \"l0_log\", \"l0_exp\", \"l1\"]\n",
    "penalty_weights = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "og_size = 41310  # Original size of the dataset\n",
    "og_loss = 6.9e-3  # Original loss from the baseline dataset\n",
    "\n",
    "for approach in approaches:\n",
    "    strategy = approach\n",
    "    reg_results_df = add_result(reg_results_df, strategy, 1.0, og_size, og_loss)\n",
    "    for penalty_weight in penalty_weights:\n",
    "        parameter = penalty_weight\n",
    "\n",
    "        # Pull length of .h5 file\n",
    "        h5_name = f\"enhanced_cps_2024_{strategy}_{parameter}_minimised.h5\"\n",
    "        h5_path = get_output_path(strategy, h5_name)\n",
    "        # see if this works\n",
    "        dataset_size = len(h5py.File(h5_path, \"r\")['household_weight/2024'])\n",
    "        #total_size.append(dataset_size)\n",
    "\n",
    "        # Pull sum of loss column\n",
    "        cal_log_name = f\"calibration_log_{strategy}_{parameter}.csv\"\n",
    "        cal_log_path = get_output_path(strategy, cal_log_name)\n",
    "        calibration_log = pd.read_csv(cal_log_path)\n",
    "        loss_value = loss_score(calibration_log)\n",
    "        \n",
    "        reg_results_df = add_result(reg_results_df, strategy, parameter, dataset_size, loss_value)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "fraction = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "for fraction in fraction:\n",
    "    strategy = \"random_sampling_minimization\"\n",
    "    parameter = fraction\n",
    "\n",
    "    # Pull length of .h5 file\n",
    "    h5_name = f\"{fraction}_enhanced_cps_2024_random_sampling_minimization_minimised.h5\"\n",
    "    h5_path = STORAGE_FOLDER / strategy / h5_name\n",
    "    dataset_size = len(h5py.File(h5_path, \"r\")['household_weight/2024'])\n",
    "\n",
    "    # Pull sum of loss column\n",
    "    cal_log_name = f\"{fraction}_enhanced_cps_2024_random_sampling_minimization_minimised_calibration_log.csv\"\n",
    "    cal_log_path = STORAGE_FOLDER / strategy / cal_log_name\n",
    "    total_loss = pd.read_csv(cal_log_path)['loss'].sum()\n",
    "\n",
    "    add_result(df, strategy, parameter, dataset_size, total_loss)\n",
    "\n",
    "'''\n",
    "reg_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b203ccd",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9602953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Synthetic dataset\n",
    "'''\n",
    "\n",
    "# Define values\n",
    "strategies = ['l0_sigmoid', 'l0_log', 'l0_exp', 'l1']\n",
    "parameters = [1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "# Synthetic values\n",
    "base_size = 41310\n",
    "min_size = 20000\n",
    "base_loss = 0.0069\n",
    "max_loss = 40.0\n",
    "\n",
    "# Construct rows\n",
    "rows = [{'strategy': 'none', 'parameter': 'none', 'dataset_size': base_size, 'total_loss': base_loss}]\n",
    "\n",
    "for strategy in strategies:\n",
    "    for i, param in enumerate(parameters):\n",
    "        # Gradually decrease size and increase loss\n",
    "        size = int(base_size - (base_size - min_size) * (i / (len(parameters) - 1)))\n",
    "        loss = round(base_loss + (max_loss - base_loss) * (i / (len(parameters) - 1)), 4)\n",
    "        rows.append({\n",
    "            'strategy': strategy,\n",
    "            'parameter': param,\n",
    "            'dataset_size': size,\n",
    "            'total_loss': loss\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "reg_results_df = pd.DataFrame(rows)\n",
    "\n",
    "# Display\n",
    "print(reg_results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "policyengine-us-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
